# 企业文化

## 游戏公司Supercell（超级细胞）

- 倒金字塔结构

    > 传统的金字塔型的组织架构是工业革命时代的产物，在流水线一样的工厂里，工作都是标准的，工人们只需日复一日地重复同样的事情，不允许犯任何错误。可是，这种做法，在游戏业甚至是所有创意行业却是行不通的。

    - 全体团队成员都需要具备很强的主动性。那些对他人依赖性强的人不太适合这种工作环境。它使人们意见更好地融合，每个人都像参与到我们正在做的事情中，形成了一种良好、健康的工作环境。

    - CEO埃卡·潘纳宁：自认为是“行业内最没权力的CEO”。他在每个项目中只有两个权力：

        - 1.审批一个团队的组建
        - 2.审批一个游戏是否可以从Beta测试进入全球上线的阶段

    - 决策权掌握在开发团队手中：那些真正参与研发的开发者们就有更多的自由，也能萌发更多的创意，他们做的决定越多，往往也更好更高效，因为决策者是真正接触用户的人。

        - 《部落冲突》的游戏主管伊诺·乔司：“我不是决定事物的人，我会提出我认为存在的问题的地方、可以改进的地方，但我不是做决定的人。团队做决定。我作为协调者，组织团队一起讨论并达成统一意见。我曾经提出过一些想法，被团队完全否决了。”

    - 要做到真正放权，就要容忍失败：失败并不是一件可耻的事情，如果一款游戏证明不具备竞争力，他们就会果敢地砍掉，甚至还会开香槟庆祝。

        - Supercell推出了5款商业化游戏，创作出《卡通农场》、《部落冲突》、《海岛奇兵》和《皇室战争》等大作。而在这些光鲜亮丽的背后，鲜为人知的是，他们砍掉了超过20款游戏。

    - 缺点：

        - 沟通很困难：我们把注意力集中在自己的事情上，没有有效地分享信息。

- 小团队+中台：开发过程中公共和通用的游戏素材和算法整合起来，并积累了非常科学的研发工具和框架体系，构建了一个功能非常强大的中台。这样强大的中台可以支持若干个小团队在短时间内开发出一款新的游戏。

    - 团队小而精：CEO说：“游戏行业竞争太激烈，未来难以预测。你只能尽可能组建最优秀的团队，允许这些团队在最好的工作环境下工作，从而实现成功几率的最大化。归根结底，这几乎是你所能够掌控的唯一一件事情。”

    - 阿里例子：

        - 每个电商业务都会涉及到商品信息，订单，支付，仓储，物流等等这样的通用系统，但各个板块之间数据不能共享，势必造成更大的浪费。

        - 阿里的6大中台（兵种）：

            | 中台     |                                                                                                                         |
            |----------|-------------------------------------------------------------------------------------------------------------------------|
            | 业务中台 | 提供重用服务，例如用户中心、订单中心之类的开箱即用可重用能力，为战场提供了空军支援能力，随叫随到，威力强大             |
            | 数据中台 | 提供数据分析能力，帮助从数据中学习改进，调整方向，为战场提供了海军支援能力                                              |
            | 算法中台 | 提供算法能力，帮助提供更加个性化的服务，增强用户体验，为战场提供了陆军支援能力，随机应变，所向披靡                      |
            | 技术中台 | 提供自建系统部分的技术支撑能力，帮助解决基础设施，分布式数据库等底层技术问题，为前台特种兵提供了精良的武器装备         |
            | 研发中台 | 提供自建系统部分的管理和技术实践支撑能力，帮助快速搭建项目、管理进度、测试、持续集成、持续交付，是前台特种兵的训练基地 |
            | 组织中台 | 为项目提供投资管理、风险管理、资源调度等，是战场的指挥部，战争的大脑，指挥前线，调度后方                                |

    - 腾讯例子：

        - 引擎中台对接的是腾讯以及外部的各大实验室，比如腾讯优图实验室、AILab实验室、XLab实验室等等，还有一些安全平台，证照库等。

        - 业务中台，是指在逻辑层有很多公用的服务，我们不需要每一个模块都去实现，有很多公用的服务可以抽取到业务中台去实现，比如像图像的处理、视频处理、下载代理，计费上报等。

        - 数据中台,因为每个业务都会有很多相关的数据处理，比如说计费、统计、业务报表、质量、分析、收入成本、客户分析对账等等，所以这一块我们统一抽象成了一个数据中台。


- 员工可以跳到别的岗位上：觉得自己的兴趣和才华在别处的员工，工作室允许他们自由地移动到别的部门。

## 云计算公司 Snowflake 

- CEO弗兰克·斯洛特曼 (Frank Slootman) 出版的一本书《Amp it up》

    >“我们的公司是海军陆战队，不是和平队。平静的生活不属于我们。像我们这样的创业公司，每天都要为了生存而与巨头对抗。我们是偏执狂，时时刻刻感到生存受威胁。加入我们，你必须有战斗心态。”

    - 1.加快节奏，时刻要求员工以更快的速度完成工作。
        > 如果你说一周后可以有结果，他就问你为什么不能明天或后天出结果？这倒不是因为着急，而是他要增加所有人的紧迫感。
        > 
        > 公司变大了，就会行动迟缓，不愿意冒险。只有加快节奏，才能让公司始终充满活力，保持兴奋度。
        > 
        > 他说：“要求某人做某事快20%，他们会使用传统策略。如果要求快2,000%，他们将不得不推翻所有基本假设，使用非传统策略，进行重大创新。”

    - 2.要求员工思考一些极端问题，打破传统思维的束缚。
        > 你如何在接下来的六个月内实现你的10年目标？
        > 
        > 如果每周只能工作一天，我们应该如何改变工作方式？
        > 
        > 如果现有的营销渠道都消失了，我们将如何发展新客户？
        > 
        > 产品增加什么特性，可以让价格提高10倍？
        > 
        > 如果你有10倍的资源，会对产品做哪些改变？

    - 3.提出明确的、雄心勃勃的目标，鼓励员工大胆行动。
        > iPod mini 的早期口号是“口袋里有 1,000 首歌曲”，SpaceX 公司的目标是让人类成为“多星球物种”。目标越清晰、越雄心勃勃，传统的惰性思维就越难生存。

    - 4.拒绝平庸的产品。
        > 他采取史蒂夫·乔布斯的标准，产品只有两种，要么是非常棒，要么是一塌糊涂，没有中间等级。
        >
        > 员工开发出新产品和新功能时，他会问：“你兴奋吗？你从心里喜欢它吗？”如果没有得到肯定答复，产品就必须重新调整。

    - 5.一流员工得到高额奖金。
        > 每个季度末，公司都要举行绩效评定，一年要评4次绩效。
        >
        > 绩效分布是一个钟形曲线，高绩效员工总是头部的少数人，可以得到极高的奖金。奖金放在一个奖金池，其他人只能分剩下的奖金，或者根本没有奖金。大多数公司里面，一流员工的薪水，相比他们的贡献都偏低，这不利于激励优秀员工。

    - 6.缩小焦点，他要求员工只关注最重要的事情。
        > “请列出接下来需要解决的100个问题，然后只留下最重要的问题1和问题2，放弃其他98个问题。”
        > 
        > 任何偏离核心使命的事情都会让人分心。对于同一个团队的每个成员，他分别挨个问：“你们团队的优先事项是什么？” 如果答案不一致，他就知道团队不够专注，必须整改。

# 项目管理

- [腾讯云开发者：项目总延期？需求乱插队？程序员如何做好项目管理](https://cloud.tencent.com/developer/article/2242782)

- 项目管理是「通过别人做成事情」的能力：

    - 例子：汉高祖刘邦有一句经典名言：“夫运筹策帷帐之中，决胜于千里之外，吾不如子房(张良)。镇国家，抚百姓，给馈饟，不绝粮道，吾不如萧何。连百万之军，战必胜，攻必取，吾不如韩信。此三者，皆人杰也，吾能用之，此吾所以取天下也。”正是刘邦具备协调张良、萧何、韩信三人协同工作的能力，才使得其能夺取天下，建立大汉王朝。

- 《项目管理精华》一书将项目管理视为「21 世纪独有的工作」，作者认为每一名知识型工作者都在工作中不知不觉中扮演着「非职业项目经理」的角色。开发者其实就是典型的知识型工作者。

- 《微权力下的项目管理》一书讲到项目经理往往需要有个人魅力去影响他人做事，进而达成目标。项目管理能提升与各类干系人打交道的能力，进而提升一个人在组织内的个人影响力。

- 项目管理的生活的例子：房屋装修。装修工期长，涉及各种材料购置。一次性购置材料会阻碍工人干活，分批购置，又怕丢三落四忙不开；等师傅进场再买，又担心延误工期。除此之外，装修还需要与各类工种打交道，要合理安排各工种工作排序、工期管理。

    - 在装修中，能不能少花点钱，就看一个人“成本管理“做得怎么样；能不能快点住进新房子取决于一个人的“进度管理”；而能不能住的安心，就要看“质量管理”有没有做好。

- 项目管理上的痛点：

    | 痛点问题           |                                                |
    |--------------------|------------------------------------------------|
    | 工作量评估问题     | 工作量评估不准确                               |
    | 进度问题           | 日常杂事或者临时问题打乱排期                   |
    | 外部依赖问题       | 设计/后台等外部资源延期/需求变更，怎么推动解决 |
    | 沟通问题           | 如何让大家对需求的理解保持一致                 |
    | 效率和质量平衡问题 | 怎么既保证开发效率又保证质量                   |

    ![image](./Pictures/soft-architecture/项目管理的好坏.avif)

- 如何做好进度管理：

    - 1.如何做好工作量的评估

        - 做好工作量评估是做好进度管理最关键的一步

        - 1.做好详细需求方案的设计。
            - 在做完后，再通过有开发经验的工作人员评审。

                - 一般经验丰富的开发人员能够通过设计方案发现背后的风险，能够及时将架构设计的不合理、兼容性未考虑等问题提前暴露出来，同时也能更加明确工作量。理论上来说，在完成需求方案评审后，后续的改动很少，整体的工作时长更加可控。

            - 如果开发周期大于一个月，建议分成多个需求迭代，以降低迭代周期，小步快跑。

        - 2.合理拆解，明确职责
        ![image](./Pictures/soft-architecture/项目管理拆解.avif)

            - 最好工作拆解的粒度为一至两天。
                - 太长，就会存在工作量评估不准确、整体项目难以把控的问题。不利于工作的合理分配，不能更好地利用人力资源。
                - 太短，就会导致工作交付的频率过快，开发者的工作之间也会存在着一定的耦合。拆解的粒度太小，会增加一定的沟通成本，得不偿失。

            - 成果作为导向原则
                - 任务拆解应该以可交互的结果作为导向，并且一定要有输出。这个输出应该是完整的，不然这个拆解就拆解得不够透彻，或者说不算一个任务。

            - 责任到人原则
                - 拆解之后的任务项，有且只能有一个负责人。即使许多人都可能在其上工作，也只能由一个人负责，其他人只能是参与者。

            - 任务分层原则
                - 任务拆解的过程也是一个解耦的过程，避免多个任务之间有耦合。拆解的过程应该是自上向下的，从一个大的任务，按照其特性进行任务拆解，不断地拆解成子任务，直到拆解到一至两天的工作量，并且是一个可交付的工作项。

            - h5项目例子：一共有四个大的功能模块，三个开发人员
            ![image](./Pictures/soft-architecture/项目管理拆解-h5例子.avif)

        - 3.工作量评估
            - 在对任务进行拆解后，下一步是对任务进行工作量评估。
            - 工作量评估不准确，就会直接导致该任务项出现问题。
                - 评估的时间偏多，会存在着资源浪费的问题
                - 评估的时间偏少，将直接造成当前任务延期完成，同时阻塞后面模块的开发，损失更大。

            - 自上而下的估算方式：
                - 有类似项目经验的工程师来说较容易评估。
                - 将工作结构从头部向尾部依次分配、传递工作量，直到到达WBS 的最底层。
                - 特点是：
                    - 项目初期信息不足，只能初步分解工作结构，很难将最基本的工作详细内容列出来。
                    - 估算的精度较差。
                    - 估算的工作量小，速度快。

            - 自下而上的估算方式：
                - 先估算各个工作项的工作量，再自下而上的将各个工作量进行汇总，算出总的工作量。
                - 特点是：
                    - 估算的精度高。
                    - 估算的成本较大。
                    - 缺少子工作项之间的工作量估算。

    - 2.如何做好依赖管理

        - 项目延期的常见情况
            - 准备开始开发了，发现设计稿还未就绪。
            - 准备联调的时候，发现我们上下游的技术团队的接口还未就绪。
            - 可以联调的时候，因为上下游的链条很长，出现推诿甩锅。

        - 针对外部依赖问题的解决方法

            - 1.明确责任人和交付时间，避免模糊

                - 当一个事情出现多个负责人的时候，责任的边界就会模糊，就容易互相推诿的情况，这就是责任分散效应。
                - 对于每个依赖项，我们需要明确其责任人，并沟通明确每个人对应依赖的交付时间，把责任人和交付时间提前确定清楚，可以减少很多争议和推诿。

                - 当项目涉及很多团队的时候，可以使用资源依赖列表，当遇到问题时，可以快速查找负责人及其应当交付的时间点。
                - 例子：云游 XX 活动的资源依赖列表
                ![image](./Pictures/soft-architecture/项目管理-资源依赖列表.avif)

            - 2.形成信息对齐机制

                - 确定了接口负责人后，如果不及时进行信息对齐，也会出现跑偏的情况。

                    - 例子：A 项目依赖外部的 sdk的 某个升级版。在最初的对齐方案中，sdk 方承诺不会修改原有的接口调用方式。而实际联调中才发现不仅接口调用方式发生了巨大变化，还有部分被依赖的接口直接在新版本中移除，导致A方需要花费大量时间进行兼容。如果提前对齐而不是等到联调阶段才介入，就能规避上述问题。

                - 信息对齐方式

                    - 1.不定期的非正式沟通
                        - 在里程碑等关键节点通过面对面、电话、企业微信等方式进行信息对齐。对齐内容包括：开发进度、依赖事项进展、技术方案变更等，对于一些关键性的结论，最好有文字落地以用于回溯。

                    - 2.定期的例会机制
                        - 如定期晨会机制。在会议上对齐项目进度，可以提前发现可能存在的风险。记录会议纪要并通过群消息/文档/邮件的形式通知到项目的相关干系人。

                    - 3.项目 owner 机制
                        - 应当确定一个项目 owner，对项目整体负责，把关整体节奏，负责组织会议。把相关信息进行整合，并同步给项目的相关干系人。

                    - 4.求同存异，达成共识

                        - 例子：作为最重要的传统节日，很多业务团队都会针对春节这个时间节点运营、上线活动，作者曾经遇到过在临近提测时，活动仍在被提需要大量变更的情况（运营人员要叠加功能，设计人员则提出更多特效的要求），开发人员如果接受了大量变更，不仅意味着不断加班，更可怕的是会由此带来很大的质量风险，一旦出现严重问题，会得不偿失。
                            - 最后只能是开发人员联合测试人员，跟运营和设计进行了沟通，研发侧认可变更对于提升活动效果有作用，同时也对变更可能带来的延期，以及影响线上质量等风险进行了全面分析评估。双方基于共同的目标做出了协商和让步（既保证活动效果，同时也保证活动正常安全上线）。

                    - 5.情感账户，软性推动

                        - 当项目依赖某个外部团队的人员支持，而这个事情并不是对方当前工作范围内的，并不是对方第一优先级的工作，该怎么办？

                            - 大部分情况，有些开发者会在沟通未果的情况下，通过上升到leader去推动事情落地，这是一种解决方案。

                            - 更优的方案是建立相关依赖方的“情感账户”，借助“情感账户”去软性推动。

                                - 大家都知道银行账户就是把钱存进去，作为储蓄，以备不时之需。“情感账户”里储蓄的是人际关系中不可或缺的信任。经营好「情感账户」，也是经营好一个人与合作伙伴的信任关系。

                                - 在日常工作中多吃亏，让自己的「情感账户」适当“存储”。例如自己曾经抽出休息时间帮助合作伙伴解决问题，当需要对方协助的时候，相信也能得到积极的响应，这也是常说的“吃亏是福"。

    - 3.如何处理意外事项

        - 例子：插入一些高优的需求，或者说发生一些不可控的因素如疫情等等导致人力不足，从而影响项目的进展。

        - 1.需求的变更。
            - 处理方式：

                - 判断需求变更的大小，如果是样式修改等简单变更，半小时能解决的小问题，可以协助快速调整；如果工作量在 0.5 天以上，并且需要依赖第三方接口，则需要将整体的需求重新评估，重新梳理排期，并同步给干系人。 

                - 先保证核心的业务流程不变，高收益的工作量优先处理，保证正常的上线时间，后续有余力再对其它功能点进行迭代优化。

        - 2.高优需求插入
            - 处理方式：
                - 如果被高优需求插入，直接带来的影响是延后当前的工作完成时间。
                - 如果在 0.5 天以内，没有被依赖的下游时，再评估对排期影响不大的情况下是否可以快速响应。并第一时间反馈风险，确保各干系人都有一个心理预期；如果大于 0.5 天的需求，则建议反馈给项目干系人来安排其他人来解决。

        - 3.不可抗力的因素。

            - 例子：开发人员有急事需要请假，又或者因为疫情导致办公效率低下，从而影响项目的进展。

            - 处理方式：
                - 如果是一些身体原因导致办公效率低下。在不影响整体项目交付的情况下，适当的延长完成项目的时间；若影响到整体项目交付的时间，则应该暴露该风险，进行项目计划调整。
                - 如果完全不能投入开发，应该尽早的将此事向上级报备，由上级进行统一的人力调整，交由其他人投入开发。

        - 4.内部依赖延后
            - 处理方式：
                - 将自身的业务流程做好，依赖部分通过模拟的方式解决。
                - 将联调的时间后移，先开发其他的功能模块。
                - 如果已经是最后联调阶段，则需要再次调整交付的时间，同时将该风险同步给相关的干系人。

    - 4.通过流程规范提高质量

        - 1.制定研发流程规范
            - 制定流程有时会让人反感，觉得降低了研发效率。但规范的流程可以大大提升项目的质量，好的流程都是在实践中不断总结出来的，是项目的最佳实践。
            - 尽量将流程变成 CICD 的约束，通过系统来约束、控制，减少其对人的依赖。
            - 当然流程也不是一成不变的，它需要根据我们的具体情况不断调整优化，才能适应当下的需要。
            - 研发团队流程：需求评审、方案设计、需求开发、测试验收、发布上线、项目复盘六个步骤。
            ![image](./Pictures/soft-architecture/项目管理-研发流程.avif)

        - 2.严格执行Code Review

            - code review 的好处不仅仅是能够大大提高代码质量，减少代码 bug，还能从心理上（自己写的代码要给别人审核）让自己更认真严谨些。

        - 3.制定发布清单（checklist）

            - 发布 checklist 一般可以分为服务、机器、流程三部分，通过日常工作中累计容易出错的地方，将其整理收集起来，持续完善。

            参考例子：

            | 提测前                                                                                       |
            |----------------------------------------------------------------------------------------------|
            | 根据前期编写的测试用例进行整体自测                                                           |
            | 根据埋点文档验证埋点，确保埋点中的事件和维度不多报、不漏报、不错报、不重复报、报的时机正确   |
            | 根据设计稿叠图并截图（2+测试机），确保无视觉问题                                             |
            | 确保分支的代码 CR 通过                                                                       |
            | 确保代码已发布到测试环境，并确认页面能够正常访问                                             |
            | 确保创建了提测单，提测单包含测试用例地址、测试范围、测试入口和二维码、终端环境、埋点文档地址 |
            | 确保需求单状态扭转到增量测试中                                                               |

            | bug修复后                                                                                                                   |
            |-----------------------------------------------------------------------------------------------------------------------------|
            | 涉及到功能、逻辑、埋点、样式和交互变更：重新走本次需求逻辑部分的自测、涉及样式的叠图、CR 和发布测试环境流程，确保全流程无误 |
            | 确保bug单状态扭转到已处理，并通知测试同学验证，保证在 1D 之内扭转到已关闭                                                   |
            | 确保需求单状态扭转到待发布                                                                                                  |

            | 发布前                                                     |
            |------------------------------------------------------------|
            | 确保产品体验、设计走查、测试都通过                         |
            | 确保所有代码（功能+bug 修复）都已经通过 CR，合入 master    |
            | 确保正式环境配置文件中的配置都是正式环境的配置             |
            | 如图片有新增和修改，确保图片已经进行过压缩                 |
            | 确认接口监控的数据正常，业务错误码屏蔽正常，不误报         |
            | 上线前和产品运营确认线上配置是否正确，涉及运营资源是否到位 |
            | 和后台、终端确认好发布顺序，并确保按照约定顺序发布         |
            | 确保在群里进行发布周知，提交的发布审批通过才能进行发布     |

            | 发布后                                                                                                |
            |-------------------------------------------------------------------------------------------------------|
            | 待 CDN 生效后，用非公司 wifi 访问页面，确保页面正常，同时确保所有的资源都是正式的 CDN 地址            |
            | 关注告警群消息，关注告警监控平台流量监控是否有较大波动，JS 报错、接口错误率是否有上涨，关注是否有告警 |
            | 发布出现问题，及时在群里周知并回滚，通知leader，并寻求团队成员协助定位排查                            |
            | 发布外网后需要留守至少 30 分钟                                                                        |
            | 确保需求单状态扭转到已交付和已接受                                                                    |

    - 5.管理变更影响
        - 在需求设计阶段提前对变更进行评估、规划，可以确保在对程序最小负面影响的情况下实施这些变更。
        - 通过详细设计评审技术方案
            - 编写技术文档对部分工程师来说是反感的事情，但好的项目质量一定是设计出来的，而不是测试出来的。
            - 所以在正式编码前，详细思考、设计整体方案并编写成技术文档在组内评审，是规避质量风险非常好用的方法，也是非常好的开发习惯。它可大大减少编码阶段的质量风险。

            - 例子：以之前笔者团队为例，我们还整理了团队详细文档模板，把大家做详细设计需要考虑的点都囊括了进去，避免大家遗漏。如性能设计、监控日志设计、安全风险设计、用例设计、容灾设计等，既是模板也是详细设计的 checkList。

# 理论
## Little’s Law（利特尔法则）

- [infoq：跟我一起认识 Little’s Law](https://www.infoq.cn/article/uzfdjVym5vEPRA8DOxYG)

    - Little’s Law：延迟和吞吐的关系是受并发数影响的，抛开并发数去找另外两者的关系是没有规律的。

        - 并发用户数：指真正对服务发送请求的用户数量，需要注意和在线用户数的区别

            - 例子：在线用户数为 1000，其中只有 100 个用户的操作触发了与远端服务的交互，并发用户数是 100

        - 响应时间：Little’s Law 中是“平均响应时间”，而实际工作中“分位值”来作为响应时间的统计值来衡量性能的。平均值只是作为一个辅助参考。

            - 例子：平均工资通常没多大参考价值，有可能很多人是被平均的。

            - 分为值：假如一共有100个请求，那么排在第90位的响应时间就是90分位值。有90分位、95分位、75分位。
                ![image](./Pictures/soft-architecture/分为值.avif)

    - 并发数 = 吞吐量 * 响应时间

        - 假如一个程序只有 1 个线程，这个线程每秒可以处理 10 次事件，那么我们说这个程序处理单次事件的延迟为 100ms，吞吐为 10 次/秒。

        - 假如一个程序有 4 个线程，每个线程每秒可以处理 5 次事件，那么我们说这个程序处理单次事件的延迟为 200ms，吞吐为 20 次/秒。

        - 假如一个程序有 1 个线程，每个线程每秒可以处理 20 次事件，那么我们说这个程序处理单次事件的延迟为 50ms，吞吐为 20 次/秒。

    ![image](./Pictures/soft-architecture/Little’s-Law.avif)

    - 拐点：并发用户数增加，吞吐量开始出现下降的趋势，同时响应时间也开始增大

    - 在“拐点”之前和刚进入拐点这段区域：系统是“稳定”的，并发数、吞吐量、平均响应时间是符合 Little’s Law 公式的。

## 长尾理论

- 长尾理论：原来不受到重视的销量小但种类多的产品或服务由于总量巨大，累积起来的总收益超过主流产品的现象。在互联网领域，长尾效应尤为显著。

    - 例子：亚马逊40%的书本销售来自于本地书店里不卖的书本。音乐影视串流市场、智能手机应用市场、线上游戏市场陆续发生这种现象，网络选择客制化的兴起也让实体市场产品逐渐零碎化，例如名不见经传的餐厅在网络市场下爆红。

    - 例子：亚马逊一半左右的销售来自于比较热门的商品，而另一半却来自相对不那么热门的商品。这跟传统的“二八定律（80%的业绩来自20%的产品）”完全相反

# 软技能

- [腾讯云开发者：如何成为优秀工程师之软技能篇](https://zhuanlan.zhihu.com/p/587383325)

- [阿里开发者：六年团队Leader实战秘诀｜程序员最重要的八种软技能](https://developer.aliyun.com/article/933310?spm=a2c6h.14164896.0.0.40ddd46eXOv9U4)

- [腾讯大讲堂：一篇文章入门专利写作（万字干货）](https://cloud.tencent.com/developer/article/2092422?areaSource=&traceId=)

# 数据中心（IDC）

- 德国的防核弹机房

    - 机房处在一个乡下的地方，上空是德国政府规定禁飞区，房子是一米多厚的混凝土墙，门是钢板的，地下每一层要经过好几道铁门可以进去。

    - 在法国有数据备份

    - 两套电力系统

- [任泽平：中国新基建研究报告2022](https://baijiahao.baidu.com/s?id=1732147608499611955&wfr=spider&for=pc)

    - 根据中国信息通信研究院测算，“十四五”期间我国新基建投资将达到10.6万亿，占全社会基础设施投资10％左右；2021-2023年，数据中心产业投资或达1.4万亿元；2020-2025年，5G网络建设投资累计将达到1.2万亿元，带动产业链上下游以及各行业应用投资超过3.5万亿元。

    - 数据中心的产业链：

        - 上游：服务器、交换机、路由器、光模块、配套软件、电力设备以及运营商等

            - 数据中心投资资金的主要流动方向。从投资占比看，服务器投资额占比最大，为69.28％。其次为交换机，占比为8.31％。第三是光模块，占比为8.31％。

            - 我国目前已经建成约500万架，相对2015年的124万架同比增长303％左右。同时，2015-2020年间，我国数据增量年均增速超过30％，预计2021年后仍以每年超过20％的速度新增。

            - 供配电系统占整体IDC系统及机柜投资的46.82％，占整体投资额的6.05％。

        - 中游：互联网及移动互联网、物联网以及工业物联网、IDC、云服务、IAAS、SAAS、数据安全以及数据交换。

        - 下游：智慧出行、智慧家居、泛娱乐、新零售、智慧医疗、金融、电信、工业以及精准营销等行业。根据

    - 东数西算：

        > 将东部大量需要运算的数据（数）通过光纤信息通道传输至西部，并使用西部的算力枢纽进行计算（算）后将结果返回东部供分析研究使用。

        - 目前数字经济产业多集中于东部地区，导致东部对数据中心的需求旺盛，因此大部分数据中心集中在算力成本高昂的东部。而西部对大数据中心的需求较低，数据中心分布少、上架率低。

            - 2020年中华北、华东以及华南三地机柜数量占全国机柜总数量的79％，上架率约在60-70％之间。但东部土地稀缺，生活成本高昂，能源相对稀缺，导致东部算力成本高居不下。与此同时，东北、西北、西南以及华中四地机柜数量只占总机柜数量的25％，上架率约在30-40％之间。

        - 城市中心的算力中心用作“边缘算力”，对一些对网络要求较高的业务如：工业互联网、金融证券、灾害预警、远程医疗、视频通话、人工智能推理等，进行实时低延迟运算。同时，工信部也对全部数据中心的上架率以及PUE（能源使用效率）进行了严格限制，防止了盲目发展。

            - 京津冀、长三角、粤港澳大湾区、成渝、内蒙古、贵州、甘肃、宁夏在内的8个国家算力枢纽节点，同时规划了10个国家数据中心集群。

        - 东数西算 + 特高压：

            - 数据中心电力消耗大，未来对清洁电力需求增强。特高压进行远距离传输比光纤损耗较大，因此“东数西算”相对于“西电东输”能够在保证成本的前提下更好地执行“双碳”目标。

# api设计

- [阿里技术：深度 | API 设计最佳实践的思考](https://developer.aliyun.com/article/701810?spm=a2c6h.12873639.0.0.33e56605iyuCWs)

- [苏三说技术：瞧瞧别人家的API接口，那叫一个优雅](https://juejin.cn/post/7176220436714225721)

# 架构设计

- [腾讯技术工程：谈谈架构设计](https://cloud.tencent.com/developer/article/2255693?areaSource=103001.1&traceId=00iu5zGPktoj8ynGnpC4n)

    - 模块与组件：模块是逻辑单元，组件是物理单元。
        - 模块：的粒度可大可小， 可以是系统，几个子系统、某个服务，函数， 类，方法、 功能块等等。
        - 组件：可以包括应用服务、数据库、网络、物理机、还可以包括 MQ、容器、Nginx 等技术组件。

    - 框架是组件实现的规范：

        - MVC、MVP、MVVM 等，是提供基础功能的产品
        - 开源框架：Ruby on Rails、Spring、Laravel、Django 等。可以拿来直接使用或者在此基础上二次开发。
            - SpringMVC 是 MVC 的开发框架，除了满足 MVC 的规范,Spring 提供了很多基础功能来帮助我们实现功能，包括注解(@Controller 等)、Spring Security、SpringJPA 等很多基础功能。

    - 架构设计目的

        - 如果没有架构设计，说明你的系统不够复杂。

            - 随着业务的增长，系统由单体应用渐进演化为分布式和微服务化。系统整体的复杂性越来越高，技术团队可能从一个团队变成多个专业化团队。

        - 架构的本质是管理和解决系统的复杂性，提高效率。管理复杂性：对系统进行有序化重构，不断减少系统的“熵”，使系统不断进化，改善软件质量为目的的内在结构性变化；提高效率：对系统进行有序化重构，以符合当前业务的发展，并可以快速扩展。

        - 无论是何种变化，架构师通过理解业务，全局把控，权衡业务需求和技术实现，选择合适技术，解决关键问题、指导研发落地实施，促进业务发展，提高效率。

            - 没有最优的架构，只有最合适的架构，一切系统设计原则都要以解决业务问题为最终目标，脱离实际业务的技术情怀架构往往会给系统带入大坑，任何不基于业务做异想天开的架构都是耍流氓。

            - 研发人员为了所谓微服务化而拆分，而不是从当前业务考虑。导致系统无序的状态，开发效率低。

                - 例子：一个简单项目拆分成 8 个子服务，问他为什么这么拆分，说微服务化是为了应对以后扩展方便。结果这个项目从 2017 年到现在都没有再修改过，接手人宁愿新开发一个项目也不愿重构。

                - 系统应用服务跟踪问题：由于微服务化后，系统逻辑复杂，服务出现问题后，你很难快速的定位问题和修复。这是我们踩过不少坑，我们使用 dubbo 服务化，系统一旦出现问题，一推人手忙脚乱。

    - 架构误区
        - 1.架构专门由架构师来做，业务开发人员无需关注
        - 2.架构师确定了架构蓝图之后任务就结束了
        - 3.不做出完美的架构设计不开工
            - 我们需要的不是一下子造出一辆汽车，而是从单轮车 --> 自行车 --> 摩托车，最后再到汽车。
        - 4.为虚无的未来埋单而过度设计
            - 在创业公司初期，业务场景和需求边界很难把握，产品需要快速迭代和变现，需求频繁更新，这个时候需要的是快速实现。
            - 不要过多考虑未来的扩展，说不定功能做完，效果不好就无用了。
        - 5.一味追随大公司的解决方案
            - 网站在讨论架构决策时，最有说服力的一句话就成了“淘宝就是这么搞的”或者“腾讯 就是这么搞的”。
        - 6.为了技术而技术
            - 技术是为业务而存在的，除此毫无意义。

    - 架构方法论

        - 不同架构方法论，定义的架构分类也不同，RUP4+1 架构方法主要是以架构生命周期为视角进行描述，而 TOGAF9 按架构涉及内容维度来描述。

        - 1.RUP4+1 架构视图

            - 1995 年，Philippe Kruchten 在《IEEE Software》上发表了题为《The 4+1 View Model of Architecture》的论文，引起了业界的极大关注，并最终被 RUP 采纳。

            - 用例驱动：在软件生命周期的各个阶段对软件进行建模,从不同视角对系统进行解读，从而形成统一软件过程架构描述。

            - 不同架构视图承载不同的架构设计决策，支持不同的目标和用途

                - 1.逻辑视图：用于描述系统软件功能拆解后的组件关系,组件约束和边界,反映系统整体组成与系统如何构建的过程。关注功能和逻辑层。
                - 2.开发视图：描述系统的模块划分和组成,以及细化到内部包的组成设计,服务于开发人员,反映系统开发实施过程。
                - 3.物理视图：描述软件如何映射到硬件，反映系统在分布方面的设计，系统的组件是如何部署到一组可计算机器节点上,用于指导软件系统的部署实施过程。
                - 4.处理流程视图：用于描述系统软件组件之间的通信时序,数据的输入输出,反映系统的功能流程与数据流程,通常由时序图和流程图表示。关注进程、线程、对象等运行时概念以及相关的并发、同步、通信等问题。

                - 运用 4+1 视图方法：针对不同需求进行架构设计。
                ![image](./Pictures/soft-architecture/架构设计-RUP4+1.avif)

        - 2.TOGAF9

            - TOGAF9 的架构分类：业务架构、应用架构、数据架构、技术架构, 代码架构, 部署架构。
            ![image](./Pictures/soft-architecture/架构设计-TOGAF9.avif)

                - 业务架构是战略，应用架构是战术，技术架构是装备。
                - 应用架构承上启下，一方面承接业务架构的落地，另一方面影响技术选型。熟悉业务，形成业务架构，根据业务架构，做出相应的应用架构，最后技术架构落地实施。

            - 1.业务架构
            ![image](./Pictures/soft-architecture/架构设计-TOGAF9-业务架构.avif)

                - 包括业务规划，业务模块、业务流程，对整个系统的业务进行拆分，对领域模型进行设计，把现实的业务转化成抽象对象。

                - 业务能力定义企业做什么，业务流程定义企业怎么做。业务架构就是对企业的业务流程，进行根本性的再思考和在思考的彻底性再设计，从而获得成本、质量、速度等方面业绩的巨大的改善或提高。

                - 今天面临的业务量有多大，增长走势是什么样，而且解决高并发的过程，一定是一个循序渐进逐步的过程。合理的架构能够提前预见业务发展 1~2 年为宜。

            - 2.产品架构

                - 将这些不同用途的功能模块围绕特定的业务目标进行分类整合。

                    - 注重产品功能的枚举、功能模块之间的分界。

                - 功能模块是用户能够完成一个操作的最小粒度的完整功能

                    - 例子：一个展示可购买商品的列表页、一个修改用户密码的功能。

                    - 确保用户能通过一个功能模块完整的完成一项工作，而不是半个工作。

                - 功能模块之间有直接关系、间接关系：
                    - 只有直接关系的功能模块才会被组织到一起，形成一个子系统。
                    - 间接关系的模块，会在不同的层级通过直接关系的模块产生联系。

                - 当具有直接关系的功能模块组合成一个子系统后，解决相同问题域的子系统就形成一个功能层级。功能层级按照接近用户实操的距离程度进行从上到下，或者从左至右的划分，这就形成了产品架构的分层。
            - 3.应用架构（剖面架构，也叫逻辑架构图）：

                - 业务架构的每一部分都有应用架构。
                - 应用架构在产品架构的基础上考虑两个事情：
                    - 1.子系统间的关系
                    - 2.将可复用的组件或模块进行下沉，沉淀到平台层，为业务组件提供统一的支撑。

                - 应用架构定义系统有哪些应用、以及应用之间如何分工和合作。

                    - 应用作为独立可部署的单元，为系统划分了明确的边界，深刻影响系统功能组织、代码开发、部署和运维等各方面

                - 应用分层：
                    - 1.水平分（横向）：按照功能处理顺序划分应用，比如把系统分为 web 前端/中间服务/后台任务，这是面向业务深度的划分。
                    - 2.垂直分（纵向）：按照不同的业务类型划分应用，比如进销存系统可以划分为三个独立的应用，这是面向业务广度的划分。

                - 应用之间的通信：
                    - 通信机制：同步调用/异步消息/共享 DB 访问等
                    - 数据格式：文本/XML/JSON/二进制等

                - 应用的分，偏向于业务，反映业务架构，应用的合，偏向于技术，影响技术架构。

            - 4.数据架构
                - 数据架构指导数据库的设计. 不仅仅要考虑开发中涉及到的数据库，实体模型，也要考虑物理架构中数据存储的设计。
                ![image](./Pictures/soft-architecture/架构设计-TOGAF9-数据架构.avif)

            - 5.代码架构（开发架构）

                - 代码架构设计不足，就会造成影响全局的架构设计。比如公司内不同的开发团队使用不同的技术栈或者组件，结果公司整体架构设计就会失控。

                - 最好的样本是参考现有《阿里巴巴 Java 开发手册》。

                - 代码架构定义的内容：
                    - 1.代码单元: 1、配置设计 2、框架、类库。
                    - 2.代码单元组织：1、编码规范，编码的惯例 2、项目模块划分 3、顶层文件结构设计，比如 mvc 设计 4、依赖关系

                    ![image](./Pictures/soft-architecture/架构设计-TOGAF9-代码架构.avif)

            - 6.技术架构

                - 应用架构本身只关心需要哪些应用系统，哪些平台来满足业务目标的需求，而不会关心在整个构建过程中你需要使用哪些技术。

                - 架构设计工作中最为困难的工作：需要具备软件和硬件的功能和性能的过硬知识

                - 技术架构考虑的内容：
                    - 确定组成应用系统的实际运行组件（lvs，nginx，tomcat，php-fpm 等）。
                    - 这些运行组件之间的关系，以及部署到硬件的策略。
                    - 系统的高可用、高性能、扩展、安全、伸缩性、简洁等做

            - 7.部署拓扑架构图（实际物理架构图）
                - 主要是运维工程师主要关注的对象。
                - 部署了几个节点，节点之间的关系，服务器的高可用，网路接口和协议等，决定了应用如何运行，运行的性能，可维护性，可扩展性，是所有架构的基础。
                ![image](./Pictures/soft-architecture/架构设计-TOGAF9-拓扑架构.avif)
                ![image](./Pictures/soft-architecture/架构设计-TOGAF9-拓扑架构1.avif)

    - 架构级别
        - 金字塔的架构级别：上层级别包含下层：系统级、应用级、模块级、代码级。
        ![image](./Pictures/soft-architecture/架构设计-架构级别-金字塔.avif)

        | 等级   | 内容                                                   |
        |--------|--------------------------------------------------------|
        | 系统级 | 整个系统内各部分的关系以及如何治理：分层               |
        | 应用级 | 单个应用的整体架构，及其与系统内单个应用的关系等       |
        | 模块级 | 应用内部的模块架构，如代码的模块化、数据和状态的管理等 |
        | 代码级 | 从代码级别保障架构实施                                 |

            - 基于架构金字塔，我们有了系统架构的战略设计与战术设计的完美结合：
            | 顶层设计 | 承上启下                                   |
            |----------|--------------------------------------------|
            | 战略设计 | 业务架构用于指导架构师如何进行系统架构设计 |
            | 战术设计 | 应用架构要根据业务架构来设计               |
            | 战术实施 | 应用架构确定以后，就是技术选型             |

    - 应用架构演进：随着业务架构不断进化，同时应用架构依托技术架构最终落地。
    ![image](./Pictures/soft-architecture/架构设计-应用架构演进.avif)

        - 架构演进过程：单体应用 -> 分布式应用服务化 -> 微服务

        - 1.单体应用：

            - 只应用某个简单场景，应用服务支持数据增删改查和简单的逻辑即可
            - 三级架构：前端（Web/手机端）+ 中间业务逻辑层 + 数据库层

            - 非功能性需求的做法：

                - 1.性能需求：使用缓存改善性能
                - 2.并发需求：使用集群改善并发
                - 3.读写分离：数据库地读写分离
                - 4.使用反向代理和 cdn 加速
                - 5.使用分布式文件和分布式数据库

            - 优点：容易部署、测试

            - 缺点：

                - 复杂性高：模块的边界模糊、 依赖关系不清晰、 代码质量参差不齐、 混乱地堆砌在一起。每次功能的变更或缺陷的修复都会导致需要重新部署整个应用，出错率比较高。

                - 可靠性差：某个应用 Bug，例如死循环、内存溢出等， 可能会导致整个应用的崩溃。

                - 扩展能力受限：无法根据业务模块的需要进行伸缩。
                    - 有的模块是计算密集型的，它需要强劲的 CPU；有的模块则是 IO 密集型的，需要更大的内存。由于这些模块部署在一起，不得不在硬件的选择上做出妥协。

                - 阻碍技术创新：单体应用往往使用统一的技术平台或方案解决所有的问题， 团队中的每个成员 都必须使用相同的开发语言和框架，要想引入新框架或新技术平台会非常困难。

                - 技术债务：随着时间推移、需求变更和人员更迭，会逐渐形成应用程序的技术债务， 并且越积 越多。“ 不坏不修”， 这在软件开发中非常常见， 在单体应用中这种思想更甚。

        - 2.分布式（微服务）

            - 对系统按照业务功能模块拆分，将各个模块服务化，变成一个分布式系统。

            - 优点：
                - 降低了耦合度：把模块拆分，使用接口通信,降低模块之间的耦合度。
                - 责任清晰：把项目拆分成若干个子项目，不同的团队负责不同的子项目。
                - 扩展方便：增加功能时只需要再增加一个子项目，调用其他系统的接口就可以。
                - 部署方便：可以灵活的进行分布式部署。
                - 提高代码的复用性：Service 层，如果不采用分布式 rest 服务方式架构就会在手机 Wap 商城，微信商城，PC，Android，iOS 每个端都要写一个 Service 层逻辑，开发量大，难以维护一起升级，这时候就可以采用分布式 rest 服务方式，公用一个 service 层。

                - 易于开发和维护：一个微服务只会关注一个特定的业务功能
                    - 单个微服务启动较快：单个微服务代码量较少， 所以启动会比较快。
                    - 局部修改容易部署：对某个微服务进行修改，只需要重新部署这个服务即可。
                    - 技术栈不受限：部分微服务使用 Java 开发，部分微服务使用 Node.js 开发

            - 缺点：
                - 运维要求较高：在单体架构中，只需要保证一个应用的正常运行。而在微服务中，需要保证几十甚至几百个服务服务的正常运行与协作
                - 系统之间的交互要使用远程通信，接口开发增大工作量
                - 如果修改某一个微服务的 API，可能所有使用了该接口的微服务都需要做调整。
                - 重复劳动：很多服务可能都会使用到相同的功能，而这个功能并没有达到分解为一个微服务的程度，这个时候，可能各个服务都会开发这一功能，从而导致代码重复。
                    - 尽管可以使用共享库来解决这个问题（例如可以将这个功能封装成公共组件，需要该功能的微服务引用该组件），但共享库在多语言环境下就不一定行得通了。

- [技术琐话：一家中型互联网公司的架构演进之路]()

    - 自如租房公司。此文摘自《云原生落地：企业级DevOps实践》一书

    - 技术与业务的关系就像汽车

        - 汽车有三大组件：车轮、发动机、方向盘

        - 对应三种技术：技术支持、技术驱动、技术颠覆。95%的企业是技术支持型企业

    - 一般都是先追求业务的快速迭代试错，架构一般会滞后于业务的发展，在架构跟不上业务的迭代速度，或有巨大的历史技术债务出现时，技术架构才会进行新一轮的迭代。同时，没有任何一个架构是“银弹”，凡是能够解决当下企业面临的问题的架构就是好架构。

    - 好的架构特征：可用性、可靠性、高性能、易维护、可拓展、安全性

    - 架构的分类：

        - 1.业务架构：一般是指业务的关键流程、组织形式、信息流。
            - 电商为例，业务架构包括选品、采购、仓储、物流、供应商、订单等一系列的业务板块。核心是定义业务痛点，厘清功能需求和非功能性需求。

        - 2.功能架构：产品具备的细分功能。
            - 电商系统的功能架构可细分为用户管理、登录注册、商品管理、仓库管理、订单管理、购物车管理、支付管理等核心模块。

        - 3.应用架构：根据业务场景设计出应用的层次结构，制定好应用间的调用、交互方式，确保它们能够融合在一起并满足业务需要。
            - 电商系统的应用架构可能有用户中心、权限中心、登录系统、商品中心、搜索引擎、推荐体系、订单系统、交易系统等。应用架构体现的是用什么样的微服务去支持功能的实现。

        - 4.技术架构：实现应用架构的关键技术栈
            - 如Spring Cloud、ZooKeeper、RocketMQ、Redis、MySQL、Elasticsearch等中间件，以及各种核心流程的时序图、状态图等信息。

        - 5.物理架构：从物理视角来看IDC中的物理拓扑关系，如防火墙、Nginx、网络、应用服务器、数据库间的调用和数据流转关系。
            - 如何通过硬件配置硬件和网络来配合软件系统达到可靠性、高可用性、性能、安全性等方面的要求。

    - 企业级架构的演变史：单体架构、分布式架构、微服务架构，中台架构

        - 1.单体架构：

            - 在Web应用发展早期，大部分工程都是将所有的服务和功能模块打包到一个单一的应用中，如以War包的形式运行在Tomcat进程中，直接与数据库和文件系统交互。

            - 一般一台服务器、一个应用、一个数据库，就足够支撑起一个单一的业务功能。比如电商业务，登录、下单、商品、库存都在一个单一的应用中进行管理和维护。

            - 随着业务的不断增长，用户的访问越来越多，单一应用对磁盘、CPU、内存、数据库的访问要求也越来越高。一台服务器一个应用的配置开始捉襟见肘，更改任何一个小的功能模块，整个应用都要重新进行编译和部署。

                - 整体的功能耦合性非常大，一个小功能的变动可能会引起整个应用不可用。多种功能的强耦合迫使单体架构走向分布式架构。

        - 2.分布式架构：即将1台服务器分散扩容为N台，分而治之

            - 如何保证用户的请求均匀分散到这N台服务器？倘若用户的流量仍然集中访问其中的某台服务器，这样的分布式架构在本质上与单体架构没有任何区别。要解决这个问题就必须增加一个新模块—负载均衡

            - 从单一架构的大单一职责，拆分出一些大的应用，逐步形成多种服务之间的分布式调用。还是以电商为例，这里可能会拆分出用户服务、订单服务、商品服务、库存服务四大应用，应用之间通过接口进行交互，调用形式可能是REST或者RPC。

            - 优点：
                - 低耦合：有了功能模块的拆分，使用接口进行通信，降低了对数据库的依赖，模块耦合性降低。
                - 职责清晰：把应用拆成若干个子应用后，一般也是由不同团队进行维护的，这样一来，不同团队与应用的职责也就更加清晰了。
                - 稳定性更高：不会因为某一个应用或功能模块出现问题导致整体服务不可用

            - 缺点：系统间的依赖和链路增多，会增加接口开发的工作量，同时增大服务之间的维护成本

        - 3.微服务架构：在分布式架构的基础上对应用架构进行更细粒度的拆分

            - 随着Spring Cloud的普及，微服务架构逐步成为大中型企业的主流架构。

            - 优点：
                - 耦合性进一步降低：模块更独立，功能拆分更加细化，使代码间的耦合以及数据库、中间件的耦合进一步降低。
                - 自治性更强：一个微服务就是一个独立的实体，它可以独立部署、升级，微服务与微服务之间通过REST等标准接口进行通信，微服务只与其上下游有关，各个微服务之间更加独立。
                - 技术独立：各个微服务之间可以用不同的技术栈，服务端应用可以用Java、Go、Python等多种语言实现，数据库可以是MySQL、MongoDB、HBase等不同的类型。
                - 高可用：随着微服务增多、链路增长，异常也会被分散，一个微服务异常可以通过线程池隔离，利用熔断等技术避免故障扩散和雪崩，大大增加了整个系统的高可用性。

            - 缺点：
                - 复杂度高：采用RPC或REST等方式进行交互，需要考虑网络抖动、消息丢失、幂等、分布式事务等问题，代码的逻辑处理更加复杂。
                - 粒度难定义：微服务拆成几个合适？什么样的功能模块需要独立成一个微服务？服务拆分的粒度是不好准确定义的，倘若拆得过粗，不利于服务间解耦；如果拆得过细，则会导致应用爆炸，增加系统的复杂性。
                - 运维复杂度高：微服务的调用关系最终会形成一个大网，故障的定位和排查依托于更加完善的监控报警系统等配套工具。
                - 性能变慢：微服务一般有一个很长的调用链路，链路过长导致整体接口的性能变慢，响应时间（Response Time，RT）会变长。

        - 4.中台架构：本质是进一步提升应用系统的复用性，当组织规模扩大，更多业务场景纷纷涌现时，各部门之间会形成一个个“系统烟囱”。在“系统烟囱”中，重复冗余的功能不断被造出来。
            - 以阿里巴巴为例，淘宝、天猫两个事业部都需要用户管理、商品管理、订单管理等功能，许多业务功能是重复的，如果两个事业部都重复建设，必然会造成极大的资源浪费。

    - 自如的技术演进过程
        - 1.2015年之前，自如以资产应用为主，管理房源信息、合同信息、客户信息，为了快速迭代业务，主语言以PHP为主，代码仓库以SVN来管理。到目前为止，老应用还存在部分未下线的功能，但是历史代码已经达到了1GB。
        - 2.2015年到2018年是架构服务化的阶段，这时自如业务蓬勃发展，长租、短租、优品、家装、服务等多条业务线崛起，各个业务线开始构建独立的专属服务，此时Java开始逐步替代PHP，成为新业务线使用的语言。各个服务间开始通过RPC进行通信。这个阶段自如从单体架构迈向了分布式架构，度过爆发性增长的3年。
        - 3.2018年7月，基础平台成立，自如开始对已有的持续交付流程进行重构，引入大量开源技术栈，如Spring Cloud、Nacos、Pinpoint、Graylog、Apollo等，使各个业务线通用的能力得到下沉，同时建设了第二机房，使自如的架构第一次具备了同城灾备的能力。
        - 4.2019年，自如开始搭建DevOps体系，所有应用运维往SRE（Site Reliability Engineer，站点可靠性工程师）方向转型，开始学习编码，准备为Kubernetes落地储备人才。自如建设了大量的平台功能，如网关、监控报警、配置中心、消息队列平台、权限平台、用户中心等，使技术中台已具雏形。

            - 2019年之前，自如某业务线的系统在30天内出现了13次线上故障，基本达到2天一次的故障频率。 发现当时最迫切的问题是中间件

                - 版本问题：各中心使用的MQ、Elasticsearch、Redis版本都极其老旧。以Elasticsearch为例，当时最新版本已经到了6.x，生产集群使用的还是2.x版本

                - 集群耦合太大：数个中心共用一个MQ、一个Redis实例，经常发生业务部门A的队列拥堵导致业务部门B的业务不可用，一个中间件瘫痪，整个公司的业务停转。经排查发现，这个情形与单体架构相似，原因是历史研发人员为了方便，直接复制中间件配置代码

                - 环境问题：代码分支、环境变量、开关配置经常出现测试环境与生产环境不一致等问题；人工参与过多，很多人为问题导致线上代码污染，进而引发故障。


        - 5.2020年，伴随着容器、Kubernetes的广泛传播，自如对持续交付流程做了颠覆性重构，完全改变了之前的发布部署方式，对环境、分支模型都进行了重新定义，成为整个自如的技术演进过程中一个新的里程碑。

        - 自如前台有多条业务线，如业主、租住、家装、客服等，每条业务线有独自的产研团队进行信息系统的构建，下方有三大中台进行支撑。
            ![image](./Pictures/soft-architecture/自如公司的中台架构.avif)

## 架构师

- [腾讯云开发者：优秀程序员，如何提高架构能力？](https://cloud.tencent.com/developer/article/1722323)

    - 架构发展史：

        - 1.演进就是技术编程框架为核心，展开的一系列规划和解耦部分

        - 2.就进入了高并发、分布式，应对大流量的状态。更加注重的是外围基础设施

        - 3.基于数据的应用架构，也就是越来越多的基于数据的挖掘产生新的应用。

    - 架构是随着整个行业的发展和社会需要去发展的：

        - 1.在 2000 年前后是门户、社交时代，PC 互联网蓬勃爆发的年代，有四大门户。互联网主要是新闻内容传递为主。

            - CDN 蓬勃发展
            - 技术上从编译型语言，逐步过度到动态解释性语言的广泛应用
            - 关系型数据库也开始被广泛应用。开始大量应用缓存，弥补关系型数据库存取能力不足的一些场景需求。

        - 2.移动互联网阶段:需要更强的存储和计算能力。云计算，大规模机器开始出现

        - 3.在 IoT 广泛应用之前不会再有指数级终端设备联网，基础工程能力不再是问题。

            - 在大数据、AI 架构方面发展。比如，如何用图数据库解决复杂关系图谱的问题，GPU 集群、弹性计算、机器学习框架都越来越重要。

    - 架构要根据从用户需求出发

        - 架构师关注业务和功能层面的连接

        - 在不同时期抓住用户当时的核心痛点，演进架构，解决掉用户的这些问题，才能成功。

        - 而不是根据已有的技术能力，YY 出产品功能，然后推给用户，可想而知，这样的产品一定会被用户用脚投票，无论背后的技术架构多么巧妙，业务注定会失败。

    - 系统的可用性：根据当月的不可用时间除以当月的总时间

        - 问题：在低峰期和高峰期挂掉十分钟，对业务的影响可能会相差很大

            - 腾讯在内部计算可用性：从请求的角度出发。被拒绝的请求量，加上超时的请求量，然后除以总的请求量

    - 可用性要考虑亚健康：

        - 我们总是假设系统里面的服务器状态是正常的或故障的。没有考虑亚健康：交换机转发能力下降、CPU 有降频、内存在做 ECC 纠错、，硬盘异常导致 IO 延迟陡增

        - 建设全链路的探测和监控，快速地把这些异常的，处于亚健康状态的节点剔除。

    - 长期方案固然要有，但短期方案也非常重要。不一定需要用最理想和技术方案去解决，但可以借鉴架构的思路。

    - 生产架构本质上也是一种架构：

        - 贝壳业务都是在白天去跑，晚上没有多少人去看房子，这个时候晚上做混沌工程，即使系统短时间出问题影响也没有那么大，还有时间修复。

    - 降级方式取决于这个架构师对于业务的了解：在什么条件下熔断什么样的东西对我们的损失是最小的？这件事情反映了一个架构师是不是对技术有非常深刻的了解

        - 做高可用架构的时候首先技术肯定是要好的。只要技术很好，业务场景不了解的话，会带来非常大的问题

    - 接手一个完全陌生领域的业务系统，比从零开始的项目要难：

        - 错误做法：把整个系统的边际和现在的系统逻辑过程看完，然后把业务梳理完，最后自己把这样的业务和技术重新地、完整地设计出一个新的架构，完全全新，基于他自己思想。

            - 问题：这是一个时间的过程，如果是一步到位的话，很容易会翻船。

                - 最理想的那个好到现实的差：其实它的距离感不仅来自于技术的好坏，还有来自于对业务的理解，以及你对团队的理解，包括来自于你对时间和商业成本的考虑。

                - 那么什么才是最好呢？要随着时间的发展去看，前提条件是第一次要让这个系统能够可用，并且达到商业目标，这是最快要做的，剩下的事情就是心中的理想。

    - 接受一个全新项目时：

        - 错误做法：花至少 1 年时间，比如要花一到两个季度调研某个技术，然后再花半年时间去做相应的落地

        - 正确做法：快速地试错，然后先把这个原型搭出来。看看这个是不是用户要的，高可用、高并发、高性能，哪个方面更重要，就投入相应的资源在上面去做相应的演进。

            - 先把整个代码的关键逻辑和分层结构列出来、画出来，弄清楚有哪些模块，模块之间是怎么通讯的，中间件都有哪些，三方服务有哪些等等。之后再去分析风险点，把风险点、呈现的问题和故障列出来，再去设计合理方案。

            - 架构师首先要了解系统现状、业务现状、团队能力现状，再因地制宜。千万不要拿到一个通用解决方案马上就去实施，要去分析自己的业务情况，是不是真的需要高并发、需要低延迟。

    - 如何提升架构能力：

        - linux发展了那么多年到现在架构的精髓依然被应用在非常多的地方，底层核心的东西是不太会变的，一定要去深入理解。

        - 架构的意义在于我们怎么去理解技术的原理，真正深入计算机原理，计算机是什么，存储是什么，为什么今天这样的东西存在，然后去想像这件事情，不停地在这里探索一系列的东西。

        - 要把这个东西尝试着去开源，放到网上让更多的人使用、验证、给你反馈。这个反馈非常重要，总是闭门造车、没有反馈的话架构能力很难提升。

            - 没有用户的话你就做你系统的用户，可以做很多的机器人测试客户端出来，往你的系统发请求

- [腾讯云开发者：大咖们如何评判优秀架构师？](https://cloud.tencent.com/developer/article/1625249)

    - 优秀的架构师要解决一个实际的问题

        - 这只是优秀的程序员，而不是优秀的架构师：Go 语言本身自带垃圾回收机制，很难干涉具体时间点进行垃圾回收，就容易导致系统实时性不足的问题。为了解决这个问题，团队做了很多方案，最后都没法上线，最终决定把 Go 语言的垃圾回收机制关掉。这样导致的内存泄漏，团队负责人竟然不把它算作泄漏，而是对上级重新 “定义” 为“内存增长”，交易关闭的时候重启就好了。

    - 最好的架构师不是具备了多丰富的知识点，他最需要的特质是折中，他可以在任何场景下都给出优雅的设计方案。

        - 不要去过度设计。比如说当前阶段用微服务 1.0 就可以的话，不用再去强行上微服务 2.0，只要架构能够满足当下的业务需求，同时具备未来一段时间的可扩展性就可以了。

         - 腾讯这样的大公司，架构设计上要关注于高并发

         - 对小型团队来说，架构设计更应满足快速迭代、持续交付的特性。

    - 优秀的架构应该满足的关键就是降本增效：是增加了人力和其他成本，还是让人力、运维等降低了，或者让效率提高了？

    - 要具备一个良好的架构认知的思维模型：

        - 在一个比较低级的思维模型里面，你即使学了再多的知识，在我看来也仅仅是低水平的重复。
        - 需要的是更高维度的认知，更高维度的架构设计能力，能够把某个点打穿打透。

    - 架构师要扛起来四门功课：能多打酱油、能和稀泥、肯背黑锅、敢拉仇恨

    - 要搞明白什么地方是重要：

        - 如果时间充裕，架构过程应该在建模、分层、耦合、调用等环节上做到充分论证讨论。

        - 时间紧任务重，横向比较的点，一定要想明白最重要的事情是什么，最重要的目标是什么，可能会有哪些变化？架构师也是一个决策环节，不一定要去做所有不可能完成的任务。

    - 代码写不好，大概率架构设计也不行，因为架构设计的本质还是结构

        - 如果你代码写得好，那么必然具备能做好拆解的前提条件，不然很难说代码写不好的人架构设计能做好。
        - 另外一点，看源码是很好的一点，一些最火的开源项目的源码，社区讨论的 issue，要多去看多去交流，了解别人是怎么做的。

## 架构安全

- 防拖库：个人和住址也要分开存放，用的时候拿到密钥再去组织，密钥也要动态更新，单独被读取只能是一个无法识别的数据片段。



## DevOps

- [阮一峰：运维的未来是平台工程](http://www.ruanyifeng.com/blog/2023/03/platform-engineering.html)

    - 编写软件和运行软件，其实是两种不同的技能：前者需要熟悉代码，后者需要熟悉服务器。互联网软件发展起来以后，这两种技能就逐渐分家了。

        - 互联网公司的核心资产和竞争力，更多的是代码，而不是运维。所以，公司也有意愿，把更多的力量投入在开发上，逐步压缩专门的运维团队，积极外包尽可能多的基础设施。

        - 问题：写代码的人不了解服务器环境，管理服务器的人不了解代码在干什么，这样不利于做出优秀的产品，也不利于排查问题。

    - DevOps：它等于 Dev（开发）+ Ops（运维）。开发与运维重新合在一起：编写软件的人也要负责运行软件。
        - 问题：DevOps 实际上没有办法取代运维

            - 越来越复杂的业务，注定了系统和基础设施也越来越复杂，同时还必须稳定可靠。普通的开发工程师，根本不可能做到这一点。他既不了解所有基础设施，也达不到专业运维的系统管理水平。

        - 传统运维的2个职责：

            - 1.构建基础架构：硬件的采购、安装、上架、联网这些工作

            - 2.管理运行环境：保障业务软件的运行。

            - DevOps 出现后：
                - 构建基础架构：这一职责逐渐消失，变成了采购云服务
                - 管理运行环境：这一职责则是转给了 DevOps 工程师
                - 问题：谁负责采购和整合云服务？

    - 平台工程：负责采购和整合云服务

        - 平台工程师：云服务纷繁复杂，各种 API、SDK 和配套工具令人眼花缭乱，即使经验丰富的运维工程师也不容易说清楚。因此，需要有专职人员来做出正确决策，选择一套满足需要的云服务，并且负责编写工具，整合所有采购来的云服务，供业务开发使用。

        - 1.基础设施是外包的，以求成本和开发周期最小化。

        - 2.平台工程师负责整合外包的基础设施，构建成一个平台。

        - 3.开发工程师在该平台上，自主搭建和管理运行环境，自己运行代码。

## SRE (Site Reliability Engineering，可靠性工程）

- SRE 的出发点是可用性是成功的先决条件。


- SLO：定义每个服务的用户可以接受的最低可靠性水平，然后将其作为你的 SLO
    - 例子：在一个月之中，99.9% 的请求延迟有在 300ms 内

        - 为什么不是100%？因为服务越可靠，其运营成本就越高。

- SLA:基于 SLO 制定的商业合约。承诺其 SLO 应在一段时间内达到特定水准，若未达到一段时间内保证的目标则会产生惩罚机制，比如向客户退款，或免费提供客户更长的服务订阅时间等。

    - 超出 SLO 会伤害到整体业务团队，因此服务应努力保持在 SLO 内。

## DDD(领域驱动设计)

- [腾讯技术工程：万字长文助你上手软件领域驱动设计 DDD](https://cloud.tencent.com/developer/article/1966457)

- 数据驱动设计（DDD以前的思维模式）：从技术的维度解决业务问题

    - 针对业务需求进行数据建模：根据业务需求提炼出类，然后通过 ORM 把类映射为表结构，并根据读写性能要求使用范式优化表与表之间的关联关系。

        - 得出的数据模型是对业务需求的直接翻译，并没有蕴含稳定的领域知识/规则。

    - 一旦需求发生变化，数据模型就得发生变化，对应的库表的设计也需要进行调整。
        - 从需求穿透到了数据层，中间并没有稳定的，不易变的层级进行阻隔，最终导致系统响应变化的能力很差。

- 协同设计：

    - 数据驱动设计例子：由产品同学提出业务需求，研发同学进行技术方案设计，并编程实现。

        > 缺点：无法形成能够消除认知差异的模型。

        - 一：需求可能是易变的、定制化的，而研发同学在缺少行业经验的情况下，往往会选择直译，即根据需求直接转换为数据模型。

        - 二：而研发同学设计的技术方案，涉及很多的技术细节，产品同学无法从中判断是否与自己提出的业务诉求和产品规划相一致，最终形成认知差异。

        - 三：且认知差异会随着迭代不断被放大，最后系统变成一个大泥球。

        ![image](./Pictures/soft-architecture/DDD-协同设计1.avif)

    - 领域驱动（DDD）例子：

        - 领域专家：具有丰富行业经验和领域知识储备的人，他们能够在易变的、定制化的需求中提炼出清晰的边界，稳定的、可复用的领域概念和业务规则，并携手产品和研发共同构建出领域模型。

        - 领域模型是对业务需求的知识表达形式，它不涉及具体的技术细节（但能够指导研发同学进行编程实现），因此消除了产品和研发在需求认知上的鸿沟。

            - 模型的变更意味着需求变更和代码变更，协作围绕模型为中心。

        ![image](./Pictures/soft-architecture/DDD-协同设计2.avif)

- 领域驱动（DDD）

    - 面对业务需求，先提炼出领域概念，并构建领域模型来表达业务问题，而构建过程中我们应该尽可能避免牵扯技术方案或技术细节。

        - 而编码实现更像是对领域模型的代码翻译，代码（变量名、方法名、类名等）中要求能够表达领域概念，让人见码明义。

    - 系统的复杂性往往并不在技术上，而是来自领域本身、用户的活动或业务服务。当这种领域复杂性在设计中没有得到解决时，基础技术的构思再好也是无济于事。

        - 系统的复杂度体现在三个方面：

            - 1.规模：功能点与功能点之间的的关系
                - 通过子领域，限界上下文，聚合等模式对问题进行拆分和归类，不断收窄问题域，保证聚合边界内所解决的问题集合足够收敛和可控。

            - 2.结构：是否分层？若分层，每层划分的职责边界是否清晰
                - 整体架构的基本管理单元是聚合，它是一个完整的、自治的管理单元，当需要进行服务拆分时，可以直接以聚合作为基本单元进行拆分。

            - 3.变化：分离不易变逻辑和易变逻辑，"以不变应万变"
                - 领域层：不变
                - 领域层以外的应用层和基础设施层：易变。通过调整该层，实现快速响应需求的变化

    - 微服务架构的兴起，大伙惊奇地发现 DDD 是作为划分“微服务边界”的一把利器，并且 DDD 提及的很多设计理念与微服务架构十分契合，因此 DDD 逐渐被开发者们接受并流行起来。毫不夸张地说，了解和学习 DDD 可以算得上是如今软件行业从业者的一门必修课了。

    - 问题空间和解空间

        - 问题空间：表示的是真实世界，是具体的问题、用户的诉求

            - 例子：学生管理系统（SMS）
                > 学校需要构建一个学生管理系统（Student Management System, SMS）。
                > 
                > 通过这个管理系统，学生可以进行选课，查询成绩，查询绩点。
                > 
                > 而老师则可以通过这个系统录入授课课程的成绩。录入的分数会由系统自动换算为绩点，规则如下：若分数>= 90，绩点为4.0；90>= 分数> 80，绩点为3.0；80 >= 分数 > 70，绩点为2.0；70 >= 分数 >= 60，绩点为1.0；成绩< 60，则没有绩点，并邮件通知教务员，由教务员联系学生商榷重修事宜。
                > 
                > 成绩录入后的一周内，若出现录入成绩错误的情况，老师可提交修改申请，由教务员审核后即可完成修改。审核完成后系统会通过邮件告知老师审核结果。一周后成绩将锁定，不予修改。成绩锁定后，次日系统会自动计算各年级、各班的学生的总绩点（总绩点由各门课程的学分与其绩点进行加权平均后所得）。
                > 
                > 而教务员则可以通过该系统发布可以选修的课程。同时，教务员能够查看到各年级，各班的学生的总绩点排名。

            - 统一语言（ubiquitous language）：全局分析阶段对问题空间进行的梳理和分析，形成统一语言，获取问题空间的价值需求以及业务需求。

                - 例子：商品的价格（Price）和商品的金额（Amount），它们本质是同一个东西，但是却有不同的术语表示。
                - 统一语言可以通过词汇表的形式展示，其中词汇表最好还要包含术语对应的英文描述
                ![image](./Pictures/soft-architecture/DDD-统一语言.avif)

                - 精炼循环：我们在提炼领域概念的时候会觉得统一语言定义不合理/有歧义，此时我们就会调整统一语言的定义，并重新进行提炼领域概念。领域专家来主导概念提炼、边界划分等宏观设计，原因就在于领域专家的经验和行业洞见来源于过去已经迭代的无数个精炼循环，因此由这些宏观设计推导出来的领域模型，往往都是非常稳定的。

                - 价值需求分析：

                    - 确定系统范围：确定系统问题空间的边界，明确系统什么该做，什么不该做。结合目标系统当前状态和未来状态进行判断。

                    - 在进行价值需求分析后，我们便能判断是否需要通过 DDD 驱动系统的设计。
                        - 并非任何系统都 DDD，DDD 的核心是解决领域复杂性，若系统逻辑简单，功能不多，引入 DDD 则会得不偿失。在进行价值需求分析后，我们便能判断是否需要通过 DDD 驱动系统的设计。

                - 业务需求分析：

                    - 业务场景：按阶段性的目标划分业务流程

                        - SMS例子：老师修改成绩就分为了老师“提交申请单”，以及教务员“同意申请单”两个场景。

        - 解空间：则是针对问题空间求解后构建的理念世界，其中包括了解决方案、模型等。

        - 战略设计覆盖了问题空间和解空间，而战术设计则聚焦在解空间上。

## 微服务

### 微服务理论

- [邱小侠：微服务架构的理论基础 - 康威定律](https://developer.aliyun.com/article/8611)

    - 康威四大定律：

        - 1.组织的沟通和系统设计之间的紧密联系。对于复杂的系统，聊设计就离不开聊人与人的沟通，解决好人与人的沟通问题，才能有一个好的系统设计。

            - 沟通成本 = n(n-1)/2，沟通成本随着项目或者组织的人员增加呈指数级增长。

                - 博物学家古尔德说：组织体大小很大程度上决定组织形态。蜘蛛真变得像大象一样大，那么它的形态也会变得和大象类似。

                - 亚马逊的Bezos这样比喻：如果2个披萨不够一个团队吃的，那么这个团队就太大了。一般一个互联网公司小产品的团队差不多就是7，8人左右（包含前后端测试交互用研等，可能身兼数职）。

                | 人数            | 复杂度                                |
                |-----------------|---------------------------------------|
                | 5个人的项目组   | 需要沟通的渠道是 5*(5–1)/2 = 10       |
                | 15个人的项目组  | 需要沟通的渠道是15*(15–1)/2 = 105     |
                | 50个人的项目组  | 需要沟通的渠道是50*(50–1)/2 = 1225    |
                | 150个人的项目组 | 需要沟通的渠道是150*(150–1)/2 = 11175 |

            - Dunbar Number（邓巴数）：150个人是人类的维系关系的极限

                | 关系             | 个数      |
                |------------------|-----------|
                | 亲密（intimate） | 朋友: 5   |
                | 信任（trusted）  | 朋友: 15  |
                | 接近（close）    | 朋友: 35  |
                | 临时（casual）   | 朋友: 150 |

                ![image](./Pictures/soft-architecture/Dunbar'Number.avif)

        - 2.时间再多一件事情也不可能做的完美，但总有时间做完一件事情

            - 对于一个巨复杂的系统，我们永远无法考虑周全。Eric Hollnagel认为最好的解决办法是抓主线

                - 有点类似于毛泽东说的《不要四面出击》

            - Eric被一家航空公司请去做安全咨询顾问，复杂保证飞机飞行系统的稳定性和安全性。Eric认为做到安全有两种方式：

                - 1.常规的安全：指的是尽可能多的发现并消除错误的部分，达到绝对安全，这是理想。
                - 2.弹性安全：即使发生错误，只要及时恢复，也能正常工作，这是现实。

                - 对于飞机这样的复杂系统，再牛逼的人也无法考虑到漏洞的方方面面，所以Eric建议放弃打造完美系统的想法，而是通过不断的试飞，发现问题，确保问题发生时，系统能自动复原即可，而不追求飞行系统的绝对正确和安全。

                - 类似于持续集成、敏捷开发：

                    - 对于一个分布式系统，我们几乎永远不可能找到并修复所有的bug，单元测试覆盖1000%也没有用。解决方法不是消灭这些问题，而是容忍这些问题，在问题发生时，能自动回复，微服务组成的系统，每一个微服务都可能挂掉，这是常态，我们只有有足够的冗余和备份即可。即所谓的 弹性设计（Resilience） 或者叫高可用设计（High Availability）。

        - 3.独立自治的子系统减少沟通成本

            - 如果你的团队分成前端团队，Java后台开发团队，DBA团队，运维团队，你的系统就会长成下面的样子

                ![image](./Pictures/soft-architecture/subsystem.avif)

            - 如果你的系统是按照业务边界划分（微服务架构）

                - 定义好系统的边界和接口，在一个团队内全栈，让团队自治，原因就是因为如果团队按照这样的方式组建，将沟通的成本维持在系统内部，每个子系统就会更加内聚，彼此的依赖耦合能变弱，跨系统的沟通成本也就能降低。

                ![image](./Pictures/soft-architecture/subsystem1.avif)

        - 4.分而治之

            - 人多管不过来啊，找几个经理帮我管，我管经理

            - 每个经理都被赋予一定的职责去做大系统的某一小部分，他们和大系统便有了沟通的边界，所以大的系统也会因此被拆分成一个个小团队负责的小系统（微服务）

#### 谁适合微服务？

- 微服务比较适合未来有一定的扩展复杂度，且有很大用户增量预期的应用，说人话就是新兴的互联网公司

    ![image](./Pictures/soft-architecture/microservice.avif)

- [王者荣耀为什么不使用微服务架构？](https://www.zhihu.com/question/359630395/answer/954452799)

    - 游戏的核心在于10 个人之间各种游戏事件的高速网络通信

        - 微服务为了把业务完美拆解，把原来的同一个进程里的模块拆分成不同的服务，显著增加额外的网络开销。

### 微服务架构

- [邱小侠：微服务（Microservice）那点事](https://developer.aliyun.com/article/2764)

    - 分布式：按功能拆分成独立的服务，跑在独立的一般都在独立的虚拟机上

    - API Gateway：提供后台服务的聚合，提供一个统一的服务出口，解除他们之间的耦合

        - 有可能成为单点故障点或者性能的瓶颈。

        ![image](./Pictures/soft-architecture/API-Gateway.avif)

    - 服务之间如何通信？

        ![image](./Pictures/soft-architecture/communication.avif)

        - 同步调用：

            - REST（JAX-RS）
            - RPC（Dubbo）

        - 异步消息调用：

            - 能成为调用之间的缓冲，确保消息积压不会冲垮被调用方

            - 不过需要付出的代价是一致性的减弱，需要接受数据最终一致性

            - 后台服务一般要实现幂等性，因为消息发送出于性能的考虑一般会有重复（保证消息的被收到且仅收到一次对性能是很大的考验）

            - 必须引入一个独立的 broker，如果公司内部没有技术积累，对 broker 分布式管理也是一个很大的挑战。

            | 软件   |
            |--------|
            | Kafka  |
            | Notify |
            | MetaQ  |

    - 负载均衡：每一个服务都是有多个拷贝。一个服务随时可能下线，也可能应对临时访问压力增加新的服务节点。
        - 通过 zookeeper 等类似技术做服务注册信息的分布式管理

            - 服务上线时，服务提供者将自己的服务信息注册到 ZK（或类似框架）
            - 通过心跳维持长链接，实时更新链接信息
            - 服务调用者通过 ZK 寻址，根据可定制算法，找到一个服务，还可以将服务信息缓存在本地以提高性能
            - 当服务下线时，ZK 会发通知给服务客户端

- [腾讯云开发者：日调1000亿，腾讯微服务平台的架构演进](https://cloud.tencent.com/developer/article/1701134)

    - 服务发现：需要一个地方记录 IP

        - 传统架构：通过 IP 和 port 来进行互相调用

        - 微服务架构：使用 K8s 和 docker，每次启动的 IP 也可能会变

            - ServiceB 的每个实例在启动时，会将自身的 IP 和 port 写入到服务发现模块的某个路径，然后 ServiceA 会从这个路径来拉取到想要访问的 ServiceB 的服务提供者的列表。

        - 组件有zookeeper，nacos，Consul等：

            - Consul： 和 Spring 的对接也很成熟，很多中小型公司，特别是比较新的公司很多都会选择 Consul 来作为服务注册发现。
                - 服务注册请求、心跳请求会被翻译成kv，然后存储到server上。
                - 服务发现会将服务注册请求、心跳请求的kv内容合并，然后返回

### 度量方式

- 要以业务价值为导向，而不是研发人员产出为导向

    - 错误例子：以代码量、功能数

    - 正确例子：前置时间（lead time）、周期时间（cycle time）。这些用户推崇的整体质量交付。

### 工程

#### 微信

- [腾讯技术工程：月活 12.8 亿的微信是如何防止崩溃的？](https://cloud.tencent.com/developer/article/2010913)

    - 单体服务，一个事件只用一个请求，但微服务下，一个事件可能要请求很多的服务，任何一个服务过载失败，就会造成其他的请求都是无效的。

    - 如何判断过载？

        - 为什么不使用响应时间？因为响应时间是跟服务相关的，很多微服务是链式调用，响应时间是不可控的，也是无法标准化的，很难作为一个统一的判断依据。

        - 为什么不使用 CPU 负载：CPU 负载高不代表服务过载，因为一个服务请求处理及时，CPU 处于高位反而是比较良好的表现。

        - 通常判断过载可以使用吞吐量，延迟，CPU 使用率，丢包率，待处理请求数，请求处理事件等等。

            - 微信使用在请求在队列中的平均等待时间作为判断标准，就是从请求到达，到开始处理的时间。腾讯微服务默认的超时时间是 500ms，通过计算每秒或每 2000 个请求的平均等待时间是否超过 20ms，判断是否过载，这个 20ms 是根据微信后台 5 年摸索出来的门槛值。

    - 过载保护策略：

        - 1.业务优先级

            ![image](./Pictures/soft-architecture/wechat-priority.avif)

        - 2.用户优先级

            > 微信分了几十个业务优先级，每个业务优先级下有 128 个用户优先级，所以总的优先级是几千个。

            - 通过 hash 用户唯一 ID，计算用户优先级，为了防止出现总是打豆豆的现象，hash 函数每小时更换
            - 为啥不采用会话 ID 计算优先级呢？采用会话 ID 在用户重新登录时刷新，用户会养成坏习惯，在服务有问题时就会重新登录，这样无疑进一步加剧了服务的过载情况。

        - 3.自适应优先级调整

#### QQ音乐

- [腾讯云开发者：优雅应对故障：QQ音乐怎么做高可用架构体系？](https://cloud.tencent.com/developer/article/2206300)

    - 1.异地双中心：

        - 深圳和上海各有API-Gateway（接入层）：从而实现STGW（腾讯基于 Nginx 自研的支持大规模并发的七层负载均衡服务）

            ![image](./Pictures/soft-architecture/qq-music.avif)

        - 逻辑层：深圳读/写；上海只读，上海的写请求由API网关路由到深圳中心处理

        - 存储层：深圳写入存储，通过同步中心/存储组件同步到上海。同步组件为[Cmongo（腾讯研发的MongoDB）](https://github.com/Tencent/CMONGO)和CKV+（腾讯自研的分布式kv数据库）

            - [CKV+之进化历程](https://cloud.tencent.com/developer/article/1387361)
                - 兼容redis协议
                - 多租户模式
                - CKV+在最终一致的数据同步基础上，引入了基于Raft协议的强一致同步逻辑

    - 2.异地容灾：在API网关上做故障转移，降低客户端参与度。

        ![image](./Pictures/soft-architecture/qq-music1.avif)

        - 1.API网关故障转移：当本地中心API返回失败时（包括触发熔断和限流），API网关把请求路由到异地处理。以此解决API故障的场景。

            - 为防止异地重试流量被压垮，出现双中心雪崩，要有自适应重试方案，在异地成功率下降的时候，取消重试：

                - 引入重试窗口：耗光后，取消重试

        - 2.客户端故障转移：当API网关发生超时的时候，客户单进行异地重试。如果网关有回包，即使API返回失败，客户端也不重试。解决API网关故障的场景。

        - 当双中心的API-Gateway均异常：所有客户端请求冻结一段时间。

    - 3.自适应限流：请求量超出阈值后在主调直接丢弃请求，在集群扩缩容后需要及时更新限流阈值

    - 4.熔断：

        ![image](./Pictures/soft-architecture/qq-music2.avif)

        - 当“统一权限”服务的其中一个依赖服务（比如歌曲权限配置服务）出现故障时，只能被动的等待依赖服务报错或者请求超时，下游连接池会逐渐被耗光，入口请求大量堆积，CPU、内存等资源逐渐耗尽，导致服务宕掉。
        - 而依赖“统一权限”服务的上游服务，也会因为相同的原因出现故障，一系列的级联故障最终会导致整个系统宕掉。

        - 传统熔断器 有三种状态：Closed、Half Open、Open

            - Open状态：拒绝所有请求

            - 进入Closed状态时瞬间会有大量请求，服务端可能还没有完全恢复，会导致熔断器又切换到Open状态，一种比较刚性的熔断策略。

        - SRE熔断器 有两种状态：Closed、Half-Open

            - 根据请求成功率自适应地丢弃请求，尽可能多地让请求成功请求到服务端，是一种更弹性的熔断策略。

            - 熔断器阈值：

                - 正常情况下 requests（窗口时间内请求数） = accepts（正常处理的请求数） 时丢弃概率为0。
                - K：敏感度，K越小丢弃概率越大，一般在1.5-2之间

                - requests不断减少直到 requests 等于 K * accepts 时：熔断器就会打开，并按照概率丢弃请求。

            - QQ音乐采用SRE熔断器

    - 动态超时

        - 微服务架构的演进，超时逐渐被标准化到RPC中

        - 传统超时会设定一个固定的阈值，响应时间超过阈值就返回失败。

        - 微服务基于EMA算法动态调整超时时长。

            - [EMA算法](https://github.com/jiamao/ema-timeout)是综合利用历史上积累到的数据，预测下一个周期内的期望。EMA算法引入“平均超时”的概念，用平均响应时间代替固定超时时间，只要平均响应时间没有超时即可，而不是要求每次都不能超时。

    - 服务分级

        - 每日邮件推送1级服务和2级服务的观测数据

        - 为针对1级服务和2级服务制定SLO（达到某一段时间内的目标数值），签订SLA（无法完成商业承诺，就付出代价）

        - API-Gateway根据服务分级限流，优先确保1级服务通过

        | 等级                                                           | 例子                                         |
        |----------------------------------------------------------------|----------------------------------------------|
        | 1级 ：如果出现故障会导致用户或业务产生重大损失                 | 登录服务、流媒体服务、权限服务、数专服务等。 |
        | 2级 ：如果出现故障会导致用户体验受到影响，但是不会完全无法使用 | 排行榜服务、评论服务等。                     |
        | 3级 ：不容易注意或很难发现                                     | 用户头像服务，弹窗服务等。                   |
        | 4级 ：即使失败，也不会对用户体验造成影响                       | 比如红点服务等。                             |

    - 工具链：在故障触发之前，尽可能多地识别风险，针对性地加固和防范，而不是等着故障发生。

        - 混沌工程：通过注入网络超时等故障，主动找出系统中的脆弱环节

            - QQ音乐使用ChaosMesh结合其他组件打造的混沌工程平台

        - 全链路压测：通过注入流量给系统施加压力的方式，来发现系统的性能瓶颈

        - Prometheus + Grafana做可视化展示

            - 秒级监控：基于Prometheus构建联邦集群，每3秒抓取一次数据，实现了准实时监控。并对活动进行快照采集，记录活动发生时所有微服务的请求峰值

            - 历史数据回溯：当我们需要回溯近一个月甚至一年前的指标趋势时，性能是个极大挑战。由于历史数据的精度要求不高。通过Prometheus联邦进行阶梯降采样，可以永久存放历史数据，同时也极大降低存储成本。
        - Logging

            - ELK（ElasticSearch、Logstash、Kibana）构建日志处理平台

                ![image](./Pictures/soft-architecture/qq-music-Logging.avif)

            - Filebeat 作为日志采集和传送器。Filebeat监视服务日志文件并将日志数据发送到Kafka。Kafka 在Filebeat和Logstash之间做解耦。
            - Logstash 解析多种日志格式并发送给下游。ElasticSearch 存储Logstash处理后的数据，并建立索引以便快速检索。
            - Kibana 是一个基于ElasticSearch查看日志的系统，可以使用查询语法来搜索日志，在查询时制定时间和日期范围或使用正则表达式来查找匹配的字符串。

        - Tracing追踪：一个客户端请求由系统中大量微服务配合完成处理，这增加了定位问题的难度。

            - Tracing在触发第一个调用时生成关联标识Trace ID，我们可以通过RPC把它传递给所有的后续调用，就能关联整条调用链。Tracing还通过Span来表示调用链中的各个调用之间的关系。

            - QQ音乐基于jaeger构建分布式链路追踪系统，实现分布式架构下的事务追踪、性能分析、故障溯源、服务依赖拓扑。

                - 1.jaeger client发送的spans通过jaeger-agent（代理）转发到jaeger-collector。
                - 2.jaeger-collector 接收后，验证和清洗数据后转发至kafka。
                - 3.jaeger-ingester 从kafka消费数据，并存储到ElasticSearch。
                - 4.jaeger-query 封装用于从ElasticSearch中检索traces的APIs。

                ![image](./Pictures/soft-architecture/qq-music-jaeger.avif)

        - profile：基于[conprof](https://github.com/geekgao/conprof)搭建持续性能分析系统

        - Dumps

            - core dumps：在进程崩溃时把进程内存写入一个镜像中以供分析，或者把panic信息写到日志中

                - 在容器环境中实施困难，panic信息写入日志则容易被其他日志冲掉且感知太弱。

            - QQ音乐使用的方式是在RPC框架中以拦截器的方式注入，发生panic后上报到sentry平台。

#### 携程

- [腾讯云社区：计算压力倍增，携程度假起价引擎架构演变](https://cloud.tencent.com/developer/inventory/334/article/1625083)

    - 任务队列：旅游产品不像单品售卖，产品价格是由多种资源组成（机票、酒店、火、X等）,任意一项资源价格、库存发生变化，都会导致整包价改变，变量太多。

        - 任务队列在算完机票资源之后，再算酒店，算完酒店后进行单选项资源处理，最终汇总出价格。

        - 1.0版本：

            - 任务队列：

                - 用MySQL做任务队列，并且针对每个任务队列同步到sqlserver。分了2个库，64张表

                - 之后用Redis的根据资源分成多队列，代替mysql

        - 2.0版本：

            - 任务队列：Kafka代替redis

            - 任务生成：我们引入了分布式计算框架Spark，从原有的.net代码体系切换到Java

            - 线路聚合：

                - 航线1北京->上海和航线2上海->北京->上海，进行聚合，从而减少请求

        - 3.0版本：

            - 任务生成：首先是消息对产品进行解析，拿到产品的出发地、出发日期，再结合其他的信息，把这些相关的信息写到分布式文件系统里面，再通过Spark进行不同资源的聚合排序，然后再把它写回到分布式文件系统里面，接着通过某一个job去把这些信息取出来，然后把消息发送出去。

                - HDFS分布式文件系统把任务信息分发到下一步的这一个步骤简化了，在消息发送的时候，直接通过Spark进行消息发送，比如它里面可能有几百个节点，我们这几百个节点同时发送这个消息

            - 数据库：HBase替换MySQL

            - 依然存在的瓶颈问题：价格库存依赖外部的资源。外部资源的变化对系统是黑盒，只能依赖API离线计算，API的调用量也会有限制等。也就是活包

        - 缓存预热：对外的价格日历班期查询是有用到缓存的，整个缓存我们相当于是7×24小时不间断一直在写入，所以其实也不怎么存在预热的问题。

- [腾讯云社区：日均20亿流量：携程机票查询系统的架构升级](https://cloud.tencent.com/developer/article/1624858)

    - 三个独立的数据中心

    - SpringCloud + K8s + AWS云服务

    - AI的应用：

        - 反爬虫：屏蔽掉9%的流量

        - 查询筛选：在聚合服务中找出价值最高实际用户，然后把他们的请求发到引擎当中。对于一些实际价值没有那么高的，更多的是用缓存

        - 智能设置TTL生命周期：TTL与业务密切相关的

            - 缓存一致性例子：刚刚看到一个低价机票，点进去就没有了。这种情况出现的原因可能是什么呢？大家知道，航空公司的低价舱位票，一次可能就只放出来几张，如果是热门航线，可能同时有几百人在查询。所以，几百人都可能会看到这几张票，它就会出现在缓存里边。如果已经有10个人去订了票，其他人看到缓存再点进去，运价就已经失效了。

            - 解决方法：
                - 1.缓存超过固定阈值就强行清除。
                - 2.动态刷新：在达到固定阈值之前，有用户查询就刷新缓存


        ![image](./Pictures/soft-architecture/携程-机票查询-AI应用.avif)

    - 缓存架构：

        - 一级缓存是最终的结果，二级缓存是中间结果

            - 聚合服务需要多个返回结果的话，那么很大程度上都是先读一级缓存，一级缓存没有命中的话，再从二级缓存里面去读中间结果

        ![image](./Pictures/soft-architecture/携程-机票查询-缓存架构.avif)

        - 一级缓存：

            - 使用了Redis
            - 固定的TTL，一般低于5分钟的，一些场景下可能只有几十秒

        - 二级缓存：

            - 一开始使用MongoDB，后来改为Redis
            - 通过AI设定的TTL，使得命中率提升了27%

        - 缓存预热：一个分布式的，可能有一小部分节点，比如要下线或者什么，但是对整个缓存机制来说影响很小，然后这一部分请求又分散到我们的多个服务器上，几乎不会产生太大的抖动的。

    - 负载均衡Pooling：我们有一些计算非常密集的引擎，存在一些耗时长，耗费CPU资源比较多的子任务，同时这些子任务中可能夹杂着一些实时请求，所以这些任务可能会留在线程里边，阻塞整个流程。

        - 把子任务放在queue里，将节点作为worker，总是动态的去取，每次只取一个，计算完了要么把结果返回，要么把中间结果再放回queue。

            - queue基于Redis队列，有的键值里加入了IP地址

                - 为什么不使用消息队列？由于它存在很明显的顺序性，不能够基于键值去读到你所写的，比如你发送了一个子任务，这时候你要定时去拿这个结果，但是你基于其他的消息队列或者内存队列是没法拿到的

            - 如果有任何实时的外部调用，我们就可以把它分成多次，放进queue进行task的整个提交执行和应用结果的返回。

        ![image](./Pictures/soft-architecture/携程-机票查询-负载均衡.avif)

        - Pooling的过载保护：

            - 如果没有过载保护，很容易就会发生滚雪球效应，queue里面的任务越来越多，当系统取到一个任务的时候，实际上它的原请求可能早就已经timeout了。

            - 当流量实在太高的情况下，把等待时间超过某一个阈值的请求全都扔掉。

            ![image](./Pictures/soft-architecture/携程-机票查询-过载保护.avif)

# 客户端架构

- [B站PC客户端-架构设计](https://www.bilibili.com/read/cv22750308?spm_id_from=333.999.0.0)

    - Electron = Chromium + nodejs + nativeAPI（系统对话框、系统托盘、系统菜单、剪切板等）

    - Electron运行时：主进程 (Main Process)  + 渲染进程(Render Process)
        - 主进程： NodeJs 和原生 API
        - 渲染进程：前端技术
        - 主进程和渲染进程之间通过 IPC 进行通信
        ![image](./Pictures/soft-architecture/electron-runtime.avif)

    - B站的Electron方案：

        - 渲染进程和主进程都是基于 TypeScript 进行开发的

            - 主进程和渲染进程的中间，设计了一个 common 共享层，用来实现 Ts 类型和常量的共享。

        - 主进程： NodeJs + NestJs + Esbuild

            - NestJs：是一个受 Angular 启发的 NodeJS 后端框架，风格有点像 SpringBoot

                - 结合 OOP (Object Oriented Programming)、FP (Functional Programming) 和 FRP (Functional Reactive Programming) 等编程范式，能够开发出高可测、好拓展、松耦合、易维护的应用。

            - Esbuild：使用 go 语言实现的 js 极速打包工具

        - 渲染进程：Vue3 + Vite2 + TypeScript

            - 并且还应用了B站自研的 Vue3 组件库 vivid-ui、函数库 @bilibili/b-utils 和样式库 @bilibili/b-style 等工具库。

        - 根据业务功能，结合框架和技术栈的整体架构：

            - 渲染进程：
                - 账号登录、大会员充值、直播间和开播页都是使用 webview 嵌 WEB 页的方式接入的
                - 其它业务模块均为本地应用页面

                - 渲染层是基于 Vue 开发的 SPA 应用，主窗口和播放窗口在打开时会有一个比较耗时的加载过程，我们专门针对窗口创建和打开过程做了优化。
                    - 1.在启动时（首次打开主窗口），会先创建一个隐藏的主窗口，加载一个相对简单的开屏页。主窗口和开屏页显示之后，在主窗口开屏页下方加载和渲染比较大的主页面。

                    - 2.主页面渲染完成时，调用主进程 mainWindowReady 接口，并关闭盖在主页面上面的开屏页。主进程收到 mainWindowReady 后，创建一个隐藏的播放窗口。

                    - 3.当用户点击播放视频时，显示已渲染好的播放窗口，并加载和播放视频。关闭主窗口和播放窗口时，只将窗口隐藏并不真实销毁窗口实例，当再次打开或播放视频时，就可以快速打开窗口，提升用户体验。

                    ![image](./Pictures/soft-architecture/b-architecture-窗口优化.avif)

                - “节能模式”：渲染进程占用资源比较大，如果用户长时间未使用又没有关闭的话就会造成资源浪费，当 APP 隐藏到托盘超过一定时间未点开后会进入到此模式
                    - 会杀掉主窗口和播放窗口对应的渲染进程，释放资源占用。
                    - 再次打开时，由于没有 APP 初始化的任务，速度会比冷启动时要快很多。

            - 主进程：

                - 本地日志、本地存储和下载 SDK 都是引入的模块，其它各个模块和服务之间通过依赖注入和 rxjs 主题订阅的方式实现相互调用

                - 一个窗口页面其实本质上就是一个 HTML 页面。
                    - 主页面都是从由主进程创建的本地服务器上拉取的 Local 页面，因此断网的情况也能响应
                    - 首页、动态网络强相关的页面，在断网时会显示失败提示
                    - 离线缓存、设置等功能都能正常使用。

            - JSB 则由主进程通过 preload 的方式注入到渲染进程各页面的 window 对象中，渲染进程不论是外嵌还是本地页面都能访问到 JSB 对象实现和主进程通信。

            ![image](./Pictures/soft-architecture/b-architecture-electron.avif)

                - 主要包含 7 个常跓进程：
                    - 1 个浏览器 (Browser) 进程
                    - 1 个 GPU 进程
                    - 1 个网络服务 (Network Service) 进程
                    - 1 个音频服务 (Audio Service) 进程
                    - 3 个页面 (Tab) 进程。
                    - 通过主进程 app.getAppMetrics()方法获取 APP 各进程的 CPU 和内存数据统计

        - 开发平台工具
            - Fawkes 平台打包构建
            - 北极星进行数据上报并在观远建立数据报表
            - 魔镜平台进行自动化巡检测试
            - 在 info.bilibili.co（企业微信） 上建有专门的产品和开发文档，开发方面也有明确的代码规范要求
            ![image](./Pictures/soft-architecture/b-architecture-开发平台工具.avif)

        - 构建都基于 NPM Script， 由于构建产物也是 js 代码，很容易被人拿去套壳、植入或篡改。

            - 对构建包进行加密处理：实现了一套使用结合代码混淆、对称加密、字节码处理、WASM 解密和啥希校验的组合式客户端加密防破解方案。

                - 在打 Release 包时，会把构建生成的代码先进行混淆、压缩处理，再将主进程主程序代码使用 AES-256 对称加密生成 / app/main/.biliapp 文件，将解密和执行入口使用字节码处理，最后计算出 APP 的哈希值。

                - 当 PC 客户端启动时，会先根据操作系统和系统架构找到对应字节码入口、解密并运行主程序、主程序中校验 APP 的哈希值，在确定运行环境安全后再正常启动程序。

                ![image](./Pictures/soft-architecture/b-architecture-防破解.avif)

        - 增量更新：

            - 基于 electron-builder 进行打包，Mac 的安装窗口通过 electron-builder 配置可以直接生成，Windows 的专属一键三连安装程序则是使用 NSIS + QT 独立开发的。
                ![image](./Pictures/soft-architecture/b-architecture-打包.avif)

            - 前期版本基于 electron-updater 进行升级更新。在 1.9.x 版本之后，只需对 app.asar 代码包进行更新而不是包含框架的完整安装包

        - 双窗口模式：

            - 主窗口和播放窗口是独立的，类似爱奇艺和腾讯视频，可以让用户一边播放视频一边刷动态、逛空间、看消息。

            - PC客户端对比WEB：

                - 播放质量：PC 客户端的首帧和 VV 卡顿率会优于 WEB，主要原因为能更多预载，单实例，编码支持统一

                - 百分钟卡顿率和错误率：PC 客户端相对 WEB 来说要略高一些，新平台有更多 case 需要处理，此项会略高但重试机制尽量不影响体验。

                - PC 客户端的播放器技术：

                    - 直接用上了WEB 强大的 Nano 播放器，拥有了高级弹幕、播放设置、播放器快捷键等功能。

                    - 还将 UGC 和 OGV 播放器集成在同一个播放窗口，让视频类型切换更加顺畅，还可以通过前进后退播放历史纪录。

# 新技术的危害

- 企业软件的成本，只有20%是早期的开发成本，剩下的80%都是后期的维护和更新成本。

- 很多的新技术，看上去可以节省前面20%的开发成本，但可能大大增加后面80%的维护成本。
