
<!-- mtoc-start -->

* [非IT事故处理](#非it事故处理)
  * [切尔诺贝利](#切尔诺贝利)
* [公有云相关事故](#公有云相关事故)
  * [2018年6月27日：阿里云出现大范围故障](#2018年6月27日阿里云出现大范围故障)
  * [2022年12月18日：阿里云香港机房节点发生故障](#2022年12月18日阿里云香港机房节点发生故障)
  * [2023年11月12日：阿里云发生故障，影响所有地区，影响所有服务，持续时长 100 分钟。](#2023年11月12日阿里云发生故障影响所有地区影响所有服务持续时长-100-分钟)
  * [2024年4月8日：腾讯云发生的一场全球性的大故障，该故障波及全球17个区域与数十款服务。74分钟](#2024年4月8日腾讯云发生的一场全球性的大故障该故障波及全球17个区域与数十款服务74分钟)
  * [2024年7月2日：阿里云光缆被挖断](#2024年7月2日阿里云光缆被挖断)
* [机房相关事故](#机房相关事故)
  * [2023年3月29日：微信、QQ等腾讯旗下社交软件出现功能异常。](#2023年3月29日微信qq等腾讯旗下社交软件出现功能异常)
  * [2023年3月29日：唯品会机房宕机12小时](#2023年3月29日唯品会机房宕机12小时)
  * [2023年11月2日：Cloudflare 的控制平面和分析服务出现故障不可用，原因是一个数据中心机房 (PDX-DC04) 发生电源故障。](#2023年11月2日cloudflare-的控制平面和分析服务出现故障不可用原因是一个数据中心机房-pdx-dc04-发生电源故障)
  * [2024年9月10日：阿里机房火灾](#2024年9月10日阿里机房火灾)
* [网络相关事故](#网络相关事故)
  * [2021年10月4日：Facebook宕机7小时](#2021年10月4日facebook宕机7小时)
  * [2023年6月8日：广东电信5小时的大面积断网事件](#2023年6月8日广东电信5小时的大面积断网事件)
  * [2024年1月11日：《英雄联盟》《王者荣耀》《和平精英》等多款腾讯旗下游戏出现服务器崩溃、掉线的问题。](#2024年1月11日英雄联盟王者荣耀和平精英等多款腾讯旗下游戏出现服务器崩溃掉线的问题)
* [linux相关事故](#linux相关事故)
  * [2023年10月23日：语雀突发 P0 级事故！宕机 8 小时](#2023年10月23日语雀突发-p0-级事故宕机-8-小时)
  * [dbaplus社群：自己亲手引发运维事故是一种怎样的体验？](#dbaplus社群自己亲手引发运维事故是一种怎样的体验)
  * [dbaplus社群：官方自爆了！去年今天的B站原来是这样崩溃的……](#dbaplus社群官方自爆了去年今天的b站原来是这样崩溃的)
  * [dbaplus社群：重大事故！IO问题引发线上20台机器同时崩溃](#dbaplus社群重大事故io问题引发线上20台机器同时崩溃)
* [kubernetes相关事故](#kubernetes相关事故)
  * [2023年11月27日：滴滴故障](#2023年11月27日滴滴故障)
* [数据库相关事故](#数据库相关事故)
  * [dba保命原则](#dba保命原则)
  * [dbaplus社群：自己亲手引发运维事故是一种怎样的体验？](#dbaplus社群自己亲手引发运维事故是一种怎样的体验-1)
  * [dbaplus社群：自己亲手引发运维事故是一种怎样的体验？](#dbaplus社群自己亲手引发运维事故是一种怎样的体验-2)
  * [dbaplus社群：不小心删了公司数据库，停机45分钟来得及跑路吗……](#dbaplus社群不小心删了公司数据库停机45分钟来得及跑路吗)
  * [dbaplus社群：又一名企遭遇P0级重大故障，竟是删库的戏码……](#dbaplus社群又一名企遭遇p0级重大故障竟是删库的戏码)
  * [dbaplus社群：同事用 insert into select 迁移数据，上线后被开除了……](#dbaplus社群同事用-insert-into-select-迁移数据上线后被开除了)
  * [dbaplus社群：数据库缓存没预热会怎样？我帮大家逝了下……](#dbaplus社群数据库缓存没预热会怎样我帮大家逝了下)
* [非技术性的IT从业人员事故](#非技术性的it从业人员事故)
  * [临时工说：搞数据库 光凭的是技术，那DBA的死多少次？](#临时工说搞数据库-光凭的是技术那dba的死多少次)

<!-- mtoc-end -->

# 非IT事故处理

## 切尔诺贝利

- 切尔诺贝利事故
    - RBMK 反应堆存在根本技术问题。
    - 以上技术问题并未有效传达。此前发生过涉及该技术问题的事故，但切尔诺贝利团队并不熟悉。
    - 安全检查期间，团队没有按照规范程序操作。
    - 切尔诺贝利核电站爆炸后，苏联政府试图掩盖事实，加剧了损害程度。

- 谁应该负责？

    - 反应堆设计师？其他电厂团队未准确传达所遇到的问题？切尔诺贝利团队？苏联政府？

    - 所有人都难逃其咎。灾难从来都不是由单一错误导致，而是因一系列错误发生。我们的工作就是尽早打断这一错误链条，并尽力做到最好。

# 公有云相关事故

## 2018年6月27日：阿里云出现大范围故障

- 事故影响：
    - 手机端和 PC 端都无法访问，时间持续一个多小时，影响范围包括阿里云官网控制台，以及 MQ，NAS，OSS 等产品功能。也有用户反映阿里巴巴、淘宝、滴滴和石墨文档等产品也出现了服务不稳定的情况，据说金融云也出现故障。

    - 资深技术专家陈皓在微博 @左耳朵耗子上也发表了自己的看法：阿里云出故障了，任何技术人员都会知道故障不可避免，对于故障我们应该给予更多的理解。只是，希望阿里云不要处理工程师，因为惩罚事故责任人完全没有意义。系统的错误往往来自于团队的工程错误，应该改善技术工程手段或软件设计，就算是人没招对，也怪招聘过程，而事故责任人反而是最无辜的……

- 事故原因：

    - 关于技术上的原因和过程，一位 twitter网友称：阿里云的函数计算挂了，导致线上故障。打算马上降级到本地计算，结果阿里云的 Kubernetes 也挂了。想着挨个机器手工改一下，发现 OSS 也挂了…整个过程没有报警，因为 SLS 也挂了。

    - 官方说明：当天下午，工程师团队在上线一个自动化运维新功能中，执行了一项变更验证操作。这一功能在测试环境验证中并未发生问题，上线到自动化运维系统后，触发了一个未知代码bug。错误代码禁用了部分内部IP，导致部分产品访问链路不通。 后续人工介入后，工程师团队快速定位问题进行了恢复。

## 2022年12月18日：阿里云香港机房节点发生故障

- [通信电源人：案例|阿里云（香港节点瘫痪）突发事件回顾和复盘](https://mp.weixin.qq.com/s/GdTCjvOgWcP8KnZ_48fAlg)

- 事故影响：

    - 阿里云香港的 C 可用区出现严重故障，致使托管在此的 Linux 中国的官网（https://linux.cn/）至今（现在是 15:30）不能访问。
    - 澳门多个关键基础设施网站受阿里云故障影响，今日 中午起无法访问使用，包括政府、传媒的网站和应用程式。
        - 澳门司警表示，网络安全事故预警及应急中心接报，因阿里云的香港机房节点发生故障，导致澳门金融管理局、澳门银河、莲花卫视、澳门水泥厂等关键基础设施营运者的网站、澳觅和mFood等外卖平台、以及澳门日报等本地传媒应用程式，自今日中午起暂时无法访问使用。网安中心已联系相关关键基础设施营运者并跟进。

- 事故原因：

    - 08:56：阿里云监控到香港Region可用区C机房包间通道温控告警，阿里云工程师介入应急处理，通知机房服务商进行现场排查。

    - 09:01：阿里云监控到该机房多个包间温升告警，此时工程师排查到冷机异常。

    - 09:09：机房服务商按应急预案对异常冷机进行4+4主备切换以及重启，但操作失败，冷水机组无法恢复正常。

    - 09:17：依照故障处理流程，启动制冷异常应急预案，进行辅助散热和应急通风。尝试对冷机控制系统逐个进行隔离和手工恢复操作，但发现无法稳定运行，联系冷机设备供应商到现场排查。此时，由于高温原因，部分服务器开始受到影响。

    - 10:30：为避免可能出现的高温消防问题，阿里云工程师陆续对整个机房计算、存储、网络、数据库、大数据集群进行降载处理。期间，继续多次对冷机设备进行操作，但均不能保持稳定运行。

    - 12:30：冷机设备供应商到场，在多方工程师诊断下，对冷塔、冷却水管路及冷机冷凝器进行手工补水排气操作，但系统仍然无法保持稳定运行。阿里云工程师对部分高温包间启动服务器关机操作。

    - 14:47：冷机设备供应商对设备问题排查遇到困难，其中一个包间因高温触发了强制消防喷淋。

    - 15:20：经冷机设备商工程师现场手工调整配置，冷机群控解锁完成并独立运行，第1台冷机恢复正常，温度开始下降。工程师随后继续通过相同方法对其他冷机进行操作。

    - 18:55：4台冷机恢复到正常制冷量。

    - 19:02：分批启动服务器，并持续观察温升情况。

    - 19:47：机房温度趋于稳定。同时，阿里云工程师开始进行服务启动恢复，并进行必要的数据完整性检查。

    - 21:36：大部分机房包间服务器陆续启动并完成检查，机房温度稳定。其中一个包间因消防喷淋启动，未进行服务器上电。因为保持数据的完整性至关重要，工程师对这个包间的服务器进行了仔细的数据安全检查，这里花费了一些必要的时间。

    - 22:50：数据检查以及风险评估完成，最后一个包间依据安全性逐步进行供电恢复和服务器启动。

- 优化改进：

    - 1.冷机系统故障恢复时间过长

        - 原因分析：机房冷却系统缺水进气形成气阻，影响水路循环导致4台主冷机服务异常，启动4台备冷机时因主备共用的水路循环系统气阻导致启动失败。水盘补水后，因机房冷却系统的群控逻辑，无法单台独立启动冷机，手工修改冷机配置，将冷机从群控调整为独立运行后，陆续启动冷机，影响了冷却系统的恢复时长。整个过程中，原因定位耗时3小时34分钟，补水排气耗时2小时57分钟，解锁群控逻辑启动4台冷机耗时3小时32分钟。

        - 改进措施：全面检查机房基础设施管控系统，在监控数据采集层面，扩大覆盖度，提升精细度，提高对故障的排查和定位速度；在设施管控逻辑层面，确保系统自动切换逻辑符合预期，同时保证手工切换的准确性，防止内部状态死锁从而影响故障的恢复。

    - 2.现场处置不及时导致触发消防喷淋

        - 原因分析：随着机房冷却系统失效，包间温度逐渐升高，导致一机房包间温度达到临界值触发消防系统喷淋，电源柜和多列机柜进水，部分机器硬件损坏，增加了后续恢复难度和时长。

        - 改进措施：加强机房服务商管理，梳理机房温升预案及标准化执行动作，明确温升场景下的业务侧关机和机房强制关电的预案，力求更简单有效，并通过常态化演练强化执行。

    - 3.客户在香港地域新购ECS等管控操作失败

        - 原因分析：ECS管控系统为B、C可用区双机房容灾，C可用区故障后由B可用区对外提供服务，由于大量可用区C的客户在香港其他可用区新购实例，同时可用区C的ECS实例拉起恢复动作引入的流量，导致可用区 B 管控服务资源不足。新扩容的ECS管控系统启动时依赖的中间件服务部署在可用区C机房，导致较长时间内无法扩容。ECS管控依赖的自定义镜像数据服务，依赖可用区C的单AZ冗余版本的OSS服务，导致客户新购实例后出现启动失败的现象。

        - 改进措施：全网巡检，整体优化多AZ产品高可用设计，避免出现依赖OSS单AZ和中间件单AZ的问题。加强阿里云管控平面的容灾演练，进一步提升云产品高可用容灾逃逸能力。

    - 4.故障信息发布不够及时透明

        - 原因分析：故障发生后阿里云启动对客钉群、公告等通知手段，由于现场冷机处理进展缓慢，有效信息不够。Status Page页面信息更新不及时引发客户困惑。

        - kk改进措施：提升故障影响和客户影响的快速评估和识别拉取能力。尽快上线新版的阿里云服务健康状态页面（Status Page），提高信息发布的速度，让客户可以更便捷地了解故障事件对各类产品服务的影响。

## 2023年11月12日：阿里云发生故障，影响所有地区，影响所有服务，持续时长 100 分钟。

- [天空的代码世界：2023-11-12阿里云故障复盘与分析](https://mp.weixin.qq.com/s/l6WViDXAWb53VYWF01bGBA)

- 事故损失：10亿

- 事故原因：

    - 北京时间2023年11月12日17:39起，阿里云云产品控制台访问及管控API调用出现异常、部分云产品服务访问异常，工程师排查故障原因与访问密钥服务（AK）异常有关。工程师修订白名单版本后，采取分批重启AK服务的措施，于18:35开始陆续恢复，19:20绝大部分Region产品控制台和管控API恢复。
        - 点评：为何没有热修复，很奇怪。

    - 访问密钥服务（AK）在读取白名单数据时出现读取异常，因处理读取异常的代码存在逻辑缺陷，生成了一份不完整白名单，导致不在此白名单中的有效请求失败，影响云产品控制台及管控API服务出现异常，同时部分依赖AK服务的产品因不完整的白名单出现部分服务运行异常。
        - 点评：应该增加一种兜底行为来强制热修复。

- 事故处理流程：

    - 2023-11-12 18:14，阿里云官方发公告，17:44 阿里云监控发现云产品控制台访问及API调用出现异常，阿里云工程师正在紧急介入排查。
        - 点评：初步公布受影响的业务场景或功能，从故障到发布公告，耗时 30 分钟。

    - 2023-11-12 18:50 阿里云发公告 17:50 已确认故障原因与某个底层服务组件有关，工程师正在紧急处理中。
        - 点评：告诉大家知道故障模块了，从故障到确认故障模块耗时 6 分钟。

    - 2023-11-12 18:59 阿里云发公告 18:54 经过工程师处理，杭州、北京等地域控制台及API服务已恢复，其他地域控制台及API服务逐步恢复中。
        - 点评：告诉大家已经修复部分服务了，从故障到初步恢复耗时 70 分钟，从发现故障模块到找到解决方法，耗时 64 分钟。

    - 2023-11-12 19:21 阿里云发公告 19:20 工程师通过分批重启组件服务，绝大部分地域控制台及API服务已恢复。
        - 点评: 大部分功能恢复，耗时 97 分钟。

    - 从上次初步恢复，过去了 27 分钟。

    - 猜测这期间阿里云的工程师一直在想如何自动修复问题，发现都不行，最后才不得已选择重启组件服务吧。

    - 果然重启大法是万能的。

    - 2023-11-12 20:05 阿里云发公告 19:43 异常管控服务组件均已完成重启，除个别云产品（如消息队列MQ、消息服务MNS）仍需处理，其余云产品控制台及API服务已恢复。

        - 点评：万能的重启大法重启了大部分服务，大部分功能都恢复了。

        - 可能对于MQ 和 MNS 服务，还在评估重启会产生什么影响，所以暂时没有重启。

    - 2023-11-12 20:24 阿里云发公告 20:12 北京、杭州等地域消息队列MQ已完成重启，其余地域逐步恢复中。
        - 点评：相比 19:43，已经过去 30 分钟，猜测大概率没有直接评估出结论来。最终尝试灰度的方式重启，验证发现可以解决问题，而且没有产生新的问题（如数据丢失），故全部都进行重启。

    - 2023-11-12 21:13 阿里云发公告 21:11 受影响云产品均已恢复，因故障影响部分云产品的数据（如监控、账单等）可能存在延迟推送情况，不影响业务运行。
        - 点评：相比上次重启，又过了一个小时。

    - 可能阿里云设计的产品或功能太多，需要一个个去确认是否修复以及是否有遗漏，最终全部修复。

- 事故影响：受影响的服务列表：

    ![image](./Pictures/事故/阿里云受影响的服务列表.avif)

    - 问题影响范围：OSS、OTS、SLS、MNS等产品的部分服务受到影响，大部分产品如ECS、RDS、网络等运行不受影响；云产品控制台、管控API等功能受到影响

- 优化改进：
    - 增加AK服务白名单生成结果的校验及告警拦截能力。
    - 增加AK服务白名单更新的灰度验证逻辑，提前发现异常。
    - 增加AK服务白名单的快速恢复能力。
    - 加强云产品侧的联动恢复能力。

- 事故分析：

    - 就是访问密钥服务（AK）存在产生脏数据，从而导致整个阿里云挂了。

    - 访问密钥服务是什么呢？首先需要知道什么是密钥？

    - 例子：我提供一个数据查询服务，不做区分对外开放。
        - 问题：正常情况下所有人都可以调用，那谁调用多少次我就不清楚了，没办法对调用方区分，也没办法收费了。
        - 解决方法：很简单，增加一个访问标识。
            ![image](./Pictures/事故/访问密钥服务（AK）-密钥.avif)

        - 问题：加了访问标识，我们会遇到一个问题：访问标识泄漏了怎么办呢？或者黑客暴力遍历怎么办？
            - 比如我的访问标识是 tiankonguse，你直接用我的标识来读数据，也是可以读到的。

        - 解决方法：增加一个密码。
            ![image](./Pictures/事故/访问密钥服务（AK）-密钥1.avif)

        - 问题：加了密码后，请求可能会被黑客拦截，从而拦截获取到访问标识和密码。
        - 解决方法：是网络中不传输密码，只传输访问标识以及密码对传输内容生产的签名。
            - 服务端收到数据后，对签名进行逆向解密，解开的数据匹配了，才能读取数据。
            ![image](./Pictures/事故/访问密钥服务（AK）-密钥2.avif)

        - 问题：那服务端怎么对签名进行逆向解密呢？
        - 解决方法：从访问密钥服务（AK）获取到访问标识对应的密码。
            ![image](./Pictures/事故/访问密钥服务（AK）.avif)

        - 问题: 所有产品都请求访问密钥服务（AK），撑不住怎么办？
        - 解决方法：密钥数据全量或者分类分发到各产品。
            ![image](./Pictures/事故/访问密钥服务（AK）1.avif)

        - 问题：访问密钥服务中的数据库里的数据不对了会怎样呢？
        - 解决方法：对应的因为无法解开签名，从而认为是非法请求，拒绝对外服务。
            ![image](./Pictures/事故/访问密钥服务（AK）2.avif)

            - 由此可以看到，访问密钥服务（AK）是云上每个服务的一层防火墙，用于鉴别请求的合法性。
            - 这样一个服务出现故障，必然导致云大面积不可用。

        - 问题：那如何规避访问密钥服务产生脏数据的问题呢？
        - 解决方法：对密钥数据进行签名，加载签名数据库后，通过签名校验后才能使用。
            ![image](./Pictures/事故/访问密钥服务（AK）3.avif)

    - 另外，所有服务需要重启服务，这个也说明数据下发的控制逻辑有问题。
        - 按理说，访问秘钥服务读取最新数据后，按日常的策略，重新下发即可。
        - 现在却需要全量重新加载最新的秘钥数据，这里面应该还有其他的设计漏洞。
        - 例如可能是配置的版本号没变，数据错误，密钥没更新，版本不会更新。
        - 但是脏数据又是如何全量下发下去的呢？
        - 怎么都想不通。

## 2024年4月8日：腾讯云发生的一场全球性的大故障，该故障波及全球17个区域与数十款服务。74分钟

- [腾讯云：腾讯云4月8日故障复盘及情况说明](https://mp.weixin.qq.com/s/2e2ovuwDrmwlu-vW0cKqcA)

- 事故影响：

    - 经过故障定位发现，客户登录不上控制台正是由云API异常所导致。云API是云上统一的开放接口集合，客户可以通过API以编程方式管理和操控云端资源，云控制台通过组合云API提供交互式的网页功能。

    - 故障发生后，依赖云API提供产品能力的部分公有云服务，也因为云API的异常出现了无法使用的情况，比如云函数、文字识别、微服务平台、音频内容安全、验证码等。此次故障一共持续了近87分钟，期间共有1957个客户报障。

    - 从客户的视角来看，云服务大概可以分为数据面和控制面，数据面承载客户自身的业务，控制面负责操作云上不同产品。比如目前使用最广泛的IaaS服务基本上都是以直接面向数据面为主，控制面仅在客户购买或需要对资源层面进行调整操作时会涉及。此次发生故障的控制台和云API是对控制面的影响。

        ![image](./Pictures/事故/云计算的控制面与数据面.avif)

        - 通俗来讲，如果把云服务类比为酒店，控制台相当于酒店的前台，是一个统一的服务入口。一旦酒店前台发生故障，会导致入住、续住等管理能力不可用，但已入住的客房不受影响。

        - 这次故障中客户已经配置好的服务器等IaaS资源，包括已经部署运行的业务，没有受到云API异常的影响。其他以非云 API 方式提供服务的PaaS和SaaS服务，处于正常服务的状态。从数据上也验证了这一点。如图1显示，当天全产品进出流量趋势没有明显变化。

            ![image](./Pictures/事故/腾讯云全产品进出流量趋势图.avif)

        - 但是，用API提供的服务类产品（需要“酒店前台服务“）有不同程度的影响，比如腾讯云存储服务调用当天有明显下滑。期间售后团队协助部分客户做了业务容灾预案的实施，将受影响服务做调度以快速恢复客户的业务服务。从图2可以看出，当天存储服务调用有一个明显的波动。

            ![image](./Pictures/事故/腾讯云全产品进出流量趋势图.avif)

        ![image](./Pictures/事故/腾讯云-事故影响.avif)

- 事故公关处理：

    - [非法加冯：腾讯云：颜面尽失的草台班子](https://mp.weixin.qq.com/s/PgduTGIvWSUgHZhVfnb7Bg)

    - 然而，更让我失望的是腾讯云的Status Page的反应迟缓。Status Page作为公有云服务的重要特性，理应在服务宕机时及时更新信息，有效减少客户的焦虑，降低沟通成本。但腾讯云的Status Page在这次故障中显然没有起到应有的作用。它更新缓慢，缺乏真实性和准确性，给客户留下了不透明、有猫腻的印象。这种表现不仅损害了客户的信任，也影响了腾讯云的品牌形象。

    - 更糟糕的是，故障出现近一小时后，腾讯云才发出第一份公告。这份公告堪称三无公告，缺乏时间、地点、范围等关键信息，让客户难以了解故障的具体情况。而且，公告的内容真实性也存在疑问，比如将故障仅限于“控制台”，而不是整个控制面。这种避重就轻、混淆概念的做法，让我对腾讯云的专业素养产生了质疑。

    - 除了技术和专业性的问题，腾讯云的公关表现也堪称灾难级别。官方微博在故障期间居然还在发布与故障无关的内容，试图通过抖机灵来转移用户的注意力。这种做法无疑加剧了用户的不满和失望，让原本已经受损的品牌形象雪上加霜。

    - 与此同时，其他云服务提供商如Cloudflare在故障处理上的表现却让人眼前一亮。他们能够及时、准确地更新故障信息，通过自动化的方式监测到故障后立即推送通知，积极与用户沟通，尽量减少故障对用户的影响。这种负责任、透明的态度赢得了用户的尊重和信任。

- 事故原因：

    - 云API服务新版本向前兼容性考虑不够和配置数据灰度机制不足的问题。

    - 本次API升级过程中，由于新版本的接口协议发生了变化，在后台发布新版本之后对于旧版本前端传来的数据处理逻辑异常，导致生成了一条错误的配置数据，由于灰度机制不足导致异常数据快速扩散到了全网地域，造成整体API使用异常。

    - 发生故障后，按照标准回滚方案将服务后台和配置数据同时回滚到旧版本，并重启API后台服务，但此时因为承载API服务的容器平台也依赖API服务才能提供调度能力，即发生了循环依赖，导致服务无法自动拉起。通过运维手工启动方式才使API服务重启，完成整个故障恢复。

- 优化改进：

    - 第一，提升系统韧性
        - 1.定期执行预定的变更策略模拟演练，确保在真实故障发生时，能够迅速切换到恢复模式，最小化服务中断时间。
        - 2.优化服务部署架构，通过分层架构、代码审查和监控等手段， 避免API服务中潜在的循环依赖问题。
        - 3.提供API服务逃生通道，当故障发生时，可供调用方快速切换。

    - 第二，强化变更管理与保护措施
        - 1.完善自动化测试用例库，在系统变更前通过沙箱环境对变更内容进行严格验证。
        - 2.实施灰度发布策略，逐步推广新功能或配置更改，按集群、可用区、地域逐步生效，以便在发现问题时能够迅速回滚。
        - 3.引入异常自动熔断机制，当检测到系统异常时，能够立即中断变更过程。

    - 第三，增强故障响应与沟通能力
        - 1.对故障处理流程进行全面升级，确保实时更新故障处理进度和预计恢复时间点，提升故障报告发布效率。
        - 2.在对外发布的故障通知中，清晰阐述受影响的业务范围、故障根因及预计修复时长，保持透明度。
        - 3.优化腾讯云健康状态看板（StatusPage）的信息展示逻辑，解除对云API等云服务的依赖，通过引入缓存和容灾机制，确保即使在云服务出现故障时，能准确、及时地传递故障信息。

## 2024年7月2日：阿里云光缆被挖断

- 事故原因：专线被挖掘机挖断了。（小道消息，请以官网通告为准）

- 事故影响：B站、小红书、酷安等也崩了
    - B站可以看视频，动态功能正常。但不能点开up主主页，稍后再看功能也不能使用。

- 事故持续时间：从发现故障到影响恢复用时 38 分钟。

- 事故范围：上海可用区 N 出现网络访问异常。
    - 受到影响的服务包括：OSS，ECS，RDS，K8S，OTS，DTS，KMS，PolarDB，Redis，Mongo，ElasticSearch

    ![image](./Pictures/事故/2024年7月2日阿里云事故.avif)

# 机房相关事故

## 2023年3月29日：微信、QQ等腾讯旗下社交软件出现功能异常。

- [博士智引：【运维技术】以IT运维角度剖析腾讯3.29断网事件](https://mp.weixin.qq.com/s/Qf9KxlwJ7NIz9LOisLedig)

- 事故影响：

    - 微信、QQ等腾讯旗下社交软件出现功能异常
        - 微信包括语音呼叫、账号登录、朋友圈以及支付在内的多个功能无法正常使用
        - QQ文件传输、QQ空间、QQ邮箱等也同样出现问题。

    - 从网上的数据看，最早记录发现故障的时间是3月29日凌晨1点59分，恢复时间是3月29日11时许，@腾讯微信团队 发布消息称，微信、微信支付相关功能已恢复正常。时间大约是9个小时。腾讯借助微博发布消息。
        ![image](./Pictures/事故/腾讯借助微博发布消息.avif)

    - 由于腾讯的用户众多，此次故障影响面大，恢复时间长，惊动了工信部，工信部进行要求腾讯对事故过程和处理进行汇报，并将此次事件上升到了安全稳定的层面。
        ![image](./Pictures/事故/工信部指导腾讯公司.avif)

- 事故处理：
    - 这次事故影响了上亿的用户，是微信历史上最严重的故障，腾讯内部对此事件的评估为“一级事故”，并对几名高管进行了处理。
        - 其中包含公司高级执行副总裁、TEG(技术工程事业群)总裁卢山和WXG(微信事业群)副总裁周颢在内的管理者承担领导责任，被予以通报批评。
        - 此外，TEG华南数据中心的两位总经理和总监被处以降级和免职处罚，WXG技术架构部的两位总监和组长当期绩效考核给予Underperform等评级(二星级别，最高为五星)。

- 事故原因：广州电信机房冷却系统故障导致

    - 官方的说法：此次大面积网络服务中断源于相关机房发生制冷系统故障，机房温度上升，多台机器宕机导致微信崩溃、政务云系统瘫痪。业内人士分析称，故障原因可能是制冷系统管道破裂导致冷冻水泄露。而为了维持机房制冷，故障机房开始使用冰块降温。

        ![image](./Pictures/事故/当事机房使用冰块制冷.avif)

    - 其实这种可能性有，但是这种情况需要很严重才出现。

        - 机房的冷却系统是重要的环境系统，一般是多点从机房地面通风制冷，像腾讯的A级机房，制冷是有冗余的，也就是说，一般一台或者几台不工作，不至于马上升温，即便是升温，也不可能太高，高到让服务器宕机，让交换机不工作，这个应该不是一台或者几台空调不工作，而可能是动力电停电了，或者其他问题导致大批量的空调不工作。

        - 我查了一下，3月29日的广州市的气温，显示3月29日 周三小雨到中雨 21℃ ~ 16℃，空气湿润温度不高，这可能就出现一个问题，机房空调不工作了，长久没有人发现。

        - 这样可以猜测，机房的空调出现大面积停工，长久没有人发现。暴露的问题来了。

            - 第一， 可能就没有人巡检机房，出现了问题没有发现；
            - 第二， 技控手段没有发挥作用，按照到底，应该有集中监控系统，有温度报警系统，或者短信或者电话，看来这个也没有发挥作用，要么没有，要么不工作，要么没人理；
            - 第三， 如果出现大规模机房空调停工，要么是动力电停电，要么是设计问题，包括超载跳闸等，要么是空调超年限服务。估计有一种致命的因素存在，这些不是第一次出现，一次大的故障都是以前若干小故障积累而成的；
            - 第四， 机房的值班人员有没有比用户提前发现问题？

        - 9个小时， 3月29日是工作日，影响办公，对腾讯造成了很大的负面影响。

        - 这么长的时间，按照运维常识，这个时候腾讯IT运维团队在努力的恢复服务器，恢复机房的温度。那么有没有其他的做法？

            - 我们假设一下，如果广州电信机房不是因为温度而是因为火灾或者爆炸，那么各位的数据估计是保存不了了。我估计这些数据没有临时运行点，也没有灾备。

        - 正常的标准的作业是先恢复通信，再恢复故障。这个道理总有人明白，哪怕开始就奔着故障去了，一点有人提醒，还是要先恢复通信的，但是腾讯没有恢复，说明缺乏恢复的条件，说白了就没有一个灾备系统来负责临时的运行，临时也搭建不了一个系统来负责运行，只能寄希望于重启服务器了。

        - 我估计支付系统可能有备份数据，恢复用了6个多小时。也只是数据备份，没有数据副中心作为热备，来临时承担运行责任。

## 2023年3月29日：唯品会机房宕机12小时

- 事故描述：2023-03-29 00:14～12:01 唯品会329机房宕机12小时P0级故障，业绩损失超亿元，影响客户800多万

- 事故原因：南沙机房重大故障的主要原因是南沙 IDC 冷冻系统故障导致机房设备温度快速升高宕机，造成线上商城停止服务。
- 事故影响：持续 12 个小时，由于崩溃时间太长，影响了很多消费者无法正常下单，导致公司业绩损失超亿元，影响客户达 800 多万，公司将此次故障判定为 P0 级故障。与此同时，唯品会认为此次事故暴露出容灾应急预案和风险防范措施不到位，并决定对此次事件严肃处理。对基础平台部负责人做了免职处理。

## 2023年11月2日：Cloudflare 的控制平面和分析服务出现故障不可用，原因是一个数据中心机房 (PDX-DC04) 发生电源故障。

- [天空的代码世界：2023年Cloudflare 机房故障复盘](https://mp.weixin.qq.com/s/fgDaSBqFsjVtwj86uCwqcw)

- 事故原因：

    - Cloudflare 的控制平面和分析服务出现故障不可用，原因是一个数据中心机房 (PDX-DC04) 发生电源故障。
    - 主要影响 Cloudflare 的控制平面和分析服务受到影响，其中控制平面包括面向客户的接口和 API，分析服务包括日志记录和分析报告。
    - 持续时间：故障持续 40 小时左右。

- 事故起因：
    - 2023-11-2 08:50 PDX-04 机房服务提供商 Flexential 发生了一次计划外维护事件，在没有通知 Cloudflare 的情况下，关闭一个独立供电电源，启动了备用发电机。
    - 2023-11-2 11:40 PDX-04 的 PGE 变压器发生接地故障，独立供电电源和发电机都被关闭。
    - 2023-11-2 11:44 宣称支撑 10 分钟的备用 UPS 电池，仅 4 分钟就无法供电。
    - 2023-11-2 11:44 Cloudflare 收到监控系统发出的数据中心出现问题的通知，故障开始。
    - 2023-11-2 12:28 Cloudflare 收到机房服务商 Flexential 的故障通知。
    - 2023-11-2 12:48 机房服务商 Flexential 成功重启了发电机，部分设施已恢复供电，遇到断路器出现故障。
    - 2023-11-2 13:40 Cloudflare 决定将故障转移到位于欧洲的 Cloudflare 灾难恢复站点。
    - 2023-11-2 13:43 Cloudflare 在灾难恢复站点上提供了第一批服务控制平面服务，但遇到惊群问题，随后开启频率控制降级服务。
    - 2023-11-2 17:57 Cloudflare 的控制平面服务全部恢复，其他服务等待机房恢复。
    - 2023-11-2 22:48 机房服务商 Flexential 更换了发生故障的断路器，恢复了公用设施供电。Cloudflare 评估太晚了，决定第二天再恢复故障。
    - 2023-11-3 开始恢复 PDX-04 的服务，遵循整个设施的完整引导，先启动网络设备，然后启动数千台服务器并恢复其服务
    - 2023-11-4 04:25 所有服务完成启动与重建。
    - 故障持续共计 40 小时左右。

- 事故影响：
    - 11-2 11:44 ~ 11-2 13:43 控制平面和分析服务整体不可用，影响所有用户。
    - 11-2 13:43 ~ 11-2 17:57 控制平面陆续恢复，恢复期间部分用户偶尔会失败；分析服务所有用户都不可用。
    - 11-2 17:57 ~ 11-4 04:25 控制平面所有用户全部恢复，分析服务所有用户都不可用。

- 事故分析：

    - 机房的容灾设计与问题
        - PDX-04 机房施工时通过 Tier III 认证，来承诺高可用性 SLA，但是实际依旧断电了。
        - PDX-04 机房电源设计为2组独立供电电源，外加 10 台发电机，外加 UPS 电池。
        - 由于通用电气进行一次计划外维护事件，影响了一个独立供电电源。
        - 正常情况下，电力应该切换到另一个独立供电电源，但是 PDX-04 却启用了发电机。
        - 未知原因（尚未得到 Flexential 或 PGE 的回复），PDX-04 的 PGE 变压器发生接地故障。
        - 接地故障可能是由 PGE 执行的计划外维护而引起的，也可能是一个非常不幸的巧合。
        - 高压（12,470 伏）电源线的接地故障非常严重。
        - 电气系统旨在快速关闭，以防止发生损坏。
        - 不幸的是，在这种情况下，保护措施也关闭了 PDX-04 的所有发电机。
        - 这意味着该设施的两个发电来源——冗余公用线路和 10 台发电机——均已离线。
        - 幸运的是，除了发电机之外，PDX-04 还包含一组 UPS 电池。这些电池据称足以为该设施供电约 10 分钟。
        - 事实上，根据自己的设备故障中观察到的情况，仅 4 分钟后电池就开始出现故障。
        - 而Flexential 花了 68 分钟才首次恢复发电机。

    - Cloudflare 的容灾设计与问题

        - Cloudflare 的控制平面和分析系统主要在三个数据中心的服务器上运行。
        - 这三个数据中心相互独立，每个数据中心都有多个公用电源，并且每个数据中心都有多个冗余且独立的网络连接。
        - 这些设施被有意选择为相距一定距离，以最大限度地减少自然灾害导致这三个设施受到影响的可能性，同时又足够接近，以便它们都可以运行主动-主动冗余数据集群。
        - 这意味着他们正在三个设施之间持续同步数据。根据设计，如果任何设施离线，则其余设施能够继续运行。

    - Cloudflare 四年前开始实施多数据源中心系统容灾。

        - 控制平面系统：虽然 Cloudflare 的大多数关键控制平面系统已迁移到高可用性集群，但某些服务，尤其是一些较新的产品，尚未添加到高可用性集群中。
            - 此外，Cloudflare 的日志系统故意不属于高可用性集群的一部分。
                - 该决定的逻辑是，日志记录已经是一个分布式问题，日志在网络边缘排队，然后发送回的核心（或使用区域服务进行日志记录的客户的另一个区域设施）。
                - 如果日志记录设施离线，那么分析日志将在网络边缘排队，直到它重新上线。
                - Cloudflare 确定延迟分析是可以接受的。

        - 分析服务系统：PDX-DC04 机房容纳了 Cloudflare 最大的分析集群以及超过三分之一的高可用性集群机器。
            - 它也是尚未加入 Cloudflare 的高可用性集群的服务的默认位置。
            - Cloudflare 是该设施的相对较大客户，消耗了其总产能的约 10%。


        - 虽然 PDX-04 的设计在施工前已通过 Tier III 认证，并有望提供高可用性 SLA，但 Cloudflare 计划了它可能离线的可能性。
            - 即使运营良好的设施也可能遇到糟糕的日子， Cloudflare 为此做好了计划。
            - 在这种情况下，Cloudflare 预计会发生的情况是，Cloudflare 的分析将处于离线状态，日志将在边缘排队并延迟，并且未集成到高可用性集群中的某些较低优先级服务将暂时离线，直到这些服务在另一个设施重新部署。
            - 该地区运行的另外两个数据中心将接管高可用性集群的责任并保持关键服务在线。
            - 一般来说，这按计划进行。

        - 不幸的是，Cloudflare 发现本应位于高可用性集群上的服务子集依赖于专门在 PDX-04 中运行的服务。
            - 特别是，处理日志并为分析提供支持的两个关键服务——Kafka 和 ClickHouse——仅在 PDX-04 中可用，但有依赖于它们的服务在高可用性集群中运行。
            - 这些依赖关系不应该如此紧密，应该更优雅地失败，应该降级这些错误。

        - 另外，Cloudflare 通过使其他两个数据中心设施中的每一个（以及全部）完全离线来对高可用性集群进行测试。
            - Cloudflare 还测试了 PDX-04 的高可用性部分离线。
            - 然而，Cloudflare 从未对整个 PDX-04 设施进行全面离线测试。
            - 因此，Cloudflare 忽略了数据平面上某些依赖项的重要性。

        - Cloudflare 对于新产品及其相关数据库与高可用性集群集成的要求也过于宽松。
            - Cloudflare 允许多个团队快速创新。
            - 因此，产品通常会采取不同的路径来实现最初的阿尔法。

        - 虽然随着时间的推移，Cloudflare 的做法是将这些服务的后端迁移到最佳实践，但在产品宣布普遍可用 (GA) 之前，并没有正式要求这样做。
            - 这是一个错误，因为这意味着 Cloudflare 所采用的冗余保护根据产品的不同而不一致。

- 优化改进：

    - 谷歌有一个流程，当发生重大事件或危机时，他们可以调用黄色代码或红色代码。
        - 在这些情况下，大部分或全部工程资源都被转移到解决手头的问题。

    - Cloudflare 过去没有这样的流程，但今天很明显 Cloudflare 需要自己实现一个版本：Code Orange。
        - Cloudflare 正在将所有非关键工程功能转向专注于确保控制平面的高可靠性。

    - 作为其中的一部分，预计会发生以下变化：

        - 1.消除对核心数据中心的所有服务控制平面配置的依赖，并将它们转移到尽可能首先由分布式网络供电的地方。
        - 2.确保即使有核心数据中心都离线，网络上运行的控制平面也能继续运行
        - 3.要求所有指定为“普遍可用”的产品和功能必须依赖于高可用性集群，而不对特定设施有任何软件依赖。
        - 4.要求所有指定为“普遍可用”的产品和功能都具有经过测试的可靠灾难恢复计划。
        - 5.测试系统故障的影响范围并最大程度地减少受故障影响的服务数量
        - 6.对所有数据中心功能实施更严格的混沌测试，包括完全拆除每个核心数据中心设施。
        - 7.对所有核心数据中心进行彻底审核并制定重新审核计划以确保它们符合标准。
        - 8.日志记录和分析灾难恢复计划，确保即使在所有核心设施发生故障的情况下也不会丢失日志

- 总结

    - 看完 Cloudflare 这个复盘总结，可以发现三个问题，值得我们思考与借鉴。
        - 1.核心机房即使做了多重容灾，依旧可能全部断电。
        - 2.核心服务即使做了异地高可用容灾，依旧可能因为某个外部依赖导致服务不可用。
        - 3.高可用验证必须充分验证，即所有机房都需要验证全员断电后是否可以正常使用，尤其是默认机房。

## 2024年9月10日：阿里机房火灾

- 阿里云新加坡可用区C数据中心因锂电池爆炸导致火灾，到现在已经过去整整一周 (09-17) 仍未恢复。

- 通常来说，如果只是机房小范围失火的话，问题并不会特别大，因为电源和UPS通常放在单独房间内，与服务器机房隔离开。但一旦触发了消防淋水，问题就大条了：一旦服务器整体性断电，恢复时间基本上要以天计；如果泡水，那就不只是什么可用性的问题了，要考虑的是数据还能不能找回 —— 数据完整性 的问题了。

- 目前公告上的说法是，14号晚上已经拖出来一批服务器，正在干燥、一直到16号都没干燥完。从这个“干燥”说法来看，有很大概率是泡水了。

# 网络相关事故

## 2021年10月4日：Facebook宕机7小时

- [TheByte：Facebook 2021年10月 宕机分析](https://mp.weixin.qq.com/s/_edQ7uTokp8rtnob4n7Gfg)

- 事故后果：
    - 这次故障绕过了所有的高活设计，让 Meta 下 facebook、instagram、whatsapp等众多服务出现了长达接近7个小时宕机
    - 世界各地的用户纷纷跑向 Twitter 询问发生了什么？被逼无奈的Meta高管只能在Twitter上发布消息澄清事件。而此次事故也导致股价重挫，市值蒸发60亿美元。
    - 影响的范围之广以至于差点产生严重的二次故障，搞崩整个互联网。

- 事故分析：
    - ThousandEye 监控到 Facebook 应用出现 DNS 失效的情况，继而出现 DNS 权威解释服务器不可达的情况：
    - 当本地 DNS 缓存失效后，会向权威解释服务器解析域名，当权威服务器不可达，那意味着所有 HTTP相关的解析请求都会出现问题。

    - 如果当时我们在现场，可以使用 nslookup以及dig 配合查询 facebook.com 的解析, Server 为 Local DNS， Non-authoritative answer 则为 域名的解析 为 缓存的结果
        ```sh
        nslookup www.facebook.com
        Server:        8.8.8.8
        Address:    8.8.8.8#53

        Non-authoritative answer:
        www.facebook.com    canonical name = star-mini.c10r.facebook.com.
        Name:    star-mini.c10r.facebook.com
        Address: 31.13.75.35
        ```

    - 当时，Cloudflare 也观察到这个错误，以为是自己的 DNS 解析服务(1.1.1.1) 出现故障, 继而开始紧急分析故障，他们在日志中发现 23:40 左右，Meta的 AS 自治域 向外发送了大量的 BGP 更新。

    - 进一步分析BGP消息发现了大量路由撤销，包含了 Meta DNS 权威服务器的路由:

        - 这期间 Meta BGP Prefix数量由129个减少103个，这26个Prefix包含了 5,888个IP。

        - Meta 有4个权威 DNS 服务器，分别是
            - a.ns.facebook.com（129.134.30.12）
            - b.ns.facebook.com（129.134.31.12）
            - c.ns.facebook.com（185.89.218.12）
            - d.ns.facebook.com（185.89.219.12）

            - 非常不巧，这四个 DNS权威服务器IP都在丢失的IP块中。

    - 在Meta的故障公告中是这样的说法： 调度数据中心之间网络流量的骨干路由器配置更改导致边界网关协议撤销了Facebook自治域AS32934下包含Facebook域名服务器IP的IP地址块，抹去了Facebook需要的DNS路由信息。

    - BGP 撤销 DNS权威服务器IP 导致所有的数据包都会丢弃在FB的边界路由器上，作为直接后果，世界各地的所有 DNS 解析器都停止解析它们的域名。

    - 当时的运维人员使用 dig 查询 各个公共 DNS服务器解析 Facebook相关的域名， 全部出现 SERVFAIL 错误。
        ```sh
        ➜  ~ dig @1.1.1.1 facebook.com
        ;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 31322
        ;facebook.com.            IN    A
        ➜  ~ dig @1.1.1.1 whatsapp.com
        ;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 31322
        ;whatsapp.com.            IN    A
        ➜  ~ dig @8.8.8.8 facebook.com
        ;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 31322
        ;facebook.com.            IN    A
        ➜  ~ dig @8.8.8.8 whatsapp.com
        ;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 31322
        ;whatsapp.com.            IN    A
        ```

- 事故原因：

    - 通过对故障的回溯分析，这次故障实际上 BGP 和 DNS 的一系列巧合操作造成了此次事件的严重后果， BGP发布了错误的路由，恰巧 Meta 权威域名解析服务器IP的地址也包含在这部分路由中， 这导致 网络域名解析 IP包 无法路由到 Meta 内部的服务器中。

    - 由于 DNS 出现问题，运维人员 基本无法再通过远程的方式修复路由，而机房的维护人员没有权限，也没有储备相关的知识去解决这个问题，只能是修复团队紧急“打飞的” 到加州的主数据中心参与维修。

    - 这就是此次故障范围、时长影响巨大的原因。

- 事故连锁反应：

    - 进一步出现的连锁反应是：由于无法连接Meta的DNS权威服务器，1.1.1.1、8.8.8.8 等各个主要的公共 DNS 解析器都开始发布（或缓存）SERVFAIL 响应。

    - 当时 CloudFlare DNS服务监测到， 因为 Facebook 用户太多了，用户无法正常登陆 APP 时会疯狂地发起重试，CloudFlare的DNS服务器请求解析瞬间增大了30倍，差点引起连锁反应，把整个互联网搞崩。

    - 所幸 1.1.1.1 在当时的情况下顶住了压力，如果也造成 1.1.1.1 宕机，恐怕整个互联网会出现相当时间的不可用。

    - 除了专业的互联网服务商收到的影响，由于Facebook的整个服务全部无法使用，全世界的用户都迫切地想知道答案，纷纷跑到 Twitter、Signal 等社交平台，导致 这些服务的 DNS 查询也大大增加。

- 事故总结：

    - BGP的问题

        - 对于这次故障，我想有几个技术需要思考：BGP协议、DNS的可用性、以及运维人员重大操作的保障措施。

        - 第一个是BGP， 实际上BGP带来的重大事故已经不止一次发生了，2017年由于Google BGP错误公告，导致了日本互联网长时间的瘫痪。

        - BGP作为整个互联网的基石，其协议已经使用了30年之久，随着互联网的发展，BGP 和 TCP 耦合的问题导致 BGP 通信过程收敛缓慢、4Byte-ASN和IPv4地址交易带了路由前缀大量更新、BGP FlowSpec 使得协议栈越来越复杂 等等问题

- DNS的问题

    - DNS本质是一个分布式的数据库，这种结构允许对整体数据库的各个部分进行本地控制且互相关联。如下图所示，亚马逊 amazon.com 的权威域授权体系肯定要优于facebook.com （包括不同的AS域），所以它的抗风险能力肯要强于Facebook。
        ![image](./Pictures/事故/DNS层次.avif)

    - Meta这次故障带给我们的经验是：

        - DNS系统在架构设计和技术路线选择时要尽量避免采用单一化架构和技术，应从部署形式和部署位置等层面考虑技术多元性。

        - 在部署形式的设计上可选择将DNS服务器节点全部放在SLB（应用负载）后方，或采用 OSPF Anycast架构等部署形式，提高DNS系统的可靠性。

        - 在部署位置的设计上可选择数据中心自建集群+公有云服务混合异构部署，利用云的分布式优势进一步增强DNS系统的健壮性，同时提升DNS系统在遭受DDoS攻击时的抵御能力。

    - 运维操作的问题：

        - 一些关键的运维维护实际上是有很大风险，比如更改BGP通告，修改内网的路由、修改本机的防火墙策略等等，严重的失误直接将造成远程连接无法再使用，这个时候想远程修复就难了，只能接近 物理机才有方法。

        - 对于这种在生产环境中很大风险性的操作，可以引用一种二次提交的策略：

        - 比如修改一个 iptables 规则，修改之后引入10分钟的“观察期”，在观察期后，系统内部再自动恢复 原来的配置，防止配置失败，运维人员连接不上远程机。运维人员确认观察期内流量、规则没有任何问题之后，再执行正式的操作。

## 2023年6月8日：广东电信5小时的大面积断网事件

- 尚未得到官方的解释，但是从独立电信分析师付亮的看法来看，应该是广东电信核心网某个关键模块出现了故障。

## 2024年1月11日：《英雄联盟》《王者荣耀》《和平精英》等多款腾讯旗下游戏出现服务器崩溃、掉线的问题。

- 事故原因：运营商线路故障导致网络波动，部分区域服务器的用户出现掉线和暂时无法登录的情况。

# linux相关事故

## 2023年10月23日：语雀突发 P0 级事故！宕机 8 小时

- [InfoQ：语雀突发 P0 级事故！宕机 8 小时被网友怒喷，运维又背锅？](https://mp.weixin.qq.com/s/LOjiaULzEgkI5VEe74kX0g)

- 事故原因：服务语雀的数据存储运维团队在进行升级操作时，由于新的运维升级工具 bug，导致华东地区生产环境存储服务器被误下线。

- 事故处理流程：

    - 14:07 数据存储运维团队收到监控系统报警，定位到原因是存储在升级中因新的运维工具 bug 导致节点机器下线；

    - 14:15 联系硬件团队尝试将下线机器重新上线；

    - 15:00 确认因存储系统使用的机器类别较老，无法直接操作上线，立即调整恢复方案为从备份系统中恢复存储数据；

    - 15:10 开始新建存储系统，从备份中开始恢复数据，由于语雀数据量庞大，此过程历时较长，19 点完成数据恢复，同时为保障数据完整性，在完成恢复后，用时 2 个小时进行数据校验；

    - 21 点存储系统通过完整性校验，开始和语雀团队联调，最终在 22 点恢复语雀全部服务。用户所有数据均未丢失。

- 事故赔偿：

    - 针对语雀个人用户，赠送 6 个月的会员服务。操作流程：进入工作台「账户设置」，点击左侧「会员信息」，在会员信息页面点击「立即领取」，即可获得赠送服务。

    - 针对语雀空间用户，由于情况比较复杂，语雀团队会单独制定赔偿方案。请空间管理员留意语雀站内信。

- 优化改进：

    - 语雀表示，作为一款服务千万级客户的文档产品，通过这次故障深刻认识到，应该做到更完善的技术风险保障和高可用架构设计，尤其是面向技术变更操作的“可监控，可灰度，可回滚”的系统化建设和流程审计，从同 Region 多副本容灾升级为两地三中心的高可用能力，设计足够的数据和系统冗余实现快速恢复，并进行定期的容灾应急演练。只有这样，才能提升严重基础设施故障时的恢复速度，并从根本上避免这类故障再次出现。

    - 为此语雀制定了如下改进措施：

        - 升级硬件版本和机型，实现离线后的快速上线。该措施在本次故障修复中已完成；
        - 运维团队加强运维工具的质量保障与测试，杜绝此类运维 bug 再次发生；
        - 缩小运维动作灰度范围，增加灰度时间，提前发现 bug；
        - 从架构和高可用层面改进服务，为语雀增加存储系统的异地灾备。

- 语雀的技术架构演进历程

    - 公开信息显示，语雀于 2016 年孵化自蚂蚁科技。当时，蚂蚁金融云需要一个工具来承载它的文档，负责的技术同学利用业余时间搭建了这个文档工具。当时，语雀底层服务完全基于体验技术部内部提供的 BaaS 服务和容器托管平台：

        - Object 服务：一个类 MongoDB 的数据存储服务；
        - File 服务：阿里云 OSS 的基础上封装的一个文件存储服务；
        - DockerLab：一个容器托管平台。

    - 2017 年，为了应对业务发展带来的挑战，语雀团队主要从下面几个点进行改造：

        - BaaS 服务虽然使用简单成本低，但是它们提供的功能不足以满足语雀业务的发展，同时稳定性上也有不足。所以语雀团队将底层服务由 BaaS 替换成了阿里云的 IaaS 服务（MySQL、OSS、缓存、搜索等服务）。

        - Web 层仍然采用了 Node.js 与 Egg 框架，但是业务层借鉴 rails 社区的实践开始变成了一个大型单体应用，通过引入 ORM 构建数据模型层，让代码的分层更清晰。

        - 前端编辑器从 codeMirror 迁移到 Slate。为了更好的实现语雀编辑器的功能，语雀团队内部 fork 了 Slate 进行深入开发，同时也自定义了一个独立的内容存储格式，以提供更高效的数据处理和更好的兼容性。

    - 2018 年初，语雀开始正式对外提供服务，进行商业化。为了应对业务发展，语雀的架构也随之发生了演进：将底层的依赖完全上云，全部迁移到了阿里云上，阿里云不仅仅提供了基础的存储、计算能力，同时也提供了更丰富的高级服务，同时在稳定性上也有保障：

        - 丰富的云计算基础服务，保障语雀的服务端可以选用最适合语雀业务的的存储、队列、搜索引擎等基础服务；

        - 更多人工智能服务给语雀的产品带来了更多的可能性，包括 OCR 识图、智能翻译等服务，最终都直接转化成为了语雀的特色服务。

    - 而在应用层，语雀的服务端依然还是以一个基于 Egg 框架的大型的 Node.js Web 应用为主。但是随着功能越来越多，也开始将一些相对比较独立的服务从主服务中拆出去，可以把这些服务分成几类：

        - 微服务类：例如多人实时协同服务，由于它相对独立，且长连接服务不适合频繁发布，所以语雀团队将其拆成了一个独立的微服务，保持其稳定性。

        - 任务服务类：像语雀提供的大量本地文件预览服务，会产生一些任务比较消耗资源、依赖复杂。语雀团队将其从主服务中剥离，可以避免不可控的依赖和资源消耗对主服务造成影响。

        - 函数计算类：类似 Plantuml 预览、Mermaid 预览等任务，对响应时间的敏感度不高，且依赖可以打包到阿里云函数计算中，语雀团队会将其放到函数计算中运行，既省钱又安全。

    - 随着编辑器越来越复杂，在 Slate 的基础上进行开发遇到的问题越来越多。最终语雀还是走上了自研编辑器的道路，基于浏览器的 Contenteditable 实现了富文本编辑器，通过 Canvas 实现了表格编辑器，通过 SVG 实现了思维导图编辑器。


## [dbaplus社群：自己亲手引发运维事故是一种怎样的体验？](https://mp.weixin.qq.com/s/y4KRjO0IJskoYTmJQjrc8A)

- 事故起因：我接到研发的请求，要给他们部门专用的服务器安装一个软件。

    - 我立马用root apt install安装好了，但是我安装过程中看的有一些不认识的包怎么装上去了，装完后发现好像也没有哪里不对劲，系统能正常运行，遂交付给研发。

    - 过了一会研发找到我说不对，怎么有些安装包没有了，然后我又继续安装不见的包，然后修修补补，终于把他们缺的东西补齐。

    - 然后再试，还是不行……

    - 继续检查，研发说glibc版本不对！得赶紧解决，这个是和客户环境一致的，我们国庆的时候release！

    - 然后我就找办法降级glibc，找到晚上也没有办法，我领导也加入一起找解决方案。
    - 我们把Google百度翻了个底朝天，也没看到有解决方案，一直到凌晨五点，两个人实在顶不住了，然后去睡觉。我睡的也不踏实，早上八点就起床了。

    - 第二天继续上班后，我和领导商量要不给他们重装吧，领导去和他们商量回来，说有一些商业软件特别不好搞，不能重装，当初花了好长时间才把这套环境搭起来的！（搭建这套环境的前领导跳槽了）

- 处理流程：然后我继续找解决方案！
    - 找啊找，找啊找，我搭了各种环境做测试了，就是不能把glibc给降级。
    - 然后毫无意外的，研发的release不能在我们的环境上搞了，还好他们还有外部环境能用，不然我不知道怎么办！
    - 就这样胆战心惊的过了个国庆，国庆我也一直在查文档，当时还跟我妈说，我可能要被辞退了……

- 优化改进：国庆回来后，我继续收拾这个烂摊子，然后研发捅到我大领导（vp）那里，为了这个事，专门开会几次，并制定了一系列针对他们部门的问题处理流程，以及当前问题该如何解决，然后得出结论，他们部门以后所有的安装软件需求，必须经过大领导审批，我们才可以动手，并且全力恢复这台服务器的环境，好吧，这个事我继续干！

- 当时9月份我们一个同事合同到期，因为表现原因，没有续签合同，只剩下我和领导两个人，压力特别大。
- 屋漏偏逢连夜雨，当时我们内网又被外部攻击，我领导全力处理那边的事，我一个人一边想办法解决该部门的烂摊子，一边处理其他部门提过来的case，忙得不可开交。
- 弄了一个多月后，我决定放弃，重新搭建一套环境给他们，遂向领导汇报并征得同意，用了两三天时间就把那套环境搞好了，并没有他们说得那么麻烦。
- 那段时间因为人员少，我领导动不动就是加班到八九点，我六点多七点这样下班（外企嘛），感觉特别对不起他，人少的情况下还给他添乱。
- 当时我还想着年终奖可能要变少了，结果还超出了我的预期！！！
- 当时我真的是胆战心惊，干运维这么久，第一次搞出这么大的事，差点影响到他们的release。后面我领导说到：人总是会出错的，出错不怕，就怕重复出错。跟了这样的领导真好！
- 去年我拿到字节的offer，和他提出离职，他特别震惊，说到：我们去年这么难都挺过来了，现在舒服了，为什么要走呢？是有什么不满意吗？然后我和他说了我的诉求，他很积极的和vp沟通，后面在他们的努力下，我又留了下来！

## [dbaplus社群：官方自爆了！去年今天的B站原来是这样崩溃的……](https://mp.weixin.qq.com/s/gcg8DMEypeBHoTsVZHHjXQ)

- 事故描述：B站无法打开，甚至APP首页也无法打开。
    - 此次事故发生时，B站挂了迅速登上全网热搜，作为技术人员，身上的压力可想而知。事故已经发生，我们能做的就是深刻反思，吸取教训，总结经验，砥砺前行。

- 事故处理流程：

    - 问题：发现在线业务主机房七层SLB（基于OpenResty构建） CPU 100%，无法处理用户请求，其他基础设施反馈未出问题，此时已确认是接入层七层SLB故障，排除SLB以下的业务层问题。

    - 解决方法（失败）：尝试恢复主机房的SLB
        - 我们通过Perf发现SLB CPU热点集中在Lua函数上，怀疑跟最近上线的Lua代码有关，开始尝试回滚最近上线的Lua代码。

        - 近期SLB配合安全同学上线了自研Lua版本的WAF，怀疑CPU热点跟此有关，尝试去掉WAF后重启SLB，SLB未恢复。

        - SLB两周前优化了Nginx在balance_by_lua阶段的重试逻辑，避免请求重试时请求到上一次的不可用节点，此处有一个最多10次的循环逻辑，怀疑此处有性能热点，尝试回滚后重启SLB，未恢复。

        - SLB一周前上线灰度了对 HTTP2 协议的支持，尝试去掉 H2 协议相关的配置并重启SLB，未恢复。

    - 解决方法（成功）： 新建源站SLB
        - SLB运维尝试回滚相关配置依旧无法恢复SLB后，决定重建一组全新的SLB集群，让CDN把故障业务公网流量调度过来，通过流量隔离观察业务能否恢复。
        - SLB新集群初始化完成，开始配置四层LB和公网IP。

        - SLB新集群初始化和测试全部完成，CDN开始切量。SLB运维继续排查CPU 100%的问题，切量由业务SRE同学协助。

        - 直播业务流量切换到SLB新集群，直播业务恢复正常。

        - 主站、电商、漫画、支付等核心业务陆续切换到SLB新集群，业务恢复。

        - 此时在线业务基本全部恢复。


- 成功恢复后的分析原因：

    - SLB新集群搭建完成后，在给业务切量止损的同时，SLB运维开始继续分析CPU 100%的原因。

    - 使用Lua 程序分析工具跑出一份详细的火焰图数据并加以分析，发现 CPU 热点明显集中在对 lua-resty-balancer 模块的调用中，从 SLB 流量入口逻辑一直分析到底层模块调用，发现该模块内有多个函数可能存在热点。

    - 选择一台SLB节点，在可能存在热点的函数内添加 debug 日志，并重启观察这些热点函数的执行结果。

    - 在分析 debug 日志后，发现 lua-resty-balancer模块中的 _gcd 函数在某次执行后返回了一个预期外的值：nan，同时发现了触发诱因的条件：某个容器IP的weight=0。

    - 怀疑是该 _gcd 函数触发了 jit 编译器的某个 bug，运行出错陷入死循环导致SLB CPU 100%，临时解决方案：全局关闭 jit 编译。

    SL 改SLB 集群的配置，关闭 jit 编译并分批重启进程，SLB CPU 全部恢复正常，可正常处理请求。同时保留了一份异常现场下的进程core文件，留作后续分析使用。

    - SLB运维修改其他SLB集群的配置，临时关闭 jit 编译，规避风险。

- 在线下环境成功复现出该 bug，同时发现SLB 即使关闭 jit 编译也仍然存在该问题。此时我们也进一步定位到此问题发生的诱因：在服务的某种特殊发布模式中，会出现容器实例权重为0的情况。

- 经过内部讨论，我们认为该问题并未彻底解决，SLB 仍然存在极大风险，为了避免问题的再次产生，最终决定：平台禁止此发布模式；SLB 先忽略注册中心返回的权重，强制指定权重。

    - 发布平台禁止此发布模式。

    - SLB 修改Lua代码忽略注册中心返回的权重。

    - SLB 在UAT环境发版升级，并多次验证节点权重符合预期，此问题不再产生。

    - 生产所有 SLB 集群逐渐灰度并全量升级完成。

- 原因：

    - B站在19年9月份从Tengine迁移到了OpenResty，基于其丰富的Lua能力开发了一个服务发现模块，从我们自研的注册中心同步服务注册信息到Nginx共享内存中，SLB在请求转发时，通过Lua从共享内存中选择节点处理请求，用到了OpenResty的lua-resty-balancer模块。到发生故障时已稳定运行快两年时间。

    - 在故障发生的前两个月，有业务提出想通过服务在注册中心的权重变更来实现SLB的动态调权，从而实现更精细的灰度能力。SLB团队评估了此需求后认为可以支持，开发完成后灰度上线。

    - 诱因：在某种发布模式中，应用的实例权重会短暂的调整为0，此时注册中心返回给SLB的权重是字符串类型的"0"。此发布模式只有生产环境会用到，同时使用的频率极低，在SLB前期灰度过程中未触发此问题。

- 问题分析

    - 1.为何故障刚发生时无法登陆内网后台？
        - 事后复盘发现，用户在登录内网鉴权系统时，鉴权系统会跳转到多个域名下种登录的Cookie，其中一个域名是由故障的SLB代理的，受SLB故障影响当时此域名无法处理请求，导致用户登录失败。流程如下：

        ![image](./Pictures/事故/b站-用户登陆流程.avif)

        - 事后我们梳理了办公网系统的访问链路，跟用户链路隔离开，办公网链路不再依赖用户访问链路。

    - 2.为何多活SLB在故障开始阶段也不可用？

        - 多活SLB在故障时因CDN流量回源重试和用户重试，流量突增4倍以上，连接数突增100倍到1000W级别，导致这组SLB过载。后因流量下降和重启，逐渐恢复。此SLB集群日常晚高峰CPU使用率30%左右，剩余Buffer不足两倍。
        - 如果多活SLB容量充足，理论上可承载住突发流量， 多活业务可立即恢复正常。此处也可以看到，在发生机房级别故障时，多活是业务容灾止损最快的方案，这也是故障后我们重点投入治理的一个方向。

    - 3.为何在回滚SLB变更无效后才选择新建源站切量，而不是并行？

        - 我们的SLB团队规模较小，当时只有一位平台开发和一位组件运维。在出现故障时，虽有其他同学协助，但SLB组件的核心变更需要组件运维同学执行或review，所以无法并行。

    - 4.为何新建源站切流耗时这么久？

        ![image](./Pictures/事故/b站-公网架构.avif)

        - 此处涉及三个团队：
            - SLB团队：选择SLB机器、SLB机器初始化、SLB配置初始化
            - 四层LB团队：SLB四层LB公网IP配置
            - CDN团队：CDN更新回源公网IP、CDN切量

        - SLB的预案中只演练过SLB机器初始化、配置初始化，但和四层LB公网IP配置、CDN之间的协作并没有做过全链路演练，元信息在平台之间也没有联动，比如四层LB的Real Server信息提供、公网运营商线路、CDN回源IP的更新等。所以一次完整的新建源站耗时非常久。在事故后这一块的联动和自动化也是我们的重点优化方向，目前一次新集群创建、初始化、四层LB公网IP配置已经能优化到5分钟以内。

    - 5.后续根因定位后证明关闭jit编译并没有解决问题，那当晚故障的SLB是如何恢复的？

        - 当晚已定位到诱因是某个容器IP的weight="0"。此应用在1:45时发布完成，weight="0"的诱因已消除。所以后续关闭jit虽然无效，但因为诱因消失，所以重启SLB后恢复正常。

        - 如果当时诱因未消失，SLB关闭jit编译后未恢复，基于定位到的诱因信息：某个容器IP的weight=0，也能定位到此服务和其发布模式，快速定位根因。

- 优化改进

    - 1.多活建设
        - 做了多活的业务核心功能基本恢复正常，如APP推荐、APP播放、评论&弹幕拉取、动态、追番、影视等。故障时直播业务也做了多活，但当晚没及时恢复的原因是：直播移动端首页接口虽然实现了多活，但没配置多机房调度。导致在主机房SLB不可用时直播APP首页一直打不开，非常可惜。通过这次事故，我们发现了多活架构存在的一些严重问题：

        - 1.多活基架能力不足
            - 机房与业务多活定位关系混乱。
            - CDN多机房流量调度不支持用户属性固定路由和分片。
            - 业务多活架构不支持写，写功能当时未恢复。
            - 部分存储组件多活同步和切换能力不足，无法实现多活。

        - 2.业务多活元信息缺乏平台管理
            - 哪个业务做了多活？
            - 业务是什么类型的多活，同城双活还是异地单元化？
            - 业务哪些URL规则支持多活，目前多活流量调度策略是什么？
            - 上述信息当时只能用文档临时维护，没有平台统一管理和编排。

        - 3.多活切量容灾能力薄弱
            - 多活切量依赖CDN同学执行，其他人员无权限，效率低。
            - 无切量管理平台，整个切量过程不可视。
            - 接入层、存储层切量分离，切量不可编排。
            - 无业务多活元信息，切量准确率和容灾效果差。

            - 我们之前的多活切量经常是这么一个场景：业务A故障了，要切量到多活机房。SRE跟研发沟通后确认要切域名A+URL A，告知CDN运维。CDN运维切量后研发发现还有个URL没切，再重复一遍上面的流程，所以导致效率极低，容灾效果也很差。

        所以我们多活建设的主要方向：

        - 4.多活基架能力建设
            - 优化多活基础组件的支持能力，如数据层同步组件优化、接入层支持基于用户分片，让业务的多活接入成本更低。
            - 重新梳理各机房在多活架构下的定位，梳理Czone、Gzone、Rzone业务域。
            - 推动不支持多活的核心业务和已实现多活但架构不规范的业务改造优化。

        - 5.多活管控能力提升
            - 统一管控所有多活业务的元信息、路由规则，联动其他平台，成为多活的元数据中心。
            - 支持多活接入层规则编排、数据层编排、预案编排、流量编排等，接入流程实现自动化和可视化。
            - 抽象多活切量能力，对接CDN、存储等组件，实现一键全链路切量，提升效率和准确率。
            - 支持多活切量时的前置能力预检，切量中风险巡检和核心指标的可观测。

- SLB治理

    - 1.架构治理
        - 故障前一个机房内一套SLB统一对外提供代理服务，导致故障域无法隔离。后续SLB需按业务部门拆分集群，核心业务部门独立SLB集群和公网IP。
        - 跟CDN团队、四层LB&网络团队一起讨论确定SLB集群和公网IP隔离的管理方案。
        - 明确SLB能力边界，非SLB必备能力，统一下沉到API Gateway，SLB组件和平台均不再支持，如动态权重的灰度能力。

    - 2.运维能力
        - SLB管理平台实现Lua代码版本化管理，平台支持版本升级和快速回滚。
        - SLB节点的环境和配置初始化托管到平台，联动四层LB的API，在SLB平台上实现四层LB申请、公网IP申请、节点上线等操作，做到全流程初始化5分钟以内。
        - SLB作为核心服务中的核心，在目前没有弹性扩容的能力下，30%的使用率较高，需要扩容把CPU降低到15%左右。
        - 优化CDN回源超时时间，降低SLB在极端故障场景下连接数。同时对连接数做极限性能压测。

    - 3.自研能力
        - 运维团队做项目有个弊端，开发完成自测没问题后就开始灰度上线，没有专业的测试团队介入。此组件太过核心，需要引入基础组件测试团队，对SLB输入参数做完整的异常测试。
        - 跟社区一起，Review使用到的OpenResty核心开源库源代码，消除其他风险。基于Lua已有特性和缺陷，提升我们Lua代码的鲁棒性，比如变量类型判断、强制转换等。
        - 招专业做LB的人。我们选择基于Lua开发是因为Lua简单易上手，社区有类似成功案例。团队并没有资深做Nginx组件开发的同学，也没有做C/C++开发的同学。

- 故障演练

    - 本次事故中，业务多活流量调度、新建源站速度、CDN切量速度&回源超时机制均不符合预期。所以后续要探索机房级别的故障演练方案：
        - 模拟CDN回源单机房故障，跟业务研发和测试一起，通过双端上的业务真实表现来验收多活业务的容灾效果，提前优化业务多活不符合预期的隐患。
        - 灰度特定用户流量到演练的CDN节点，在CDN节点模拟源站故障，观察CDN和源站的容灾效果。
        - 模拟单机房故障，通过多活管控平台，演练业务的多活切量止损预案。

- 应急响应

    - B站一直没有NOC/技术支持团队，在出现紧急事故时，故障响应、故障通报、故障协同都是由负责故障处理的SRE同学来承担。如果是普通事故还好，如果是重大事故，信息同步根本来不及。所以事故的应急响应机制必须优化：
    - 优化故障响应制度，明确故障中故障指挥官、故障处理人的职责，分担故障处理人的压力。
    - 事故发生时，故障处理人第一时间找backup作为故障指挥官，负责故障通报和故障协同。在团队里强制执行，让大家养成习惯。
    - 建设易用的故障通告平台，负责故障摘要信息录入和故障中进展同步。

    - 本次故障的诱因是某个服务使用了一种特殊的发布模式触发。我们的事件分析平台目前只提供了面向应用的事件查询能力，缺少面向用户、面向平台、面向组件的事件分析能力：
        - 跟监控团队协作，建设平台控制面事件上报能力，推动更多核心平台接入。
        - SLB建设面向底层引擎的数据面事件变更上报和查询能力，比如服务注册信息变更时某个应用的IP更新、weight变化事件可在平台查询。
        - 扩展事件查询分析能力，除面向应用外，建设面向不同用户、不同团队、不同平台的事件查询分析能力，协助快速定位故障诱因。

## [dbaplus社群：重大事故！IO问题引发线上20台机器同时崩溃](https://mp.weixin.qq.com/s/WNXaY9xVFc7zo4gv19PmPw)

- 事故原因：

    - 几年前的一个下午，公司里码农们正在安静地敲着代码，突然很多人的手机同时“哔哔”地响了起来。本来以为发工资了，都挺高兴！打开一看，原来是告警短信

    - 告警提示“线程数过多，超出阈值”，“CPU空闲率太低”。打开监控系统一看，订单服务所有20个服务节点都不行了，服务没响应。

    - 到一个全链路性能监控工具上看监控，每个springboot节点线程数全都达到了最大值。但是JVM堆内存和GC没有明显异常。CPU空闲率基本都是0%，但是CPU使用率并不高，反而IO等待却非常高。下面是执行top命令查看CPU状况的截图：

        ![image](./Pictures/事故/top命令查看CPU状况.avif)

        > 注意：wa只代表磁盘IO Wait，不包括网络IO Wait。

        - CPU空闲率是0%（上图中红框id）
        - CPU使用率是22%（上图中红框 us 13% 加上 sy 9%，us可以理解成用户进程占用的CPU，sy可以理解成系统进程占用的CPU）
        - CPU 在等待磁盘IO操作上花费的时间占比是76.6% （上图中红框 wa）

    - 到现在可以确定，问题肯定发生在IO等待上。利用监控系统和jstack命令，最终定位问题发生在文件写入上。大量的磁盘读写导致了JVM线程资源耗尽（注意，不代表系统CPU耗尽）。最终导致订单服务无法响应上游服务的请求。

- 优化改进：如何避免IO问题带来的系统故障

    - I/O:
        - 磁盘IO：磁盘的输入输出，比如磁盘和内存之间的数据传输。
        - 网络IO：不同系统间跨网络的数据传输，比如两个系统间的远程接口调用。

        ![image](./Pictures/事故/io操作场景.avif)

        - 通过上图，我们可以了解到IO操作发生的具体场景。一个请求过程可能会发生很多次的IO操作：

            - 1.页面请求到服务器会发生网络IO
            - 2.服务之间远程调用会发生网络IO
            - 3.应用程序访问数据库会发生网络IO
            - 4.数据库查询或者写入数据会发生磁盘IO

    - Java中线程状态和IO的关系

        - 当我们用jstack查看Java线程状态时，会看到各种线程状态。当发生IO等待时（比如远程调用时），线程是什么状态呢，Blocked还是Waiting？

        - 答案是Runnable状态，是不是有些出乎意料！实际上，在操作系统层面Java的Runnable状态除了包括Running状态，还包括Ready（就绪状态，等待CPU调度）和IO Wait等状态。

            ![image](./Pictures/事故/jstack-线程状态.avif)
            - 如上图，Runnable状态的注解明确说明了，在JVM层面执行的线程，在操作系统层面可能在等待其他资源。如果等待的资源是CPU，在操作系统层面线程就是等待被CPU调度的Ready状态；如果等待的资源是磁盘网卡等IO资源，在操作系统层面线程就是等待IO操作完成的IO Wait状态。

        - 有人可能会问，为什么Java线程没有专门的Running状态呢？

            - 目前绝大部分主流操作系统都是以时间分片的方式对任务进行轮询调度，时间片通常很短，大概几十毫秒，也就是说一个线程每次在cpu上只能执行几十毫秒，然后就会被CPU调度出来变成Ready状态，等待再一次被CPU执行，线程在Ready和Running两个状态间快速切换。

            - 通常情况，JVM线程状态主要为了监控使用，是给人看的。当你看到线程状态是Running的一瞬间，线程状态早已经切换N次了。所以，再给线程专门加一个Running状态也就没什么意义了。

    - 对于磁盘文件访问的操作，可以采用线程池方式，并设置线程上线，从而避免整个JVM线程池污染，进而导致线程和CPU资源耗尽。

    - 对于网络间远程调用。为了避免服务间调用的全链路故障，要设置合理的TImeout值，高并发场景下可以采用熔断机制。在同一JVM内部采用线程隔离机制，把线程分为若干组，不同的线程组分别服务于不同的类和方法，避免因为一个小功能点的故障，导致JVM内部所有线程受到影响。

    - 此外，完善的运维监控（磁盘IO，网络IO）和APM（全链路性能监控）也非常重要，能及时预警，防患于未然，在故障发生时也能帮助我们快速定位问题。

# kubernetes相关事故

## 2023年11月27日：滴滴故障

- [天空的代码世界：8个措施规避11.27滴滴变更故障](https://mp.weixin.qq.com/s/dOPRcc3idqiQP_A76Y5yFg)

- 事故影响：2023 年 11 月 27 日晚上 10 点 30 分左右，滴滴出行平台突然出现系统故障，导致乘客和司机无法正常使用 APP 进行叫车、接单等操作。

- 事故损失：4亿交易额

- 事故原因

    - 关于故障原因，滴滴官方只说了一个信息：底层系统软件发生故障。

    - 猜测原因：
        - 网上不少在传，是因为 k8s 升级版本不对导致全部故障的。
        - 有人说应该升级到 1.12 版本，升级到了 1.20 版本。
        - 也有人说升错版本，相当于降级了（应该升级到 1.20，升为了 1.12 ？）。


    - 这意味着滴滴的 k8s 线上至少存在 3 个可使用版本。
        - 这里面其实暴露出滴滴 k8s 内部版本控制的一个严重发布流程缺陷。
        - 对于比较重要的系统，为了防止线上版本误操作，一般同一时间只允许存在两个版本：一个是稳定版本，一个是灰度版本。
        - 此时，只允许继续灰度，即将稳定版本升级为灰度版本；或者进行回滚，即将灰度版本降级为稳定版本。
        - 其他版本都应该处于加锁状态，不能进行安装部署，防止误操作。

    - 10 月 17 日滴滴在“滴滴技术”发布的[《滴滴弹性云基于 K8S 的调度实践》](https://mp.weixin.qq.com/s?__biz=MzU1ODEzNjI2NA==&mid=2247566470&idx=1&sn=fcb1f051b981d94806210e59c183e988&scene=21#wechat_redirect)文章。

        - 文中提到滴滴把 k8s 的版本从 1.12 升级到 1.20了。
        - 在这个升级方案选项中，滴滴对比了搭建新集群与集群原地升级。
            - 原地升级的其中一个难点就是：集群体量大，最大集群规模已经远远超出了社区推荐的5千个 node 上限，有问题的爆炸半径大；
        - 不过滴滴最终还是选择了集群原地升级这种高风险操作。

        - 10 月 17 日的文章中说：目前已无损升级滴滴所有核心机房万级别的 node 和十万级别 pod ，且升级过程中业务完全无感，未发生一次故障。

        - 故障时间是11月27日，已经过去两个月，按道理已经将所有版本全部升级到 1.20 了。
            - 所以，滴滴的这次故障，与上次升级的故障可能没啥直接关系。
            - 没有直接关系，不代表没有关系。
                - 这就涉及到线上发布流程的另一个问题：可回滚性与有效版本。
                - 滴滴由于在原地升级的，一旦升级完毕，这个操作应该是不可回滚的。
                - 那新版本全部升级完之后，旧版本应该标记为不可发布，防止误操作发布上线，产生问题。

- 版本控制

    - 汇总下有无效版本、有效版本、稳定版本、可发布版本，可以发现版本之间扭转是一个有向图状态机。
        ![image](./Pictures/事故/版本控制.avif)

    - 各个版本的含义看上图很清晰了，状态如下。

        - 1.部分版本不可回退，称为无效版本，其他的称为有效版本。
        - 2.默认情况下全量部署的有效版本称为稳定版本。
        - 3.稳定版本的容器可以直接发布或回退的版本，称为可发布版本。
        - 4.一旦部署其中一个可发布版本，选择的可发布版本就变为灰度版本。
        - 5.存在两个版本时，只能部署灰度版本或者稳定版本，不能部署其他版本。

    - 附加：不向下兼容的版本一旦变为稳定版本，旧版本应该全部变为无效版本

- 滴滴的这次故障是变更导致的，所以不得不提怎么规避变更导致灾难性故障。

    - 措施1：可回滚。
        - 变更的第一要义是可回滚，一旦有问题，赶紧回滚即可。

    - 措施2：可灰度。
        - 灰度的概念比较抽象，根据影响范围，粒度应该逐渐变大，例如从单机、单SET、单集群、单机房依次扩大。
        - 另外，应该从优先级较低的机房、集群、SET来灰度验证。

    - 措施3：爆炸半径。
        - 有时候没办法选择单机或单SET灰度变更，选择的是一刀切。
        - 这个时候就需要从集群维度控制爆炸半径了。

        - 不同业务物理隔离为不同的集群，从而在遇到极端故障时，不同集群互不影响。
        - 另外，即使进行了集群划分，后续也需要对集群进行持续优化，即分析集群是否合理（是否过大），是否需要调整。

    - 错误4：故障降级。

        - 线上出问题是不可避免的。

        - 所以我们需要有降级措施来降低故障的影响范围。

        - 故障降级应该从全局维度来做，而不是各模块各做各的。

            - PS：如果各模块之间有 SLA 承诺，那每个模块就需要做各自的降级了。

            - 简单来说，从全局维度分析核心链路，并在最靠近用户的某一层增加柔性降级功能（当然，一般各层都会做自己的降级）。

        - 底层的某个模块出现故障，自动开启上层的柔性降级，从而可以降低故障的影响。

        - 例如服务加一个兜底缓存就是常见的降级措施。

        - 记得之前我这边有个储存故障，失败率100%，由于有一层缓存，读业务竟然全部都不知道这个故障的存在。

    - 措施5：故障演习。
        - 故障降级有了，还需要确保降级是否可以生效，以及影响范围是否降到足够小。
        - 这时候就需要进行故障演习，模拟各模块故障时，是否可以降级，以及影响范围是否符合预期。

    - 措施6：版本控制
        - 上面提到，发布系统应该做版本控制，不能随意发布任何版本，需要做一些限制，这里不再重复介绍。

    - 措施7：发布checklist

        - 我们团队是有一个 发布checklist 的流程。

            - 简单来说，发布一个版本时，需要写一个简单的文档，介绍做了啥功能，哪些依赖需要变更、变更会影响哪些功能、对应的监控指标有哪些、测试用例集、回滚流程。
            - 这个文档写好后，需要发到团队群里，供大家评审检查，当然，主要是我检查。
            - 不过，我们团队的这个发布checklist 还没有形成一个固定的模版，每个人写的不一样。
        - 我已经把这个事项放到 TODO list 中了，后面会分配给某个同事来制定一个标准模版来供团队内其他人参考。

        ![image](./Pictures/事故/checklist流程.avif)

    - 措施8：其他
        - 关于如何规避故障或者降低故障的影响范围，相关的措施有很多，这里就不一一介绍了。
        - 比如单元测试覆盖率、接口测试覆盖率、代码静态扫描、代码review、混沌工程、防御性编程、架构方案评审、故障复盘评审与改进等。

    - 当然，也需要注意，一定要注意这些措施是否执行到位。

        - 例如我们团队有个同事，写了很多单侧，但是所有的单侧都是只调用功能函数，没有判断函数的返回值是否符合预期。

        - 这样的单侧是没有意义的，需要严格禁止。

        - 这是态度问题，我看到后非常生气的，公开进行点名批评。

# 数据库相关事故

## dba保命原则

- [白鳝的洞穴：遇到重大运维问题时的保命原则](https://mp.weixin.qq.com/s/L1jvJuH1_XMyq2cqd7cEjQ)

- 如果你遇到了某个重大的运维问题，采取什么样的措施才是正确的呢？
    - 搞清楚这一点相当重要，如果出现策略选择错误，那很可能会丢饭碗的。

- 前几天和一个经历过十年前一次十分著名的大故障的DBA聊天，最后难免会问到那次事故。
    - 他回顾完这个事件后说，当时我们的最大错误决策是按照厂家的建议去停了那个第三方复制设备，其实在这种业务高峰叠加设备性能故障的场景中，很多因素是不确定的，对于第三方设备的特性我们也是知之甚少，当时不应该做这种操作，而是**应该先通过业务限流**的方式让系统能维持运行，等营业厅下班后，**非业务高峰期再去做高风险的动作**。如果那样，那次事故可能能避免了。

- 原则1：

    > 在各种处置策略中，先选择最为简单的，风险小的处置策略

    > 在承担的责任中，要选择责任小的责任来承担

    - 例子：系统运行性能虽然大幅下降，但是还在业务能忍受范围内，并无恶化迹象的时候，我们可以选择承担这次性能故障的责任。
        - 如果我们不想承担这个责任，非要在短时间内解决问题，那么也要尽可能在自身能力范围内去做优化调整。如果当时的故障已经超出了自身的能力范围，那么宁可承担这个小一点的责任也不要冒险去犯错误，从而承担更大的责任。

    - 在实际工作中，能够想明白这一点，并按照上面的原则去做并不容易，我们在实际工作中看到的往往是一些更小的运维故障因为处置不当而导致超级大故障的案例。

        - 例子：Oracle RAC有一个节点故障宕机了，这时候我们应该做什么操作呢？大多数朋友可能会选择重新启动一下，也有些朋友会选择观望，什么都不做。
            - 实际上，如果是一些负载较高的核心业务系统，那么我们应该首先检查活着的节点的日志，看看是否存在异常，是否存在也宕机的风险。
            - 然后去观察活着节点的活跃会话数、会话数、负载、等待事件等，看看有没有风险。
                - 如果存在风险，先通过杀会话把系统稳定住。等一切稳定后，才去分析宕机的原因，并判断重启故障实例的风险。
                - 如果你无法判断风险，而当时正好是业务高峰，那么你可以选择暂时不重启故障节点，等业务高峰过去后再去处理。
                - 最为忌讳的是，RAC故障切换后不久，业务还没有稳定之前就去重启故障节点。采取这种做法的惨痛案例比比皆是。

- 原则2：

    > 不要以为一切都在你的掌握之中，作为DBA ,数据中心里你不了解的东西太多了，因此考虑问题的时候必须要留有余地。不要选择看似最佳的解决方案。

    - 例子：
        - 问题：大概是十五年前吧，某企业的数据中心经历了一次机房双路停电的事故。虽然数据中心是两路供电的，但是供电公司的两路电同时故障。这种故障是因为数据中心建设时选择双路供电时为了省钱导致的，虽然两路电来自于两个220KV变电站，但是上位变电站是同一个，上位站故障两路电就都没了，而且供电公司无法给出明确的修复时间。

        - 策略1：我和他们的IT主管通电话讨论策略，我的策略是先把核心业务系统和存储都停了，外围系统先跑着。我的理由是适逢盛夏，如果三四个小时不来电，UPS虽然能撑得住，但是机房温度会过高，把核心系统停了，也就是几个小时的停机。

        - 策略2：但是IT主管不同意这个方案，他认为如果把外围系统都停了，八个小时内能恢复供电，他的UPS也都撑得住，保住了核心系统，那就是大功一件。对于机房温度的事情，他立即找到了制冰公司，让他们送冰块到机房降温。

        - 结局：最后的结局机房温湿度超标导致核心存储系统自动保护，有损自动关机。核心系统数据库出现大量坏块，ADG备机存储同样故障，磁带库磁带损坏，无法恢复。最后我们通过BBED帮他忙强行拉起了数据库，把数据导出后重新建库、补充丢失数据。核心系统2天后才恢复对内服务，一星期后才恢复对外提供查单业务，给企业声誉造成了很大的影响。

- 在一些特别严重的运维故障发生时，以自己的能力范围来选择采取的措施，先考虑那些风险与危害较小，自己比较擅长的方式去处置，是DBA保命的重要原则。这种事故一旦变成大故障，肯定是要有人出来担责的，DBA是最好的替罪羊。

## [dbaplus社群：自己亲手引发运维事故是一种怎样的体验？](https://mp.weixin.qq.com/s/y4KRjO0IJskoYTmJQjrc8A)

- 事故起因

    - 某互联网公司，有一个实时计费系统。有一天我闲着没事干，到前台转悠。前台小姑娘和我说，计费系统的时间不准，慢了刚好1年。
        - 我问她之前是不是也这样，她说是的，一直都比实际慢1年。我估计是系统上线的时候，实施工程师把年度时间改错了。但是用了这么长时间都没有问题，说明并不影响计费系统的正常运行。

    - 我不经思考就直接对她说：“这简单，把linux系统时间改一下就可以了。”然后，在计费系统里熟练地输入了更正时间的代码，毫不犹豫地按下了回车。

    - 前台小姑娘一脸微笑，但是突然，她脸色凝重了起来，指着计费屏问我：“怎么在线用户都不见了？”
        - 我一看，也觉得奇怪，正常在线用户都有1000多人呢？现在怎么只有几十人了？
        - 我纳闷了好长一会，然后接到了客服部的电话，客服部急迫地问我：“是不是有什么故障？投诉台有上百个电话同时打进来，说是断网了！”
        - 监控室几乎也是同一时间，也打电话过来了，问我是不是出了什么故障了，他们监控到有大范围用户断线的异常告警。


    - 正当我不知所措的时候，已经惊动到了直属领导涛哥，因为后台监控系统一旦有告警，告警短信就会第一时间自动发到相关维护人员的手机上。
        - 涛哥打电话问我怎么回事，我实话实说了，是边哭边说的。
        - 涛哥也是很有领导魅力，当下叫我先保住现场，稳住用户，他和运维组的工程师们马上赶过来。
        - 10多分钟后，涛哥和运维组的工程师及DBA火速抵达了现场。

- 事故原因：故障的原因是时间变快了1年导致的，所以在1年内过期的账号全部被踢下线了，而且无法重新登录。

    - 当时DBA写了个语句查询之后发现，这些账号多达3千多个。
    - 将时间再改回去也行不通，系统时间就会颠倒错乱，数据就全乱套了，后果更严重。
    - 涛哥果断做了决定，直接修改数据库，将这3千多个账号的到期时间，全部改到年底。

- 事故处理流程：
    - 当时DBA写了个语句查询之后发现，这些账号多达3千多个。将时间再改回去也行不通，系统时间就会颠倒错乱，数据就全乱套了，后果更严重。涛哥果断做了决定，直接修改数据库，将这3千多个账号的到期时间，全部改到年底。
    - DBA赶紧写了相关语句，同时对相关的数据表进行了备份。
    - 语句准备执行的时候，DBA手都抖了，涉及到的账号不是一两个，而是几千个，影响范围太大了，万一有啥差错，就吃不了兜着走。
    - 语句执行的时间很长，我们的心都在颤抖，好在顺利执行了。

    - 之后，我们赶紧抽查一部分账号，发现这些账号已经能正常登录了，然后赶紧通知客服部的工作人员，叫用户重新登录，借口是网络波动导致的。
        - 从故障发生到恢复，用了40多分钟。

- 财务损失：

    - 但是，计费金额和财务账上的已经对不上号了，后续财务部算了一下，出现了40多万元的空缺。
    - 正常情况下，故障时间超过10分钟就会被定性为事故，总部将这次事故定性为1级：严重事故，人为。
    - 这件事结束后，我被调离了工作岗位，公司对我进行了长达3个月的重新考核，职称从T2降级到了T3，年终奖和绩效全没了... ...
    - 我的直属领导涛哥，因管理不善，被记大过处分... ...

## [dbaplus社群：自己亲手引发运维事故是一种怎样的体验？](https://mp.weixin.qq.com/s/y4KRjO0IJskoYTmJQjrc8A)

- 曾经给公司的一个客户维护数据库，要删除一个掉测试用户，输入完 delete from users，顺手快捷键执行了……

- 最坑爹的是数据库是游戏组的老哥搭建的，用的phpstudy搞的，没有开启binlog，数据库的几十万用户，客户花了几百万推广费。那一瞬间，就感觉背后汗水流下来了。

- 结果因为有外键，没删掉！！

- 真是吓死了……

## [dbaplus社群：不小心删了公司数据库，停机45分钟来得及跑路吗……](https://mp.weixin.qq.com/s/TyfACWqOALtijP_O78HUWA)

- 事故起因：

    - 我收到来自支持团队的消息，说我们的一个客户遇到问题。我判断这个问题的重要程度很高，需要立即解决。15分钟之后，我明确了问题来源——应该删除数据库中的一些损坏订单。


        - 由于需要删除的订单有几百个，所以我决定编写一个简单的 SQL 查询语句，而不是手动操作（警告！）。

    - 实际语句稍显复杂，在此简化为：
        ```sql
        UPDATE orders
        SET is_deleted = true

        WHERE id in (1, 2, 3)

    - 我按下 CTRL + Enter 并运行这条命令。耗时超过1秒时，我才醒悟过来，客户端 DBeaver 看到空的第三行，同时忽略了第四行。```

        - 没错，我删除了数据库里的全部订单。

- 事故处理流程：

    - 深吸一口气后，我意识到得快速行动起来，不能再犯错误浪费时间了。
    - 好在恢复工作执行得很好。

    - 停止系统——约 5 分钟
    - 创建变更前数据库（幸运的是我们有 PITR）的克隆——约 20 分钟
    - 在等待期间给老板打电话
    - 根据克隆更新生产数据库的信息*—— 15 分钟
    - 启动系统——约 5 分钟

    - 因为公司具备多个独立系统，无法停止所有系统，所以我决定不还原整个数据库，避免在恢复过程中丢失已完成的更改。公司使用 GCP（Google Cloud Platform，谷歌云平台） 提供的托管 PostgreSQL，所以我在更新前创建了新的克隆，导出克隆中的 id 和 is_deleted 列，然后将结果导入到生产数据库中。随后，使用简单的 update + select 语句。
        - 显然，我本可以轻易避免这长达 45 分钟的停机时间……

- 回顾事故处理流程的处理：

    - 整个事件听起来是一个你永远不会犯、愚不可及（甚至在大公司根本不允许犯）的错误。

    - 没错，问题根本不在于错误的 SQL 语句，人为的小过失从来都不是真正的问题。我运行那条命令这一动作，只是整个事故链条的最后一环。

        - 为什么在周末处理生产环境？当时问题并不紧急，也没有人要求我立刻修复故障，我本可以等到周一再处理。
        - 谁会不先在 QA 环境上运行就贸然在生产数据库中更改呢？
        - 为什么我不调用 API而选择手动编辑？
        - 如果没有 API，为什么我没及时与同事沟通，对这一敏感操作双重检查？
        - 最糟糕的是，**为什么我没使用事务？**其实只要用了 Begin语句，一旦出错，使用 Rollback命令（回滚操作） 就能解决。

    - 错误环环相扣，但凡避免任何一个错误，都不可能发生事故。这些失误都可以归因于：我太自信了。

    - 不过好在有序的恢复程序阻止了事故连锁反应，如果无法将数据库恢复到正常状态，我都不敢想象后果如何……

- 优化改进：

    - 减少对数据库直接访问的需求，并创建相关的 API
    - 我总是先在 QA 上运行查询（显然，没有什么比灾难更能给人教训）
    - 我与产品经理商量，了解真正紧急和可以等待的事项
    - 任何对生产环境进行更删改操作都需要两个人来完成，这实际上防止了其他错误！
    - 我开始使用事务处理机制

    - 事故发生后，我向团队同事们讲述了详细过程，没有隐瞒任何细节，也没有淡化我的过错。
        - 责备他人还是不追责，是一个相当微妙的选择。当你犯错时，就是一个传递正确信息的好机会。
        - 如果你为此道歉 1000 次，同事会认为，当事情发生在他们身上时，你期待他们也要给出相同反应。
        - 如果你一笑了之，完全忽视事故影响，同事会认为这程度的错误是可以接受的。
        - 如果你承担责任、学习并予以改进——同事也会用这种态度行事。

## [dbaplus社群：又一名企遭遇P0级重大故障，竟是删库的戏码……](https://mp.weixin.qq.com/s/poKCwAAjp2sdiufCbRwV9Q)

- 软件里的爱马仕

    - Linear 是硅谷这两年涌现出的一家明星公司。它做的是类似于 Jira, Asana 的项目管理工具，但定位聚焦在软件研发的项目管理上。

    - 因为创始人强大的设计背景，其在创办 Linear 前曾先后负责过 Coinbase 和 Airbnb 的设计团队。所以 Linear 的产品设计尤其突出，以致在行业里掀起了 Linear style 的设计潮流。

    - 而就是这样一支全身散发着光环的顶级硅谷团队，也在前不久迎来了 5 年公司历史上最大的一次故障。而元凶依然是我们熟悉的桥段「删库」。

- 事故影响

    - 所有用户都经历了 1 个小时的不可用，这段时间 Linear 在从备份中恢复数据。
    - Linear 在 36 小时内恢复了 99% 的误删数据，剩下的因为冲突的原因没有办法自动解决。Linear 也说这是它 5 年历史上最严重的一次故障。

- 事故时间线

    - 1 月 24 日

        - 04:47: 全量备份完成 (事发前)。
        - 07:01: 造成数据丢失的变更合并入主干。
        - 07:20: 变更完成。
        - 07:52: 发现异常，开始自查。
        - 08:10: 启动严重事故预案，呼叫更多的工程师。
        - 08:36: 更新公开渠道, 包括 status page 和 X，提示正在调查数据访问问题。
        - 09:20: 进一步更新。
        - 09:56: Linear 进入维护模式，防止进一步的变更操作，并且开始从备份进行恢复。
        - 10:48: 数据库恢复到 4:47 的备份，Linear 重新开始可以访问。
        - 11:09: 更新 Status page 到观察状态。
        - 11:30: 开始恢复 4:47 到 9:56 之间的数据。
        - 13:50: 给 4:47 到 9:56 之间新建了 workspace 的用户发送了邮件，因为 Linear 无法重建这些 workspace。
        - 15:35: 给所有受到影响的用户和 workspace 管理员发送邮件，告知故障的信息和恢复方式。

    - 1 月 25 日

        - 14:00: 通知管理员上线了专门的数据还原页面。
        - 14:25: 修复了数据还原页面的 bug，强制客户端刷新 (这又打挂了 API 导致了应用加载的问题).
        - 16:40: 数据恢复试运行启动。
        - 17:49: 正式的数据恢复开始。
        - 19:48: 恢复了 98% 受影响的 workspace。
        - 23:20: 恢复了 99% 受影响的 workspace。

    - 1 月 26 日

        - 07:37: 除了一个 workspace 之外，所有的都恢复了。
        - 08:39: 完成了最后一个 workspace 的恢复。

- 事故原因

    - Linear 使用的是 PostgreSQL 数据库，导致删数据的是这条语句：

        ```sql
        TRUNCATE TABLE <new_table> CASCADE
        ```

        - `CASCADE` 关键词会把所有有外健关联到 new_table 的表数据都清空掉。

        ![image](./Pictures/事故/postgresql语句导致删库模拟复现.avif)

    - 上面我们简单模拟了一下场景，可以看到当我们 TRUNCATE t CASCADE 后，会把 ref_t 的数据也都清空。我们看到通过命令行操作是有提示的，但是变更生产环境的流程通常不会采用命令行直连的方式。

    - Linear 采用的是主干开发，要变更的 SQL 脚本是保存在代码仓库里的。每次变更时，先提交 SQL 语句进行审核，审核会进行一些 CI 的自动检查和人工审核，没有问题后，SQL 脚本合并入主干，然后触发变更。因为代码发布和数据库变更是分离的，为了保持代码能同时兼容新旧两个数据库版本，会使用 feature flag。

        - Linear 的 CI 自动检查没有捕捉到这个问题，因为 CI 里没有针对 TRUNCATE 的规则。而在本地测试和人工审核时也都没有发现这个问题，也是因为这个 TRUNCATE 的隐蔽性更强。Linear 这次是在开发一个新功能时从现有的生产数据库表里创建了一份测试数据库表。工程师在测试完成后，打算清理测试数据库表。但因为生产数据表也关联了测试数据表的外键。所以 TRUNCATE 测试数据表时，也把生产数据表跟端掉了。

- 事故排查和恢复

    - 因为 Linear 使用了好几层缓存，所以在误删了生产数据后，客户端因为缓存的关系，还没有直接报错，识别到问题的时间就靠后了些。

    - 不过 Linear 因为本身要实现同步的需要，把用户的每一次操作都以日志的形式记录了下来，这部分的数据是单独存放的，所以他们可以基于 4:47 的全量备份，回放所有的用户操作。但还是会有少部分操作，因为冲突的原因是无法自动回放的，这就还是需要用户手动介入。

    - 另外 Linear 的 PostgreSQL 数据库是开启了 Point-in-time recovery (PITR)，但因为团队之前没有测试过 PITR，所以在恢复时没有选择这个方案，不然的话，其实可以通过 PITR 恢复到 7:01 这个变更发生的时间点，然后再回放接下来的用户操作。这样可以缩小回放的数据量。

- 事故反思优化

    - Linear 自己列了如下的改进措施：

        - 剥夺所有用户在生产数据库上的 TRUNCATE 权限。
        - 改进如何创建和执行数据库的变更操作。其中包括通过 DBA 的审核实践，和代码审核分离，以及自动检查高危操作。
        - 改进预发环境的数据库变更测试流程。
        - 构建和演练使用 PITR 的数据恢复流程。
        - 改进内部事故处理的流程。
        - 改进数据完整性检查。
        - 给 Linear 添加只读模式。这样即使数据库不可写时，依然可以能让用户访问。

## [dbaplus社群：同事用 insert into select 迁移数据，上线后被开除了……](https://mp.weixin.qq.com/s/kossMg97i6vvJrd8oB7M_g)

- 血一般的教训，请慎用 insert into select。同事应用之后，导致公司损失了近10w元，最终被公司开除。

- 事故起因

    - 公司的交易量比较大，使用的数据库是mysql，每天的增量差不多在百万左右，公司并没有分库分表，所以想维持这个表的性能只能考虑做数据迁移。

    - 同事李某接到了这个任务，于是他想出了这两个方案：

        - 1.先通过程序查询出来，然后插入历史表，再删除原表
        - 2.使用`insert into select`让数据库IO来完成所有操作

        - 第一个方案使用的时候发现一次性全部加载，系统直接就OOM了，但是分批次做就过多io和时间长，于是选用了第二种方案，测试的时候没有任何问题，开开心心上线，然后被开除。

- 到底发生了啥，我们复盘一下

    - 来看第一个方案，先看伪代码

        ```sql
        // 1、查询对应需要迁移的数据
        List<Object> list = selectData();

        // 2、将数据插入历史表
        insertData(list);

        // 3、删除原表数据
        deleteByIds(ids);
        ```

        - 我们可以从这段代码中看到，OOM的原因很简单，我们直接将数据全部加载内存，内存不爆才怪。


    - 再来看看第二个方案，到底发生了啥。

        - 为了维持表的性能，同时保留有效数据，经过商量定了一个量，保留10天的数据，差不多要在表里面保留1kw的数据。所以同事就做了一个时间筛选的操作，直接`insert into select ... dateTime < (Ten days ago)`，爽极了，直接就避免了要去分页查询数据，这样就不存在OOM啦。还简化了很多的代码操作，减少了网络问题。

        - 为了测试，还特意建了1kw的数据来模拟，测试环境当然是没有问题啦，顺利通过。考虑到这个表是一个支付流水表，于是将这个任务做成定时任务，并且定在晚上8点执行。

        - 晚上量也不是很大，自然是没有什么问题，但是第二天公司财务上班，开始对账，发现资金对不上，很多流水都没有入库。最终排查发现晚上8点之后，陆陆续续开始出现支付流水插入失败的问题，很多数据因此丢失。

        - 最终定位到了是迁移任务引起的问题，刚开始还不明所以，白天没有问题，然后想到晚上出现这样的情况可能是晚上的任务出现了影响，最后停掉该任务的第二次上线，发现没有了这样的情况。

    - 问题在哪里？

        - 为什么停掉迁移的任务之后就好了呢？这个insert into select操作到底做了什么？我们来看看这个语句的explain。

        - 这个查询语句直接走了全表扫描。这个时候，我们不难猜想到一点点问题。如果全表扫描，我们这个表这么大，是不是意味着迁移的时间会很长？假若我们这个迁移时间为一个小时，那是不是意味着就解释了我们白天没有出现这样问题的原因了。

        - 但是全表扫描是最根本的原因吗？

        - 我们不妨试试，一边迁移，一边做些的操作，还原现场。最终还是会出现这样的问题。这个时候，我们可以调整一下，大胆假设，如果不全表扫描，是不是就不会出现这样的问题。当我们将条件修改之后，果然发现没有走了全表扫描了。

        - 最终再次还原现场，问题解决了

    - 得出结论：全表扫描导致了这次事故的发生。

- 原因

    - 在默认的事务隔离级别下：`insert into a select b`的操作a表示直接锁表，b表是逐条加锁。这也就解释了为什么出现陆续的失败的原因。在逐条加锁的时候，流水表由于多数是复合记录，所以最终部分在扫描的时候被锁定，部分拿不到锁，最终导致超时或者直接失败，还有一些在这加锁的过成功成功了。

- 为什么测试没有问题？

    - 在测试的时候充分的使用了正式环境的数据来测试，但是别忽视一个问题，那就是测试环境毕竟是测试环境，在测试的时候，数据量真实并不代表就是真实的业务场景。比方说，这个情况里面就少了一个迁移的时候，大量数据的插入这样的情况。最终导致线上bug

- 解决办法

    - 既然我们避免全表扫描就可以解决，我们避免它就行了。想要避免全表扫描，对where后面的条件做索引，让我们的select查询都走索引即可。

    - insert into还能用吗？

        - 可以。使用`insert into select`的时候请慎重，一定要做好索引。

## [dbaplus社群：数据库缓存没预热会怎样？我帮大家逝了下……](https://mp.weixin.qq.com/s/j11OKAmb3EfhQwukR4igWA)

- 缓存不预热会怎么样？我帮大家淌了路。缓存不预热会导致系统接口性能下降，数据库压力增加，更重要的是导致我写了两天的复盘文档，在复盘会上被骂得狗血淋头。

- 事故起因

    - 事情发生在几年前，我刚毕业时，第一次使用缓存内心很激动。需求场景是虚拟商品页面需要向用户透出库存状态，提单时也需要校验库存状态是否可售卖。但是由于库存状态的计算包含较复杂的业务逻辑，耗时比较高，在500ms以上。如果要在商品页面透出库存状态那么商品页面耗时增加500ms，这几乎是无法忍受的事情。

    - 如何实现呢？最合适的方案当然是缓存了。我当时设计的方案是如果缓存有库存状态直接读缓存，如果缓存查不到，则计算库存状态，然后加载进缓存，同时设定过期时间。何时写库存呢？答案是过期后，cache miss时重新加载进缓存。由于计算逻辑较复杂，库存扣减等用户写操作没有同步更新缓存，但是产品认可库存状态可以有几分钟的状态不一致。为什么呢？

        - 因为仓库有冗余库存，就算库存状态不一致导致超卖，也能容忍。同时库存不足以后，需要运营补充库存，而补充库存的时间是肯定比较长的。虽然补充库存完成几分钟后，才变为可售卖的，产品也能接受。梳理完缓存的读写方案，我就沉浸于学习Redis的过程。

    - 第一次使用缓存，我把时间和精力都放在Redis存储结构，Redis命令，Redis为什么那么快等方面的关注。如饥似渴地学习Redis知识。

        - 直到上线阶段我也没有意识到系统设计的缺陷。

        - 代码写得很快，测试验证也没有问题。然而上线过程中，就开始噼里啪啦地报警，开始我并没有想到报警这事和我有关。直到有人问我，“XXX，你是不是在上线库存状态的需求？”

        - 我人麻了，”怎么了，啥事”，我颤抖地问。

        - “商品页面耗时暴涨，赶紧回滚。”一个声音传来。

        - “我草”，那一瞬间，我的血压上涌，手心发痒，心跳加速，头皮发麻，颤抖的手不知道怎么在发布系统点回滚，“我没回滚过啊，咋回滚啊？”

        - “有降级开关吗？”一个声音传来。

        - "没写……"我回答的时候觉得自己真是二笔，为啥没加降级啊。（这也是复盘被骂的重要原因）

- 那么如何对缓存进行预热呢？

    - 灰度放量

        - 灰度放量实际上并不是缓存预热的办法，但是确实能避免缓存雪崩的问题。例如这个需求场景中，如果我没有放开全量数据，而是选择放量1%的流量。这样系统的性能不会有较大的下降，并且逐步放量到100%。

        - 虽然这个过程中，没有主动同步数据到缓存，但是通过控制放量的节奏，保证了初始化缓存过程中，不会出现较大的耗时波动。

        - 例如新上线的缓存逻辑，可以考虑逐渐灰度放量。

    - 扫描数据库刷缓存

        - 如果缓存维度是商品维度或者用户维度，可以考虑扫描数据库，提前预热部分数据到缓存中。

        - 开发成本较高。除了开发缓存部分的代码，还需要开发扫描全表的任务。为了控制缓存刷新的进度，还需要使用线程池增加并发，使用限流器限制并发。这个方案的开发成本较高。

    - 通过数据平台刷缓存

        - 这是比较好的方式，具体怎么实现呢？

        - 数据平台如果支持将数据库离线数据同步到Hive，Hive数据同步到Kafka，我们就可以编写Hive SQL，建立ETL任务。把业务需要被刷新的数据同步到Kafka中，再消费Kafka，把数据写入到缓存中。在这个过程中通过数据平台控制并发度，通过Kafka 分片和消费线程并发度控制 缓存写入的速率。

        - 这个方案开发逻辑包括ETL 任务，消费Kafka写入缓存。这两部分的开发工作量不大。并且相比扫描全表任务，ETL可以编写更加复杂的SQL，修改后立即上线，无需自己控制并发、控制限流。在多个方面ETL刷缓存效率更高。

        - 但是这个方案需要公司级别支持 多个存储系统之间可以进行数据同步。例如mysql、kafka、hive等。

        - 除了首次上线，是否还有其他场景需要预热缓存呢？

- 需要预热缓存的其他场景

    - 如果Redis挂了，数据怎么办

        - 刚才提到上线前，一定要进行缓存预热。还有一个场景：假设Redis挂了，怎么办？全量的缓存数据都没有了，全部请求同时打到数据库，怎么办。

        - 除了首次上线需要预热缓存，实际上如果缓存数据丢失后，也需要预热缓存。所以预热缓存的任务一定要开发的，一方面是上线前预热缓存，同时也是为了保证缓存挂掉后，也能重新预热缓存。

    - 假如有大量数据冷启动怎么办

        - 假如促销场景，例如春节抢红包，平时非活跃用户会在某个时间点大量打开App，这也会导致大量cache miss，进而导致雪崩。此时就需要提前预热缓存了。具体的办法，可以考虑使用ETL任务。离线加载大量数据到Kafka，然后再同步到缓存。

- 总结

    - 一定要预热缓存，不然线上接口性能和数据库真的扛不住。

    - 可以通过灰度放量，扫描全表、ETL数据同步等方式预热缓存

    - Redis挂了，大量用户冷启动的促销场景等场景都需要提前预热缓存。

# 非技术性的IT从业人员事故

## [临时工说：搞数据库 光凭的是技术，那DBA的死多少次？](https://mp.weixin.qq.com/s/87aXOLsQq-2WJZdln9rS8w)

- 以下故事，均是虚构，如有雷同，纯属巧合。这里只是提醒初入职场的DBA 同学，技术固然重要，但任你技术再高，能力再强，你要明白你就一个DBA，这已经限定了你的发展和上升，如果你想一日DBA ，终身DBA，那么就持续的不加思索的在DBA 这条路上 飞奔，注意带好头盔，任何一个职业除了技术，还有其他在上更重要的部分，千万别唯技术论，最近流传的一句名言，在资本面前，技术算个X，人外有人，天外飞仙！

- 1.带有情绪的DBA

    - 情绪本身是一包糖，在糖尿病人的眼里他是毒药，在孩子的眼里那就是美味，情绪也是，但带着负面情绪的DBA 本身就是危险的。 由于某些原因某个DBA 被领导训斥了一番，人比较年前，工作中就开始带有情绪了，自己也内向，不容易抒发在处理一件日常的工作，清理无用的数据的过程中，没有和业务部门在进行沟通，因为情绪低落，所以就想当然的，将自己认为的数据库表给删除了，并且因为情绪的问题删除后，也没有和相关业务部门的人员进行核对和确认。因为业务的因素，表并不是OLTP的业务，在业务使用到相关表的时候，已经是转过一个礼拜了，可备份保留的时间被设置为保留3天，因为磁盘等因素导致了，最终发现数据丢失的时候，已经找不到对应的备份了。

    - 事后通报批评，扣罚当月工资，奖金能不能发不知道，本年度不能评选优秀员工，最终在发生这些事情后的转个月，这个人就莫名的离职了。

- 2.不知深浅的DBA

    - 不知深浅，不知深浅的多了，我算老几，算老十。 其他的岗位不知道不知深浅会怎么样，但如果是DBA 不知深浅，那么死的一定是很灿烂。某公司的DBA 一直使用ORACLE数据库，但领导某天突然下达了，要进行ORACLE 替换的国产数据库进程，并且将这个问题摆到了IT部门的今年重要的工作之一。这个DBA 也算是老人了，对于国产数据库是有一些看法的，不是特别看好，公司在国产数据库选型的工作中，彻底惹恼了这个DBA ,因为他看好的 某数据库最终没有入选，而入选另一个他挺反感的数据库，后面在相关的工作中，他就一个劲的对这个数据库提出各种非难，POC的时候也是提出各种问题，并且专门找这个数据库的软肋，提出各种令人尴尬的问题，乙方的项目负责人一开始还容忍，后面干脆根本不搭理这个DBA，结果可想而知，后面这个DBA 被调离了这个项目和总部，升级到了一个下属单位，后来据说是项目非常成功，周边的系统都被更换了，效果非常好。

- 3.任务轻重分不清的DBA

    - 某公司，DBA 就那么几个人，但是管理的工作包含下属的公司的一部分数据库产品，总公司的也管理，但绩效是总公司来进行评定，所以下属公司的一些数据库产品被维护和轮训的次数相比总公司来讲，要低的多，但是某领导被下派到下面公司做一个紧急的项目，据称 未来这个领导会凭借这个项目称为XXX ，这个领导在下属公司的项目，总公司的人都特别的配合，项目里面有一个数据库产品，乙方提供的方案，但乙方的技术不是太过关，所以这个领导的下面的人员向请总公司的DBA 来协助处理。

    - 但总公司的DBA因为某些原因，并没有积极配合。后来这个领导回到总公司做了二把手后，听说这个IT部门有了调整，说IT部门要积极配合下属公司的工作，并且要积极理解下属公司的问题，一些DBA 就优化到了下属公司工作，后面的事情就不知道。

- 4.一味的忍让，忍不出海阔天空

    - 某外企，有一堆DBA。 因为某DBA 要到其他的部门，据悉是和DBA的部门领导闹的不是太愉快，所以要去别的部门。而这个DBA 部门的领导对于某 S 数据库并不熟悉，他只熟悉O 数据库，对于M 数据库更是嗤之以鼻，因为这样的领导，公司在数据库选型中基本上都是 O数据库，其他的数据库就算被项目要钱，也是百般刁难，最后形成了O 数据库的DBA 一堆，其他的数据库DBA 数量较少的地步，新招来的S 数据库的DBA，面对一堆的烂摊子，将近20T的S数据库，和一堆历史遗留问题，逐个解决。但也经常不受这个DBA 领导的待见，找到时机就刁难一下，要不就是你英语不好了，要不就是你这技术行不行，遇到问题，也是把下属推出去，有一次他忘记了某项目需要S 库配合，晚上进行工作的事情，待早上就给这个S 数据库的DBA 打电话，直接训斥，我不和你说你就不知道做吗，如果出现任何的损失都是你的问题。S数据库的DBA一脸懵逼，项目都是外国人来操作，也没有和我说过呀，再说了说也不会找我一个小卡了米，也是找DBA的领导协调工作，我能预测天机？但面临甩过来的锅，不受着怎么办，只能硬着头皮顶着压力干，最后是有两把刷子把没有主键的2亿的表，平稳的进行了方案设定，方案实施，也没有测试系统，完全凭经验和幸运完成了工作，得到外国人的认可。

    - 最后好像从某领导的嘴里给出的评语是，技术还凑合，就是人不行，我不信任他。或许这个S DBA 最大的问题就是太能忍了。有的时候要么忍，要么滚，后来这个DBA 也忍不了，滚蛋了，不过听说后续过的还行。

- 5.多嘴多舌就你能耐的DBA

    - 某DBA 新到一个公司，因为公司里面没有人懂数据库，所以每次有数据库问题都是他来出面，日子稍微一长，对自己的地位没有清晰的认识，就跳到了领导的头上，每次都是抢话，多话，老是表达一些正确，但刺耳的意见，领导也就起了杀心，直接开始冷处理他，他说任何东西都直接告诉他，SHUTUP ，做任何事情都不给任何的支持，不出功劳，出苦劳的工作都是他的，最后领导放了一个大招，就是让他去完成一个不能完成的工作，直接压垮了这个DBA ，后来是裸辞离开。

- 6.工作没有重点的DBA

    - 公司要进行数据库优化，领导要求对数据库进行优化，并提出一些指导意见，这里注意提出一些指导意见，但DBA并未按照领导的意思行事，而是按照自己的规矩，进行相关的工作，虽然工作中有条理，但没有体现出工作中的“指导意见”，将自己认为的工作重点进行了汇报，而这样的汇报和工作，得到了就是 --- 工作没有重点的，逻辑混乱的，某DBA 的评语。
